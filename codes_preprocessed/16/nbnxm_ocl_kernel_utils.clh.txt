#define GMX_DOUBLE 0 #include "gromacs/gpu_utils/device_utils.clh" #include "gromacs/gpu_utils/vectype_ops.clh" #include "gromacs/pbcutil/ishift.h" #include "nbnxm_ocl_consts.h" #define CL_SIZE (c_nbnxnGpuClusterSize) #define WARP_SIZE (CL_SIZE * CL_SIZE / 2) #if defined _NVIDIA_SOURCE_ || defined _AMD_SOURCE_ # define USE_CJ_PREFETCH 1 #else # define USE_CJ_PREFETCH 0 #endif #if defined cl_intel_subgroups || defined cl_khr_subgroups \ || (defined __OPENCL_VERSION__ && __OPENCL_VERSION__ >= 210) # define HAVE_SUBGROUP 1 #else # define HAVE_SUBGROUP 0 #endif #ifdef cl_intel_subgroups # define HAVE_INTEL_SUBGROUP 1 #else # define HAVE_INTEL_SUBGROUP 0 #endif #if defined _INTEL_SOURCE_ # define SUBGROUP_SIZE 8 #elif defined _AMD_SOURCE_ # define SUBGROUP_SIZE 64 #else # define SUBGROUP_SIZE 32 #endif #define REDUCE_SHUFFLE (HAVE_INTEL_SUBGROUP && CL_SIZE == 4 && SUBGROUP_SIZE == WARP_SIZE) #define USE_SUBGROUP_ANY (HAVE_SUBGROUP && SUBGROUP_SIZE == WARP_SIZE) #define USE_SUBGROUP_PRELOAD HAVE_INTEL_SUBGROUP #define M_FLOAT_1_SQRTPI 0.564189583547756F #ifndef NBNXN_OPENCL_KERNEL_UTILS_CLH # define NBNXN_OPENCL_KERNEL_UTILS_CLH # if CL_SIZE == 8 # define WARP_SIZE_LOG2 (5) # define CL_SIZE_LOG2 (3) # elif CL_SIZE == 4 # define WARP_SIZE_LOG2 (3) # define CL_SIZE_LOG2 (2) # else # error unsupported CL_SIZE # endif # define CL_SIZE_SQ (CL_SIZE * CL_SIZE) # define FBUF_STRIDE (CL_SIZE_SQ) # define ONE_SIXTH_F 0.16666667F # define ONE_TWELVETH_F 0.08333333F # define HALF_F 0.5F # ifdef __GNUC__ # define gmx_unused __attribute__((unused)) # else # define gmx_unused # endif typedef struct { float c2; float c3; float cpot; } shift_consts_t; typedef struct { float c3; float c4; float c5; } switch_consts_t; typedef struct cl_nbparam_params { int eeltype; int vdwtype; float epsfac; float c_rf; float two_k_rf; float ewald_beta; float sh_ewald; float sh_lj_ewald; float ewaldcoeff_lj; float rcoulomb_sq; float rvdw_sq; float rvdw_switch; float rlistOuter_sq; float rlistInner_sq; shift_consts_t dispersion_shift; shift_consts_t repulsion_shift; switch_consts_t vdw_switch; float coulomb_tab_scale; } cl_nbparam_params_t; typedef struct { int sci; int shift; int cj4_ind_start; int cj4_ind_end; } nbnxn_sci_t; typedef struct { unsigned int imask; int excl_ind; } nbnxn_im_ei_t; typedef struct { int cj[4]; nbnxn_im_ei_t imei[2]; } nbnxn_cj4_t; typedef struct { unsigned int pair[CL_SIZE * CL_SIZE / 2]; } nbnxn_excl_t; __constant unsigned supercl_interaction_mask = ((1U << c_nbnxnGpuNumClusterPerSupercluster) - 1U); __constant float c_nbnxnMinDistanceSquared = NBNXM_MIN_DISTANCE_SQUARED_VALUE_FLOAT; gmx_opencl_inline void preloadCj4Generic(__local int* sm_cjPreload, const __global int* gm_cj, int tidxi, int tidxj, bool gmx_unused iMaskCond) { # if defined _AMD_SOURCE_ if (tidxj == 0 & tidxi < c_nbnxnGpuJgroupSize) { sm_cjPreload[tidxi] = gm_cj[tidxi]; } # else const int c_clSize = CL_SIZE; const int c_nbnxnGpuClusterpairSplit = 2; const int c_splitClSize = c_clSize / c_nbnxnGpuClusterpairSplit; if ((tidxj == 0 | tidxj == c_splitClSize) & (tidxi < c_nbnxnGpuJgroupSize)) { sm_cjPreload[tidxi + tidxj * c_nbnxnGpuJgroupSize / c_splitClSize] = gm_cj[tidxi]; } # endif } # if USE_SUBGROUP_PRELOAD gmx_opencl_inline int preloadCj4Subgroup(const __global int* gm_cj) { return intel_sub_group_block_read((const __global uint*)gm_cj); } # endif # if USE_SUBGROUP_PRELOAD typedef size_t CjType; # else typedef __local int* CjType; # endif gmx_opencl_inline void preloadCj4(CjType gmx_unused* cjs, const __global int gmx_unused* gm_cj, int gmx_unused tidxi, int gmx_unused tidxj, bool gmx_unused iMaskCond) { # if USE_SUBGROUP_PRELOAD *cjs = preloadCj4Subgroup(gm_cj); # elif USE_CJ_PREFETCH preloadCj4Generic(*cjs, gm_cj, tidxi, tidxj, iMaskCond); # else # endif } gmx_opencl_inline int loadCjPreload(__local int* sm_cjPreload, int jm, int gmx_unused tidxi, int gmx_unused tidxj) { # if defined _AMD_SOURCE_ int warpLoadOffset = 0; # else const int c_clSize = CL_SIZE; const int c_nbnxnGpuClusterpairSplit = 2; const int c_splitClSize = c_clSize / c_nbnxnGpuClusterpairSplit; int warpLoadOffset = (tidxj & c_splitClSize) * c_nbnxnGpuJgroupSize / c_splitClSize; # endif return sm_cjPreload[jm + warpLoadOffset]; } gmx_opencl_inline int loadCj(CjType cjs, const __global int gmx_unused* gm_cj, int jm, int gmx_unused tidxi, int gmx_unused tidxj) { # if USE_SUBGROUP_PRELOAD return sub_group_broadcast(cjs, jm); # elif USE_CJ_PREFETCH return loadCjPreload(cjs, jm, tidxi, tidxj); # else return gm_cj[jm]; # endif } gmx_opencl_inline float2 convert_sigma_epsilon_to_c6_c12(const float sigma, const float epsilon) { const float sigma2 = sigma * sigma; const float sigma6 = sigma2 * sigma2 * sigma2; const float c6 = epsilon * sigma6; const float2 c6c12 = (float2)(c6, c6 * sigma6); return c6c12; } gmx_opencl_inline void calculate_force_switch_F(const cl_nbparam_params_t* nbparam, float c6, float c12, float inv_r, float r2, float* F_invr) { const float disp_shift_V2 = nbparam->dispersion_shift.c2; const float disp_shift_V3 = nbparam->dispersion_shift.c3; const float repu_shift_V2 = nbparam->repulsion_shift.c2; const float repu_shift_V3 = nbparam->repulsion_shift.c3; const float r = r2 * inv_r; float r_switch = r - nbparam->rvdw_switch; r_switch = r_switch >= 0.0F ? r_switch : 0.0F; *F_invr += -c6 * (disp_shift_V2 + disp_shift_V3 * r_switch) * r_switch * r_switch * inv_r + c12 * (repu_shift_V2 + repu_shift_V3 * r_switch) * r_switch * r_switch * inv_r; } gmx_opencl_inline void calculate_force_switch_F_E(const cl_nbparam_params_t* nbparam, float c6, float c12, float inv_r, float r2, float* F_invr, float* E_lj) { const float disp_shift_V2 = nbparam->dispersion_shift.c2; const float disp_shift_V3 = nbparam->dispersion_shift.c3; const float repu_shift_V2 = nbparam->repulsion_shift.c2; const float repu_shift_V3 = nbparam->repulsion_shift.c3; const float disp_shift_F2 = nbparam->dispersion_shift.c2 / 3; const float disp_shift_F3 = nbparam->dispersion_shift.c3 / 4; const float repu_shift_F2 = nbparam->repulsion_shift.c2 / 3; const float repu_shift_F3 = nbparam->repulsion_shift.c3 / 4; const float r = r2 * inv_r; float r_switch = r - nbparam->rvdw_switch; r_switch = r_switch >= 0.0F ? r_switch : 0.0F; *F_invr += -c6 * (disp_shift_V2 + disp_shift_V3 * r_switch) * r_switch * r_switch * inv_r + c12 * (repu_shift_V2 + repu_shift_V3 * r_switch) * r_switch * r_switch * inv_r; *E_lj += c6 * (disp_shift_F2 + disp_shift_F3 * r_switch) * r_switch * r_switch * r_switch - c12 * (repu_shift_F2 + repu_shift_F3 * r_switch) * r_switch * r_switch * r_switch; } gmx_opencl_inline void calculate_potential_switch_F(const cl_nbparam_params_t* nbparam, float inv_r, float r2, float* F_invr, const float* E_lj) { const float switch_V3 = nbparam->vdw_switch.c3; const float switch_V4 = nbparam->vdw_switch.c4; const float switch_V5 = nbparam->vdw_switch.c5; const float switch_F2 = nbparam->vdw_switch.c3; const float switch_F3 = nbparam->vdw_switch.c4; const float switch_F4 = nbparam->vdw_switch.c5; const float r = r2 * inv_r; const float r_switch = r - nbparam->rvdw_switch; if (r_switch > 0.0F) { const float sw = 1.0F + (switch_V3 + (switch_V4 + switch_V5 * r_switch) * r_switch) * r_switch * r_switch * r_switch; const float dsw = (switch_F2 + (switch_F3 + switch_F4 * r_switch) * r_switch) * r_switch * r_switch; *F_invr = (*F_invr) * sw - inv_r * (*E_lj) * dsw; } } gmx_opencl_inline void calculate_potential_switch_F_E(const cl_nbparam_params_t* nbparam, float inv_r, float r2, float* F_invr, float* E_lj) { const float switch_V3 = nbparam->vdw_switch.c3; const float switch_V4 = nbparam->vdw_switch.c4; const float switch_V5 = nbparam->vdw_switch.c5; const float switch_F2 = nbparam->vdw_switch.c3; const float switch_F3 = nbparam->vdw_switch.c4; const float switch_F4 = nbparam->vdw_switch.c5; const float r = r2 * inv_r; float r_switch = r - nbparam->rvdw_switch; r_switch = r_switch >= 0.0F ? r_switch : 0.0F; const float sw = 1.0F + (switch_V3 + (switch_V4 + switch_V5 * r_switch) * r_switch) * r_switch * r_switch * r_switch; const float dsw = (switch_F2 + (switch_F3 + switch_F4 * r_switch) * r_switch) * r_switch * r_switch; *F_invr = (*F_invr) * sw - inv_r * (*E_lj) * dsw; *E_lj *= sw; } gmx_opencl_inline void calculate_lj_ewald_comb_geom_F(__constant const float* nbfp_comb_climg2d, int typei, int typej, float r2, float inv_r2, float lje_coeff2, float lje_coeff6_6, float* F_invr) { const float c6grid = nbfp_comb_climg2d[2 * typei] * nbfp_comb_climg2d[2 * typej]; const float inv_r6_nm = inv_r2 * inv_r2 * inv_r2; const float cr2 = lje_coeff2 * r2; const float expmcr2 = exp(-cr2); const float poly = 1.0F + cr2 + HALF_F * cr2 * cr2; *F_invr += c6grid * (inv_r6_nm - expmcr2 * (inv_r6_nm * poly + lje_coeff6_6)) * inv_r2; } gmx_opencl_inline void calculate_lj_ewald_comb_geom_F_E(__constant const float* nbfp_comb_climg2d, const cl_nbparam_params_t* nbparam, int typei, int typej, float r2, float inv_r2, float lje_coeff2, float lje_coeff6_6, float int_bit, float* F_invr, float* E_lj) { const float c6grid = nbfp_comb_climg2d[2 * typei] * nbfp_comb_climg2d[2 * typej]; const float inv_r6_nm = inv_r2 * inv_r2 * inv_r2; const float cr2 = lje_coeff2 * r2; const float expmcr2 = exp(-cr2); const float poly = 1.0F + cr2 + HALF_F * cr2 * cr2; *F_invr += c6grid * (inv_r6_nm - expmcr2 * (inv_r6_nm * poly + lje_coeff6_6)) * inv_r2; const float sh_mask = nbparam->sh_lj_ewald * int_bit; *E_lj += ONE_SIXTH_F * c6grid * (inv_r6_nm * (1.0F - expmcr2 * poly) + sh_mask); } gmx_opencl_inline void calculate_lj_ewald_comb_LB_F_E(__constant const float* nbfp_comb_climg2d, const cl_nbparam_params_t* nbparam, int typei, int typej, float r2, float inv_r2, float lje_coeff2, float lje_coeff6_6, float int_bit, bool with_E_lj, float* F_invr, float* E_lj) { const float sigma = nbfp_comb_climg2d[2 * typei] + nbfp_comb_climg2d[2 * typej]; const float epsilon = nbfp_comb_climg2d[2 * typei + 1] * nbfp_comb_climg2d[2 * typej + 1]; const float sigma2 = sigma * sigma; const float c6grid = epsilon * sigma2 * sigma2 * sigma2; const float inv_r6_nm = inv_r2 * inv_r2 * inv_r2; const float cr2 = lje_coeff2 * r2; const float expmcr2 = exp(-cr2); const float poly = 1.0F + cr2 + HALF_F * cr2 * cr2; *F_invr += c6grid * (inv_r6_nm - expmcr2 * (inv_r6_nm * poly + lje_coeff6_6)) * inv_r2; if (with_E_lj) { const float sh_mask = nbparam->sh_lj_ewald * int_bit; *E_lj += ONE_SIXTH_F * c6grid * (inv_r6_nm * (1.0F - expmcr2 * poly) + sh_mask); } } gmx_opencl_inline float interpolate_coulomb_force_r(__constant const float* coulomb_tab_climg2d, float r, float scale) { float normalized = scale * r; int index = (int)normalized; float fract2 = normalized - (float)index; float fract1 = 1.0F - fract2; return fract1 * coulomb_tab_climg2d[index] + fract2 * coulomb_tab_climg2d[index + 1]; } gmx_opencl_inline float pmecorrF(float z2) { const float FN6 = -1.7357322914161492954e-8F; const float FN5 = 1.4703624142580877519e-6F; const float FN4 = -0.000053401640219807709149F; const float FN3 = 0.0010054721316683106153F; const float FN2 = -0.019278317264888380590F; const float FN1 = 0.069670166153766424023F; const float FN0 = -0.75225204789749321333F; const float FD4 = 0.0011193462567257629232F; const float FD3 = 0.014866955030185295499F; const float FD2 = 0.11583842382862377919F; const float FD1 = 0.50736591960530292870F; const float FD0 = 1.0F; const float z4 = z2 * z2; float polyFD0 = FD4 * z4 + FD2; float polyFD1 = FD3 * z4 + FD1; polyFD0 = polyFD0 * z4 + FD0; polyFD0 = polyFD1 * z2 + polyFD0; polyFD0 = 1.0F / polyFD0; float polyFN0 = FN6 * z4 + FN4; float polyFN1 = FN5 * z4 + FN3; polyFN0 = polyFN0 * z4 + FN2; polyFN1 = polyFN1 * z4 + FN1; polyFN0 = polyFN0 * z4 + FN0; polyFN0 = polyFN1 * z2 + polyFN0; return polyFN0 * polyFD0; } # if REDUCE_SHUFFLE gmx_opencl_inline void reduce_force_j_shfl(float3 fin, __global float* fout, int gmx_unused tidxi, int gmx_unused tidxj, int aidx) { fin.x += intel_sub_group_shuffle_down(fin.x, fin.x, 1); fin.y += intel_sub_group_shuffle_up(fin.y, fin.y, 1); fin.z += intel_sub_group_shuffle_down(fin.z, fin.z, 1); if ((tidxi & 1) == 1) { fin.x = fin.y; } fin.x += intel_sub_group_shuffle_down(fin.x, fin.x, 2); fin.z += intel_sub_group_shuffle_up(fin.z, fin.z, 2); if (tidxi == 2) { fin.x = fin.z; } if (tidxi < 3) { atomicAdd_g_f(&fout[3 * aidx + tidxi], fin.x); } } # endif gmx_opencl_inline void reduce_force_j_generic(__local float* f_buf, float3 fcj_buf, __global float* fout, int tidxi, int tidxj, int aidx) { int tidx = tidxi + tidxj * CL_SIZE; f_buf[tidx] = fcj_buf.x; f_buf[FBUF_STRIDE + tidx] = fcj_buf.y; f_buf[2 * FBUF_STRIDE + tidx] = fcj_buf.z; if (tidxi < 3) { float f = 0.0F; for (int j = tidxj * CL_SIZE; j < (tidxj + 1) * CL_SIZE; j++) { f += f_buf[FBUF_STRIDE * tidxi + j]; } atomicAdd_g_f(&fout[3 * aidx + tidxi], f); } } gmx_opencl_inline void reduce_force_j(__local float gmx_unused* f_buf, float3 fcj_buf, __global float* fout, int tidxi, int tidxj, int aidx) { # if REDUCE_SHUFFLE reduce_force_j_shfl(fcj_buf, fout, tidxi, tidxj, aidx); # else reduce_force_j_generic(f_buf, fcj_buf, fout, tidxi, tidxj, aidx); # endif } # if REDUCE_SHUFFLE gmx_opencl_inline void reduce_force_i_and_shift_shfl(float3* fci_buf, __global float* fout, bool bCalcFshift, int tidxi, int tidxj, int sci, int shift, __global float* fshift) { float2 fshift_buf = 0; for (int ci_offset = 0; ci_offset < c_nbnxnGpuNumClusterPerSupercluster; ci_offset++) { int aidx = (sci * c_nbnxnGpuNumClusterPerSupercluster + ci_offset) * CL_SIZE + tidxi; float3 fin = fci_buf[ci_offset]; fin.x += intel_sub_group_shuffle_down(fin.x, fin.x, CL_SIZE); fin.y += intel_sub_group_shuffle_up(fin.y, fin.y, CL_SIZE); fin.z += intel_sub_group_shuffle_down(fin.z, fin.z, CL_SIZE); if (tidxj & 1) { fin.x = fin.y; } atomicAdd_g_f(&fout[3 * aidx + (tidxj & 1)], fin.x); if (bCalcFshift) { fshift_buf[0] += fin.x; } if ((tidxj & 1) == 0) { atomicAdd_g_f(&fout[3 * aidx + 2], fin.z); if (bCalcFshift) { fshift_buf[1] += fin.z; } } } if (bCalcFshift) { atomicAdd_g_f(&(fshift[3 * shift + (tidxj & 1)]), fshift_buf[0]); if ((tidxj & 1) == 0) { atomicAdd_g_f(&(fshift[3 * shift + 2]), fshift_buf[1]); } } } # endif gmx_opencl_inline void reduce_force_i_and_shift_pow2(volatile __local float* f_buf, float3* fci_buf, __global float* fout, bool bCalcFshift, int tidxi, int tidxj, int sci, int shift, __global float* fshift) { float fshift_buf = 0; for (int ci_offset = 0; ci_offset < c_nbnxnGpuNumClusterPerSupercluster; ci_offset++) { int aidx = (sci * c_nbnxnGpuNumClusterPerSupercluster + ci_offset) * CL_SIZE + tidxi; int tidx = tidxi + tidxj * CL_SIZE; f_buf[tidx] = fci_buf[ci_offset].x; f_buf[FBUF_STRIDE + tidx] = fci_buf[ci_offset].y; f_buf[2 * FBUF_STRIDE + tidx] = fci_buf[ci_offset].z; barrier(CLK_LOCAL_MEM_FENCE); int i = CL_SIZE / 2; for (int j = CL_SIZE_LOG2 - 1; j > 0; j--) { if (tidxj < i) { f_buf[tidxj * CL_SIZE + tidxi] += f_buf[(tidxj + i) * CL_SIZE + tidxi]; f_buf[FBUF_STRIDE + tidxj * CL_SIZE + tidxi] += f_buf[FBUF_STRIDE + (tidxj + i) * CL_SIZE + tidxi]; f_buf[2 * FBUF_STRIDE + tidxj * CL_SIZE + tidxi] += f_buf[2 * FBUF_STRIDE + (tidxj + i) * CL_SIZE + tidxi]; } i >>= 1; } barrier(CLK_LOCAL_MEM_FENCE); if (tidxj < 3) { float f = f_buf[tidxj * FBUF_STRIDE + tidxi] + f_buf[tidxj * FBUF_STRIDE + i * CL_SIZE + tidxi]; atomicAdd_g_f(&fout[3 * aidx + tidxj], f); if (bCalcFshift) { fshift_buf += f; } } } if (bCalcFshift) { if (tidxj < 3) { atomicAdd_g_f(&(fshift[3 * shift + tidxj]), fshift_buf); } } } gmx_opencl_inline void reduce_force_i_and_shift(__local float gmx_unused* f_buf, float3* fci_buf, __global float* f, bool bCalcFshift, int tidxi, int tidxj, int sci, int shift, __global float* fshift) { # if REDUCE_SHUFFLE reduce_force_i_and_shift_shfl(fci_buf, f, bCalcFshift, tidxi, tidxj, sci, shift, fshift); # else reduce_force_i_and_shift_pow2(f_buf, fci_buf, f, bCalcFshift, tidxi, tidxj, sci, shift, fshift); # endif } # if REDUCE_SHUFFLE gmx_opencl_inline void reduce_energy_shfl(float E_lj, float E_el, volatile __global float* e_lj, volatile __global float* e_el, int tidx) { E_lj = sub_group_reduce_add(E_lj); E_el = sub_group_reduce_add(E_el); if (tidx == 0 || tidx == WARP_SIZE) { atomicAdd_g_f(e_lj, E_lj); atomicAdd_g_f(e_el, E_el); } } # endif gmx_opencl_inline void reduce_energy_pow2(volatile __local float* buf, volatile __global float* e_lj, volatile __global float* e_el, int tidx) { int i = WARP_SIZE / 2; for (int j = WARP_SIZE_LOG2 - 1; j > 0; j--) { if (tidx < i) { buf[tidx] += buf[tidx + i]; buf[FBUF_STRIDE + tidx] += buf[FBUF_STRIDE + tidx + i]; } i >>= 1; } if (tidx == 0) { float e1 = buf[tidx] + buf[tidx + i]; float e2 = buf[FBUF_STRIDE + tidx] + buf[FBUF_STRIDE + tidx + i]; atomicAdd_g_f(e_lj, e1); atomicAdd_g_f(e_el, e2); } } gmx_opencl_inline void reduce_energy(volatile __local float gmx_unused* buf, float E_lj, float E_el, volatile __global float* e_lj, volatile __global float* e_el, int tidx) { # if REDUCE_SHUFFLE reduce_energy_shfl(E_lj, E_el, e_lj, e_el, tidx); # else buf[tidx] = E_lj; buf[FBUF_STRIDE + tidx] = E_el; reduce_energy_pow2(buf + (tidx & WARP_SIZE), e_lj, e_el, tidx & ~WARP_SIZE); # endif } gmx_opencl_inline bool gmx_sub_group_any_localmem(volatile __local int* warp_any, int widx, bool pred) { if (pred) { warp_any[widx] = 1; } bool ret = warp_any[widx]; warp_any[widx] = 0; return ret; } gmx_opencl_inline bool gmx_sub_group_any(volatile __local int gmx_unused* warp_any, int gmx_unused widx, bool pred) { # if USE_SUBGROUP_ANY return sub_group_any(pred); # else return gmx_sub_group_any_localmem(warp_any, widx, pred); # endif } #endif