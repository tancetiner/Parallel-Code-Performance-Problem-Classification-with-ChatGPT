#include <color_spinor_field_order.h> #include <gauge_field_order.h> #include <clover_field_order.h> #include <multigrid_helper.cuh> #include <index_helper.cuh> #include <gamma.cuh> #include <linalg.cuh> #include <matrix_tile.cuh> namespace quda { typedef int storeType; template <typename Float_, int fineSpin, int coarseSpin, int fineColor, int coarseColor, typename coarseGauge, typename coarseGaugeAtomic, typename fineGauge, typename fineSpinor, typename fineSpinorTmp, typename fineSpinorV, typename fineClover> struct CalculateYArg { using Float = Float_; coarseGauge Y; coarseGauge X; coarseGaugeAtomic Y_atomic; coarseGaugeAtomic X_atomic; fineSpinorTmp UV; fineSpinor AV; const fineGauge U; const fineSpinorV V; const fineClover C; const fineClover Cinv; int_fastdiv x_size[QUDA_MAX_DIM]; int xc_size[QUDA_MAX_DIM]; int_fastdiv geo_bs[QUDA_MAX_DIM]; const int spin_bs; const spin_mapper<fineSpin,coarseSpin> spin_map; int comm_dim[QUDA_MAX_DIM]; Float kappa; Float mu; Float mu_factor; Float rescale; const int fineVolumeCB; const int coarseVolumeCB; const int *fine_to_coarse; const int *coarse_to_fine; const bool bidirectional; static constexpr bool coarse_color_wave = true; bool shared_atomic; bool parity_flip; int_fastdiv aggregates_per_block; int_fastdiv grid_z; int_fastdiv coarse_color_grid_z; Float max_h; Float *max_d; int dim; QudaDirection dir; int dim_index; static constexpr int tile_height_uv = fineColor % 4 == 0 ? 4 : fineColor % 3 == 0 ? 3 : fineColor % 2 ? 2 : 1; static constexpr int tile_width_uv = coarseColor % 2 == 0 ? 2 : 1; TileSize<fineColor, coarseColor, fineColor, tile_height_uv, tile_width_uv, 1> uvTile; static constexpr int tile_height_vuv = (coarseColor % 4 == 0 && fineSpin == 4) ? 4 : coarseColor % 3 == 0 ? 3 : 2; static constexpr int tile_width_vuv = coarseColor % 2 == 0 ? 2 : 1; TileSize<coarseColor, coarseColor, fineColor, tile_height_vuv, tile_width_vuv, 1> vuvTile; static constexpr int max_color_height_per_block = coarseColor < 8 ? coarseColor : ((8 + tile_height_vuv - 1) / tile_height_vuv) * tile_height_vuv; static constexpr int max_color_width_per_block = coarseColor < 8 ? coarseColor : ((8 + tile_width_vuv - 1) / tile_width_vuv) * tile_width_vuv; static constexpr int max_height_tiles_per_block = max_color_height_per_block / tile_height_vuv; static constexpr int max_width_tiles_per_block = max_color_width_per_block / tile_width_vuv; static_assert(max_color_height_per_block % tile_height_vuv == 0, "max_color_height_per_block must be divisible by tile height"); static_assert(max_color_width_per_block % tile_width_vuv == 0, "max_color_width_per_block must be divisible by tile width"); CalculateYArg(coarseGauge &Y, coarseGauge &X, coarseGaugeAtomic &Y_atomic, coarseGaugeAtomic &X_atomic, fineSpinorTmp &UV, fineSpinor &AV, const fineGauge &U, const fineSpinorV &V, const fineClover &C, const fineClover &Cinv, double kappa, double mu, double mu_factor, const int *x_size_, const int *xc_size_, int *geo_bs_, int spin_bs_, const int *fine_to_coarse, const int *coarse_to_fine, bool bidirectional) : Y(Y), X(X), Y_atomic(Y_atomic), X_atomic(X_atomic), UV(UV), AV(AV), U(U), V(V), C(C), Cinv(Cinv), spin_bs(spin_bs_), spin_map(), kappa(static_cast<Float>(kappa)), mu(static_cast<Float>(mu)), mu_factor(static_cast<Float>(mu_factor)), fineVolumeCB(V.VolumeCB()), coarseVolumeCB(X.VolumeCB()), fine_to_coarse(fine_to_coarse), coarse_to_fine(coarse_to_fine), bidirectional(bidirectional), shared_atomic(false), parity_flip(shared_atomic ? true : false), aggregates_per_block(1), max_d(nullptr) { if (V.GammaBasis() != QUDA_DEGRAND_ROSSI_GAMMA_BASIS) errorQuda("Gamma basis %d not supported", V.GammaBasis()); for (int i=0; i<QUDA_MAX_DIM; i++) { x_size[i] = x_size_[i]; xc_size[i] = xc_size_[i]; geo_bs[i] = geo_bs_[i]; comm_dim[i] = comm_dim_partitioned(i); } } }; template<bool from_coarse, int dim, QudaDirection dir, int fineSpin, int coarseSpin, typename Wtype, typename Arg> __device__ __host__ inline void computeUV(Arg &arg, const Wtype &Wacc, int parity, int x_cb, int i0, int j0) { int coord[4]; getCoords(coord, x_cb, arg.x_size, parity); constexpr int uvSpin = fineSpin * (from_coarse ? 2 : 1); constexpr int nFace = 1; using complex = complex<typename Arg::Float>; auto &tile = arg.uvTile; using Ctype = decltype(make_tile_C<complex, false>(tile)); Ctype UV[uvSpin]; if ( arg.comm_dim[dim] && (coord[dim] + nFace >= arg.x_size[dim]) ) { int ghost_idx = ghostFaceIndex<1>(coord, arg.x_size, dim, nFace); if (!from_coarse) { for (int k = 0; k < tile.k; k+=tile.K) { auto U = make_tile_A<complex, false>(tile); U.load(arg.U, dim, parity, x_cb, i0, k); for (int s = 0; s < fineSpin; s++) { auto W = make_tile_B<complex, true>(tile); W.loadCS(Wacc, dim, 1, (parity+1)&1, ghost_idx, s, k, j0); UV[s].mma_nn(U, W); } } } else { for (int k = 0; k < tile.k; k+=tile.K) { for (int s_col=0; s_col<fineSpin; s_col++) { auto W = make_tile_B<complex, true>(tile); W.loadCS(Wacc, dim, 1, (parity+1)&1, ghost_idx, s_col, k, j0); for (int s = 0; s < fineSpin; s++) { auto U = make_tile_A<complex, false>(tile); U.load(arg.U, dim + (dir == QUDA_FORWARDS ? 4 : 0), parity, x_cb, s, s_col, i0, k); UV[s_col * fineSpin + s].mma_nn(U, W); } } } } } else { int y_cb = linkIndexP1(coord, arg.x_size, dim); if (!from_coarse) { for (int k = 0; k < tile.k; k+=tile.K) { auto U = make_tile_A<complex, false>(tile); U.load(arg.U, dim, parity, x_cb, i0, k); for (int s = 0; s < fineSpin; s++) { auto W = make_tile_B<complex, false>(tile); W.loadCS(Wacc, 0, 0, (parity+1)&1, y_cb, s, k, j0); UV[s].mma_nn(U, W); } } } else { for (int k = 0; k < tile.k; k+=tile.K) { for (int s_col = 0; s_col < fineSpin; s_col++) { auto W = make_tile_B<complex, false>(tile); W.loadCS(Wacc, 0, 0, (parity+1)&1, y_cb, s_col, k, j0); for (int s = 0; s < fineSpin; s++) { auto U = make_tile_A<complex, false>(tile); U.load(arg.U, dim + (dir == QUDA_FORWARDS ? 4 : 0), parity, x_cb, s, s_col, i0, k); UV[s_col * fineSpin + s].mma_nn(U, W); } } } } } for (int s = 0; s < uvSpin; s++) UV[s].saveCS(arg.UV, 0, 0, parity, x_cb, s, i0, j0); } template<bool from_coarse, typename Float, int dim, QudaDirection dir, int fineSpin, int coarseSpin, typename Arg> void ComputeUVCPU(Arg &arg) { auto &tile = arg.uvTile; for (int parity=0; parity<2; parity++) { #pragma omp parallel for for (int x_cb=0; x_cb<arg.fineVolumeCB; x_cb++) { for (int ic=0; ic < tile.m; ic += tile.M) for (int jc=0; jc < tile.n; jc += tile.N) if (dir == QUDA_FORWARDS) computeUV<from_coarse,dim,dir,fineSpin,coarseSpin>(arg, arg.V, parity, x_cb, ic, jc); else computeUV<from_coarse,dim,dir,fineSpin,coarseSpin>(arg, arg.AV, parity, x_cb, ic, jc); } } } template<bool from_coarse, typename Float, int dim, QudaDirection dir, int fineSpin, int coarseSpin, typename Arg> __global__ void ComputeUVGPU(Arg arg) { int x_cb = blockDim.x*blockIdx.x + threadIdx.x; if (x_cb >= arg.fineVolumeCB) return; int ic_parity = blockDim.y*blockIdx.y + threadIdx.y; if (ic_parity >= 2*arg.uvTile.M_tiles) return; int ic = ic_parity % arg.uvTile.M_tiles; int parity = ic_parity / arg.uvTile.M_tiles; int jc = blockDim.z*blockIdx.z + threadIdx.z; if (jc >= arg.uvTile.N_tiles) return; if (dir == QUDA_FORWARDS) computeUV<from_coarse,dim,dir,fineSpin,coarseSpin>(arg, arg.V, parity, x_cb, ic * arg.uvTile.M, jc * arg.uvTile.N); else computeUV<from_coarse,dim,dir,fineSpin,coarseSpin>(arg, arg.AV, parity, x_cb, ic * arg.uvTile.M, jc * arg.uvTile.N); } template <typename Float, int fineSpin, int fineColor, int coarseColor, typename Arg> __device__ __host__ inline void computeAV(Arg &arg, int parity, int x_cb, int ch, int ic_c) { constexpr int N = fineSpin * fineColor / 2; HMatrix<Float, N> A; #pragma unroll for (int i = 0; i < N; i++) { int s_i = 2 * ch + i / fineColor; int c_i = i % fineColor; #pragma unroll for (int j = 0; j <= i; j++) { int s_j = 2 * ch + j / fineColor; int c_j = j % fineColor; #ifndef DYNAMIC_CLOVER A(i, j) = arg.Cinv(0, parity, x_cb, s_i, s_j, c_i, c_j); #else A(i, j) = arg.C(0, parity, x_cb, s_i, s_j, c_i, c_j); #endif } } ColorSpinor<Float, fineColor, fineSpin / 2> V; for (int s = 0; s < fineSpin / 2; s++) { for (int c = 0; c < fineColor; c++) { V(s, c) = arg.V(parity, x_cb, 2 * ch + s, c, ic_c); } } #ifndef DYNAMIC_CLOVER auto AV = A * V; #else linalg::Cholesky<HMatrix, Float, N> cholesky(A); auto AV = cholesky.backward(cholesky.forward(V)); #endif #pragma unroll for (int s = 0; s < fineSpin / 2; s++) { #pragma unroll for (int ic = 0; ic < fineColor; ic++) { arg.AV(parity, x_cb, 2 * ch + s, ic, ic_c) = AV(s, ic); } } } template <typename Float, int fineSpin, int fineColor, int coarseColor, typename Arg> void ComputeAVCPU(Arg &arg) { for (int parity=0; parity<2; parity++) { #pragma omp parallel for for (int x_cb=0; x_cb<arg.fineVolumeCB; x_cb++) { for (int ch = 0; ch < 2; ch++) { for (int ic_c = 0; ic_c < coarseColor; ic_c++) { computeAV<Float, fineSpin, fineColor, coarseColor>(arg, parity, x_cb, ch, ic_c); } } } } } template <typename Float, int fineSpin, int fineColor, int coarseColor, typename Arg> __global__ void ComputeAVGPU(Arg arg) { int x_cb = blockDim.x*blockIdx.x + threadIdx.x; if (x_cb >= arg.fineVolumeCB) return; int ch_parity = blockDim.y * blockIdx.y + threadIdx.y; if (ch_parity >= 4) return; int ch = ch_parity % 2; int parity = ch_parity / 2; int ic_c = blockDim.z * blockIdx.z + threadIdx.z; if (ic_c >= coarseColor) return; if (ch == 0) computeAV<Float, fineSpin, fineColor, coarseColor>(arg, parity, x_cb, 0, ic_c); else computeAV<Float, fineSpin, fineColor, coarseColor>(arg, parity, x_cb, 1, ic_c); } template<typename Float, int fineSpin, int fineColor, int coarseColor, typename Arg> __device__ __host__ inline void computeTMAV(Arg &arg, int parity, int x_cb, int v) { complex<Float> fp(1./(1.+arg.mu*arg.mu),-arg.mu/(1.+arg.mu*arg.mu)); complex<Float> fm(1./(1.+arg.mu*arg.mu),+arg.mu/(1.+arg.mu*arg.mu)); for(int s = 0; s < fineSpin/2; s++) { for(int c = 0; c < fineColor; c++) { arg.AV(parity,x_cb,s,c,v) = arg.V(parity,x_cb,s,c,v)*fp; } } for(int s = fineSpin/2; s < fineSpin; s++) { for(int c = 0; c < fineColor; c++) { arg.AV(parity,x_cb,s,c,v) = arg.V(parity,x_cb,s,c,v)*fm; } } } template<typename Float, int fineSpin, int fineColor, int coarseColor, typename Arg> void ComputeTMAVCPU(Arg &arg) { for (int parity=0; parity<2; parity++) { #pragma omp parallel for for (int x_cb=0; x_cb<arg.fineVolumeCB; x_cb++) { for (int v=0; v<coarseColor; v++) computeTMAV<Float,fineSpin,fineColor,coarseColor,Arg>(arg, parity, x_cb, v); } } } template<typename Float, int fineSpin, int fineColor, int coarseColor, typename Arg> __global__ void ComputeTMAVGPU(Arg arg) { int x_cb = blockDim.x*blockIdx.x + threadIdx.x; if (x_cb >= arg.fineVolumeCB) return; int parity = blockDim.y*blockIdx.y + threadIdx.y; int v = blockDim.z*blockIdx.z + threadIdx.z; if (v >= coarseColor) return; computeTMAV<Float,fineSpin,fineColor,coarseColor,Arg>(arg, parity, x_cb, v); } #ifdef DYNAMIC_CLOVER template <typename Float, bool twist, typename Arg> __device__ __host__ inline Float computeCloverInvMax(Arg &arg, int parity, int x_cb) { Float max = 0.0; constexpr int nColor = 3; constexpr int nSpin = 4; constexpr int N = nColor * nSpin / 2; typedef HMatrix<Float, N> Mat; #pragma unroll for (int ch = 0; ch < 2; ch++) { Mat A; #pragma unroll for (int i = 0; i < N; i++) { #pragma unroll for (int j = 0; j <= i; j++) { A(i, j) = arg.C(0, parity, x_cb, 2 * ch + i / nColor, 2 * ch + j / nColor, i % nColor, j % nColor); } } if (twist) { A = A.square(); A += arg.mu * arg.mu; } linalg::Cholesky<HMatrix, Float, N> cholesky(A); Mat Ainv = cholesky.invert(); Float inv_max = Ainv.max(); max = max > inv_max ? max : inv_max; } return max; } template <typename Float, bool twist, typename Arg> void ComputeCloverInvMaxCPU(Arg &arg) { Float max = 0.0; for (int parity=0; parity<2; parity++) { #pragma omp parallel for reduction(max:max) for (int x_cb=0; x_cb<arg.fineVolumeCB; x_cb++) { Float max_x = computeCloverInvMax<Float, twist, Arg>(arg, parity, x_cb); max = max > max_x ? max : max_x; } } arg.max_h = max; } template <typename Float, bool twist, typename Arg> __global__ void ComputeCloverInvMaxGPU(Arg arg) { int x_cb = blockDim.x*blockIdx.x + threadIdx.x; if (x_cb >= arg.fineVolumeCB) return; int parity = blockDim.y*blockIdx.y + threadIdx.y; arg.max_d[parity + 2 * x_cb] = computeCloverInvMax<Float, twist, Arg>(arg, parity, x_cb); } #endif template <typename Float, int fineSpin, int fineColor, int coarseColor, typename Arg> __device__ __host__ inline void computeTMCAV(Arg &arg, int parity, int x_cb, int ch, int ic_c) { constexpr int N = fineSpin * fineColor / 2; HMatrix<Float, N> A; #pragma unroll for (int i = 0; i < N; i++) { int s_i = 2 * ch + i / fineColor; int c_i = i % fineColor; #pragma unroll for (int j = 0; j <= i; j++) { int s_j = 2 * ch + j / fineColor; int c_j = j % fineColor; A(i, j) = arg.C(0, parity, x_cb, s_i, s_j, c_i, c_j); } } complex<Float> mu(0., arg.mu); if (ch == 0) mu *= static_cast<Float>(-1.0); ColorSpinor<Float, fineColor, fineSpin / 2> V; for (int s = 0; s < fineSpin / 2; s++) { for (int c = 0; c < fineColor; c++) { V(s, c) = arg.V(parity, x_cb, 2 * ch + s, c, ic_c); } } auto UV = A * V; UV += mu * V; #ifndef DYNAMIC_CLOVER HMatrix<Float, N> Ainv; #pragma unroll for (int i = 0; i < N; i++) { int s_i = 2 * ch + i / fineColor; int c_i = i % fineColor; #pragma unroll for (int j = 0; j <= i; j++) { int s_j = 2 * ch + j / fineColor; int c_j = j % fineColor; Ainv(i, j) = arg.Cinv(0, parity, x_cb, s_i, s_j, c_i, c_j); } } auto AV = Ainv * UV; #else A = A.square(); A += arg.mu * arg.mu; linalg::Cholesky<HMatrix, Float, N> cholesky(A); const auto AV = cholesky.backward(cholesky.forward(UV)); #endif for (int s = 0; s < fineSpin / 2; s++) for (int c = 0; c < fineColor; c++) arg.AV(parity, x_cb, 2 * ch + s, c, ic_c) = AV(s, c); } template <typename Float, int fineSpin, int fineColor, int coarseColor, typename Arg> void ComputeTMCAVCPU(Arg &arg) { for (int parity = 0; parity < 2; parity++) { #pragma omp parallel for for (int x_cb=0; x_cb<arg.fineVolumeCB; x_cb++) { for (int ch = 0; ch < 2; ch++) { for (int ic_c = 0; ic_c < coarseColor; ic_c++) { computeTMCAV<Float, fineSpin, fineColor, coarseColor, Arg>(arg, parity, x_cb, ch, ic_c); } } } } } template <typename Float, int fineSpin, int fineColor, int coarseColor, typename Arg> __global__ void ComputeTMCAVGPU(Arg arg) { int x_cb = blockDim.x*blockIdx.x + threadIdx.x; if (x_cb >= arg.fineVolumeCB) return; int ch_parity = blockDim.y * blockIdx.y + threadIdx.y; if (ch_parity >= 4) return; int ch = ch_parity % 2; int parity = ch_parity / 2; int ic_c = blockDim.z * blockIdx.z + threadIdx.z; if (ic_c >= coarseColor) return; if (ch == 0) computeTMCAV<Float, fineSpin, fineColor, coarseColor, Arg>(arg, parity, x_cb, 0, ic_c); else computeTMCAV<Float, fineSpin, fineColor, coarseColor, Arg>(arg, parity, x_cb, 1, ic_c); } template<typename Arg> __device__ __host__ inline int virtualThreadIdx(const Arg &arg) { constexpr int warp_size = 32; int warp_id = threadIdx.x / warp_size; int warp_lane = threadIdx.x % warp_size; int tx = warp_id * (warp_size / arg.aggregates_per_block) + warp_lane / arg.aggregates_per_block; return tx; } template<typename Arg> __device__ __host__ inline int virtualBlockDim(const Arg &arg) { int block_dim_x = blockDim.x / arg.aggregates_per_block; return block_dim_x; } template<typename Arg> __device__ __host__ inline int coarseIndex(const Arg &arg) { constexpr int warp_size = 32; int warp_lane = threadIdx.x % warp_size; int x_coarse = (arg.coarse_color_wave ? blockIdx.y : blockIdx.x)*arg.aggregates_per_block + warp_lane % arg.aggregates_per_block; return x_coarse; } template <bool from_coarse, typename Float, int dim, QudaDirection dir, int fineSpin, int coarseSpin, typename Arg, typename Gamma, typename Out> __device__ __host__ inline void multiplyVUV(Out &vuv, const Arg &arg, const Gamma &gamma, int parity, int x_cb, int i0, int j0) { using complex = complex<Float>; auto &tile = arg.vuvTile; if (!from_coarse) { #pragma unroll for (int s = 0; s < fineSpin; s++) { const int s_c_row = arg.spin_map(s,parity); const int s_col = gamma.getcol(s); const int s_c_col = arg.spin_map(s_col,parity); #pragma unroll for (int k = 0; k < tile.k; k+=tile.K) { if (dir == QUDA_BACKWARDS) { auto V = make_tile_At<complex, false>(tile); V.loadCS(arg.V, 0, 0, parity, x_cb, s, k, i0); auto UV = make_tile_B<complex, false>(tile); UV.loadCS(arg.UV, 0, 0, parity, x_cb, s, k, j0); vuv[s_c_row*coarseSpin+s_c_row].mma_tn(V, UV); auto gammaV = make_tile_A<complex, false>(tile); for (int i=0; i<tile.K; i++) for (int j=0; j<tile.M; j++) { gammaV(j,i) = gamma.apply(s, conj(V(i,j))); } UV.loadCS(arg.UV, 0, 0, parity, x_cb, s_col, k, j0); vuv[s_c_row*coarseSpin+s_c_col].mma_nn(gammaV, UV); } else { auto AV = make_tile_At<complex, false>(tile); AV.loadCS(arg.AV, 0, 0, parity, x_cb, s, k, i0); auto UV = make_tile_B<complex, false>(tile); UV.loadCS(arg.UV, 0, 0, parity, x_cb, s, k, j0); vuv[s_c_row*coarseSpin+s_c_row].mma_tn(AV, UV); auto gammaAV = make_tile_A<complex, false>(tile); for (int i=0; i<tile.K; i++) for (int j=0; j<tile.M; j++) { gammaAV(j,i) = -gamma.apply(s, conj(AV(i,j))); } UV.loadCS(arg.UV, 0, 0, parity, x_cb, s_col, k, j0); vuv[s_c_row*coarseSpin+s_c_col].mma_nn(gammaAV, UV); } } } } else { #pragma unroll for(int k = 0; k < tile.k; k+=tile.K) { #pragma unroll for (int s = 0; s < fineSpin; s++) { auto AV = make_tile_At<complex, false>(tile); AV.loadCS(arg.AV, 0, 0, parity, x_cb, s, k, i0); #pragma unroll for (int s_col=0; s_col<fineSpin; s_col++) { auto UV = make_tile_B<complex, false>(tile); UV.loadCS(arg.UV, 0, 0, parity, x_cb, s_col*fineSpin+s, k, j0); vuv[s*coarseSpin+s_col].mma_tn(AV, UV); } } } } } template <typename Float, typename storeType, typename Accessor> inline __host__ __device__ void atomic_helper(complex<storeType> *Y, const Accessor &A, const complex<Float> &vuv) { if (gauge::fixed_point<Float,storeType>()) { Float scale = A.accessor.scale; complex<storeType> a(round(scale * vuv.real()), round(scale * vuv.imag())); atomicAdd(Y,a); } else { atomicAdd(Y,reinterpret_cast<const complex<storeType>&>(vuv)); } } template <bool parity_flip, typename Float, QudaDirection dir, int coarseSpin, typename VUV, typename Arg> inline __device__ __host__ void storeCoarseSharedAtomic(VUV &vuv, bool isDiagonal, int coarse_x_cb, int coarse_parity, int i0, int j0, int parity, Arg &arg) { #ifdef __CUDA_ARCH__ const int dim_index = arg.dim_index % arg.Y_atomic.geometry; __shared__ complex<storeType> X[Arg::max_color_height_per_block][Arg::max_color_width_per_block][4][coarseSpin][coarseSpin]; __shared__ complex<storeType> Y[Arg::max_color_height_per_block][Arg::max_color_width_per_block][4][coarseSpin][coarseSpin]; int x_ = coarse_x_cb%arg.aggregates_per_block; int tx = virtualThreadIdx(arg); int s_col = tx / coarseSpin; int s_row = tx % coarseSpin; auto &tile = arg.vuvTile; int i_block0 = threadIdx.y * tile.M * (!parity_flip ? 2 : 1); int j_block0 = threadIdx.z * tile.N; #pragma unroll for (int i=0; i<tile.M; i++) { #pragma unroll for (int j=0; j<tile.N; j++) { if (tx < coarseSpin*coarseSpin) { Y[i_block0+i][j_block0+j][x_][s_row][s_col] = 0; X[i_block0+i][j_block0+j][x_][s_row][s_col] = 0; } } } __syncthreads(); #pragma unroll for (int i=0; i<tile.M; i++) { #pragma unroll for (int j=0; j<tile.N; j++) { if (!isDiagonal) { #pragma unroll for (int s_row = 0; s_row < coarseSpin; s_row++) { #pragma unroll for (int s_col = 0; s_col < coarseSpin; s_col++) { atomic_helper<Float, storeType>(&Y[i_block0+i][j_block0+j][x_][s_row][s_col], arg.Y_atomic, vuv[s_row*coarseSpin+s_col](i,j)); } } } else { #pragma unroll for (int s_row = 0; s_row < coarseSpin; s_row++) { #pragma unroll for (int s_col = 0; s_col < coarseSpin; s_col++) { atomic_helper<Float, storeType>(&X[i_block0+i][j_block0+j][x_][s_row][s_col], arg.X_atomic, vuv[s_row*coarseSpin+s_col](i,j)); } } } } } __syncthreads(); if (tx < coarseSpin*coarseSpin && (parity == 0 || parity_flip == 1) ) { #pragma unroll for (int i=0; i<tile.M; i++) { #pragma unroll for (int j=0; j<tile.N; j++) { arg.Y_atomic.atomicAdd(dim_index,coarse_parity,coarse_x_cb,s_row,s_col,i0+i,j0+j, Y[i_block0+i][j_block0+j][x_][s_row][s_col]); if (dir == QUDA_BACKWARDS) { arg.X_atomic.atomicAdd(0,coarse_parity,coarse_x_cb,s_col,s_row,j0+j,i0+i, conj(X[i_block0+i][j_block0+j][x_][s_row][s_col])); } else { arg.X_atomic.atomicAdd(0,coarse_parity,coarse_x_cb,s_row,s_col,i0+i,j0+j, X[i_block0+i][j_block0+j][x_][s_row][s_col]); } if (!arg.bidirectional) { if (s_row == s_col) arg.X_atomic.atomicAdd(0,coarse_parity,coarse_x_cb,s_row,s_col,i0+i,j0+j, X[i_block0+i][j_block0+j][x_][s_row][s_col]); else arg.X_atomic.atomicAdd(0,coarse_parity,coarse_x_cb,s_row,s_col,i0+i,j0+j, -X[i_block0+i][j_block0+j][x_][s_row][s_col]); } } } } #else errorQuda("Shared-memory atomic aggregation not supported on CPU"); #endif } template <bool parity_flip, typename Float, QudaDirection dir, int coarseSpin, typename VUV, typename Arg> inline __device__ __host__ void storeCoarseGlobalAtomic(VUV &vuv, bool isDiagonal, int coarse_x_cb, int coarse_parity, int i0, int j0, Arg &arg) { const int dim_index = arg.dim_index % arg.Y_atomic.geometry; auto &tile = arg.vuvTile; if (!isDiagonal) { #pragma unroll for (int s_row = 0; s_row < coarseSpin; s_row++) { #pragma unroll for (int s_col = 0; s_col < coarseSpin; s_col++) { #pragma unroll for (int i=0; i<tile.M; i++) #pragma unroll for (int j=0; j<tile.N; j++) arg.Y_atomic.atomicAdd(dim_index,coarse_parity,coarse_x_cb,s_row,s_col,i0+i,j0+j,vuv[s_row*coarseSpin+s_col](i,j)); } } } else { if (dir == QUDA_BACKWARDS) { #pragma unroll for (int s_row = 0; s_row < coarseSpin; s_row++) { #pragma unroll for (int s_col = 0; s_col < coarseSpin; s_col++) { #pragma unroll for (int i=0; i<tile.M; i++) #pragma unroll for (int j=0; j<tile.N; j++) arg.X_atomic.atomicAdd(0,coarse_parity,coarse_x_cb,s_col,s_row,j0+j,i0+i,conj(vuv[s_row*coarseSpin+s_col](i,j))); } } } else { #pragma unroll for (int s_row = 0; s_row < coarseSpin; s_row++) { #pragma unroll for (int s_col = 0; s_col < coarseSpin; s_col++) { #pragma unroll for (int i=0; i<tile.M; i++) #pragma unroll for (int j=0; j<tile.N; j++) arg.X_atomic.atomicAdd(0,coarse_parity,coarse_x_cb,s_row,s_col,i0+i,j0+j,vuv[s_row*coarseSpin+s_col](i,j)); } } } if (!arg.bidirectional) { #pragma unroll for (int s_row = 0; s_row < coarseSpin; s_row++) { #pragma unroll for (int s_col = 0; s_col < coarseSpin; s_col++) { if (s_row != s_col) vuv[s_row * coarseSpin + s_col] *= static_cast<Float>(-1.0); #pragma unroll for (int i=0; i<tile.M; i++) #pragma unroll for (int j=0; j<tile.N; j++) arg.X_atomic.atomicAdd(0,coarse_parity,coarse_x_cb,s_row,s_col,i0+i,j0+j,vuv[s_row*coarseSpin+s_col](i,j)); } } } } } template<bool shared_atomic, bool parity_flip, bool from_coarse, typename Float, int dim, QudaDirection dir, int fineSpin, int coarseSpin, typename Arg, typename Gamma> __device__ __host__ void computeVUV(Arg &arg, const Gamma &gamma, int parity, int x_cb, int i0, int j0, int parity_coarse_, int coarse_x_cb_) { constexpr int nDim = 4; int coord[QUDA_MAX_DIM]; int coord_coarse[QUDA_MAX_DIM]; getCoords(coord, x_cb, arg.x_size, parity); for(int d = 0; d < nDim; d++) coord_coarse[d] = coord[d]/arg.geo_bs[d]; const bool isDiagonal = ((coord[dim]+1)%arg.x_size[dim])/arg.geo_bs[dim] == coord_coarse[dim] ? true : false; int coarse_parity = shared_atomic ? parity_coarse_ : 0; if (!shared_atomic) { for (int d=0; d<nDim; d++) coarse_parity += coord_coarse[d]; coarse_parity &= 1; coord_coarse[0] /= 2; } int coarse_x_cb = shared_atomic ? coarse_x_cb_ : ((coord_coarse[3]*arg.xc_size[2]+coord_coarse[2])*arg.xc_size[1]+coord_coarse[1])*(arg.xc_size[0]/2) + coord_coarse[0]; using Ctype = decltype(make_tile_C<complex<Float>, false>(arg.vuvTile)); Ctype vuv[coarseSpin * coarseSpin]; multiplyVUV<from_coarse,Float,dim,dir,fineSpin,coarseSpin,Arg>(vuv, arg, gamma, parity, x_cb, i0, j0); if (isDiagonal) { #pragma unroll for (int s2=0; s2<coarseSpin*coarseSpin; s2++) vuv[s2] *= -arg.kappa; } if (shared_atomic) storeCoarseSharedAtomic<parity_flip, Float, dir, coarseSpin>(vuv, isDiagonal, coarse_x_cb, coarse_parity, i0, j0, parity, arg); else storeCoarseGlobalAtomic<parity_flip, Float, dir, coarseSpin>(vuv, isDiagonal, coarse_x_cb, coarse_parity, i0, j0, arg); } template <bool shared_atomic, bool parity_flip, typename Arg> __device__ inline void getIndices(const Arg &arg, int &parity, int &x_cb, int &parity_coarse, int &x_coarse_cb, int &c_row, int &c_col) { if (arg.coarse_color_wave) { int parity_c_row_block_idx_z = blockDim.y*blockIdx.x + threadIdx.y; int c_row_block_idx_z = parity_flip ? (parity_c_row_block_idx_z % arg.coarse_color_grid_z ) : (parity_c_row_block_idx_z / 2); parity = parity_flip ? (parity_c_row_block_idx_z / arg.coarse_color_grid_z ) : (parity_c_row_block_idx_z % 2); c_row = c_row_block_idx_z % arg.vuvTile.M_tiles; int block_idx_z = c_row_block_idx_z / arg.vuvTile.M_tiles; c_col = blockDim.z*block_idx_z + threadIdx.z; } else { int parity_c_row = blockDim.y*blockIdx.y + threadIdx.y; c_row = parity_flip ? (parity_c_row % arg.vuvTile.M_tiles) : (parity_c_row / 2); parity = parity_flip ? (parity_c_row / arg.vuvTile.M_tiles) : (parity_c_row % 2); c_col = blockDim.z*blockIdx.z + threadIdx.z; } if (!shared_atomic) { x_cb = blockDim.x*(arg.coarse_color_wave ? blockIdx.y : blockIdx.x) + threadIdx.x; x_coarse_cb = 0; parity_coarse = 0; } else { int block_dim_x = virtualBlockDim(arg); int thread_idx_x = virtualThreadIdx(arg); int x_coarse = coarseIndex(arg); parity_coarse = x_coarse >= arg.coarseVolumeCB ? 1 : 0; x_coarse_cb = x_coarse - parity_coarse*arg.coarseVolumeCB; int x_fine = arg.coarse_to_fine[ (x_coarse*2 + parity) * block_dim_x + thread_idx_x]; x_cb = x_fine - parity*arg.fineVolumeCB; } } template<bool from_coarse, typename Float, int dim, QudaDirection dir, int fineSpin, int coarseSpin, typename Arg> void ComputeVUVCPU(Arg &arg) { Gamma<Float, QUDA_DEGRAND_ROSSI_GAMMA_BASIS, dim> gamma; constexpr bool shared_atomic = false; constexpr bool parity_flip = true; for (int parity=0; parity<2; parity++) { #pragma omp parallel for for (int x_cb=0; x_cb<arg.fineVolumeCB; x_cb++) { for (int ic=0; ic<arg.vuvTile.m; ic+=arg.vuvTile.M) for (int jc=0; jc<arg.vuvTile.n; jc+=arg.vuvTile.N) computeVUV<shared_atomic,parity_flip,from_coarse,Float,dim,dir,fineSpin,coarseSpin>(arg, gamma, parity, x_cb, ic, jc, 0, 0); } } } template<bool shared_atomic, bool parity_flip, bool from_coarse, typename Float, int dim, QudaDirection dir, int fineSpin, int coarseSpin, typename Arg> __global__ void ComputeVUVGPU(Arg arg) { Gamma<Float, QUDA_DEGRAND_ROSSI_GAMMA_BASIS, dim> gamma; int parity, x_cb, parity_coarse, x_coarse_cb, c_col, c_row; getIndices<shared_atomic,parity_flip>(arg, parity, x_cb, parity_coarse, x_coarse_cb, c_row, c_col); if (parity > 1) return; if (c_row >= arg.vuvTile.M_tiles) return; if (c_col >= arg.vuvTile.N_tiles) return; if (!shared_atomic && x_cb >= arg.fineVolumeCB) return; computeVUV<shared_atomic,parity_flip,from_coarse,Float,dim,dir,fineSpin,coarseSpin> (arg, gamma, parity, x_cb, c_row * arg.vuvTile.M, c_col * arg.vuvTile.N, parity_coarse, x_coarse_cb); } template<typename Float, int nSpin, int nColor, typename Arg> __device__ __host__ void computeYreverse(Arg &arg, int parity, int x_cb, int ic_c, int jc_c) { auto &Y = arg.Y; #pragma unroll for (int d=0; d<4; d++) { #pragma unroll for (int s_row = 0; s_row < nSpin; s_row++) { #pragma unroll for (int s_col = 0; s_col < nSpin; s_col++) { if (s_row == s_col) Y(d+4,parity,x_cb,s_row,s_col,ic_c,jc_c) = Y(d,parity,x_cb,s_row,s_col,ic_c,jc_c); else Y(d+4,parity,x_cb,s_row,s_col,ic_c,jc_c) = -Y(d,parity,x_cb,s_row,s_col,ic_c,jc_c); } } } } template<typename Float, int nSpin, int nColor, typename Arg> void ComputeYReverseCPU(Arg &arg) { for (int parity=0; parity<2; parity++) { #pragma omp parallel for for (int x_cb=0; x_cb<arg.coarseVolumeCB; x_cb++) { for (int ic_c = 0; ic_c < nColor; ic_c++) { for (int jc_c = 0; jc_c < nColor; jc_c++) { computeYreverse<Float,nSpin,nColor,Arg>(arg, parity, x_cb, ic_c, jc_c); } } } } } template<typename Float, int nSpin, int nColor, typename Arg> __global__ void ComputeYReverseGPU(Arg arg) { int x_cb = blockDim.x*blockIdx.x + threadIdx.x; if (x_cb >= arg.coarseVolumeCB) return; int parity_jc_c = blockDim.y*blockIdx.y + threadIdx.y; if (parity_jc_c >= 2*nColor) return; int parity = parity_jc_c / nColor; int jc_c = parity_jc_c % nColor; int ic_c = blockDim.z*blockIdx.z + threadIdx.z; if (ic_c >= nColor) return; computeYreverse<Float,nSpin,nColor,Arg>(arg, parity, x_cb, ic_c, jc_c); } template<bool from_coarse, typename Float, int fineSpin, int coarseSpin, int fineColor, int coarseColor, typename Arg> __device__ __host__ void computeCoarseClover(Arg &arg, int parity, int x_cb, int ic_c, int jc_c) { const int nDim = 4; int coord[QUDA_MAX_DIM]; int coord_coarse[QUDA_MAX_DIM]; getCoords(coord, x_cb, arg.x_size, parity); for (int d=0; d<nDim; d++) coord_coarse[d] = coord[d]/arg.geo_bs[d]; int coarse_parity = 0; for (int d=0; d<nDim; d++) coarse_parity += coord_coarse[d]; coarse_parity &= 1; coord_coarse[0] /= 2; int coarse_x_cb = ((coord_coarse[3]*arg.xc_size[2]+coord_coarse[2])*arg.xc_size[1]+coord_coarse[1])*(arg.xc_size[0]/2) + coord_coarse[0]; coord[0] /= 2; complex<Float> X[coarseSpin*coarseSpin]; for (int i=0; i<coarseSpin*coarseSpin; i++) X[i] = 0.0; if (!from_coarse) { for (int s = 0; s < fineSpin; s++) { const int s_c = arg.spin_map(s,parity); for (int s_col = s_c*arg.spin_bs; s_col < (s_c+1)*arg.spin_bs; s_col++) { for (int ic = 0; ic < fineColor; ic++) { for (int jc = 0; jc < fineColor; jc++) { X[s_c*coarseSpin + s_c] += conj(arg.V(parity, x_cb, s, ic, ic_c)) * arg.C(0, parity, x_cb, s, s_col, ic, jc) * arg.V(parity, x_cb, s_col, jc, jc_c); } } } } } else { for (int s = 0; s < fineSpin; s++) { for (int s_col = 0; s_col < fineSpin; s_col++) { for (int ic = 0; ic < fineColor; ic++) { for (int jc = 0; jc < fineColor; jc++) { X[s*coarseSpin + s_col] += conj(arg.V(parity, x_cb, s, ic, ic_c)) * arg.C(0, parity, x_cb, s, s_col, ic, jc) * arg.V(parity, x_cb, s_col, jc, jc_c); } } } } } for (int si = 0; si < coarseSpin; si++) { for (int sj = 0; sj < coarseSpin; sj++) { arg.X_atomic.atomicAdd(0,coarse_parity,coarse_x_cb,si,sj,ic_c,jc_c,X[si*coarseSpin+sj]); } } } template <bool from_coarse, typename Float, int fineSpin, int coarseSpin, int fineColor, int coarseColor, typename Arg> void ComputeCoarseCloverCPU(Arg &arg) { for (int parity=0; parity<2; parity++) { #pragma omp parallel for for (int x_cb=0; x_cb<arg.fineVolumeCB; x_cb++) { for (int jc_c=0; jc_c<coarseColor; jc_c++) { for (int ic_c=0; ic_c<coarseColor; ic_c++) { computeCoarseClover<from_coarse,Float,fineSpin,coarseSpin,fineColor,coarseColor>(arg, parity, x_cb, ic_c, jc_c); } } } } } template <bool from_coarse, typename Float, int fineSpin, int coarseSpin, int fineColor, int coarseColor, typename Arg> __global__ void ComputeCoarseCloverGPU(Arg arg) { int x_cb = blockDim.x*blockIdx.x + threadIdx.x; if (x_cb >= arg.fineVolumeCB) return; int parity_c_col = blockDim.y*blockIdx.y + threadIdx.y; if (parity_c_col >= 2*coarseColor) return; int jc_c = parity_c_col % coarseColor; int parity = parity_c_col / coarseColor; int ic_c = blockDim.z*blockIdx.z + threadIdx.z; if (ic_c >= coarseColor) return; computeCoarseClover<from_coarse,Float,fineSpin,coarseSpin,fineColor,coarseColor>(arg, parity, x_cb, ic_c, jc_c); } template<typename Float, int nSpin, int nColor, typename Arg> void AddCoarseDiagonalCPU(Arg &arg) { for (int parity=0; parity<2; parity++) { #pragma omp parallel for for (int x_cb=0; x_cb<arg.coarseVolumeCB; x_cb++) { for(int s = 0; s < nSpin; s++) { for(int c = 0; c < nColor; c++) { arg.X_atomic(0,parity,x_cb,s,s,c,c) += complex<Float>(1.0,0.0); } } } } } template<typename Float, int nSpin, int nColor, typename Arg> __global__ void AddCoarseDiagonalGPU(Arg arg) { int x_cb = blockDim.x*blockIdx.x + threadIdx.x; if (x_cb >= arg.coarseVolumeCB) return; int parity = blockDim.y*blockIdx.y + threadIdx.y; for(int s = 0; s < nSpin; s++) { for(int c = 0; c < nColor; c++) { arg.X_atomic(0,parity,x_cb,s,s,c,c) += complex<Float>(1.0,0.0); } } } template<typename Float, int nSpin, int nColor, typename Arg> void AddCoarseTmDiagonalCPU(Arg &arg) { const complex<Float> mu(0., arg.mu*arg.mu_factor); for (int parity=0; parity<2; parity++) { #pragma omp parallel for for (int x_cb=0; x_cb<arg.coarseVolumeCB; x_cb++) { for(int s = 0; s < nSpin/2; s++) { for(int c = 0; c < nColor; c++) { arg.X_atomic(0,parity,x_cb,s,s,c,c) += mu; } } for(int s = nSpin/2; s < nSpin; s++) { for(int c = 0; c < nColor; c++) { arg.X_atomic(0,parity,x_cb,s,s,c,c) -= mu; } } } } } template<typename Float, int nSpin, int nColor, typename Arg> __global__ void AddCoarseTmDiagonalGPU(Arg arg) { int x_cb = blockDim.x*blockIdx.x + threadIdx.x; if (x_cb >= arg.coarseVolumeCB) return; int parity = blockDim.y*blockIdx.y + threadIdx.y; const complex<Float> mu(0., arg.mu*arg.mu_factor); for(int s = 0; s < nSpin/2; s++) { for(int ic_c = 0; ic_c < nColor; ic_c++) { arg.X_atomic(0,parity,x_cb,s,s,ic_c,ic_c) += mu; } } for(int s = nSpin/2; s < nSpin; s++) { for(int ic_c = 0; ic_c < nColor; ic_c++) { arg.X_atomic(0,parity,x_cb,s,s,ic_c,ic_c) -= mu; } } } template<typename Float, int nSpin, int nColor, typename Arg> __device__ __host__ void convert(Arg &arg, int parity, int x_cb, int c_row, int c_col) { if (arg.dim_index < 8) { const auto &in = arg.Y_atomic; int d_in = arg.dim_index % in.geometry; int d_out = arg.dim_index % arg.Y.geometry; #pragma unroll for (int s_row = 0; s_row < nSpin; s_row++) { #pragma unroll for (int s_col = 0; s_col < nSpin; s_col++) { complex<Float> M = in(d_in,parity,x_cb,s_row,s_col,c_row,c_col); arg.Y(d_out,parity,x_cb,s_row,s_col,c_row,c_col) = M; } } } else { const auto &in = arg.X_atomic; int d_in = arg.dim_index % in.geometry; int d_out = arg.dim_index % arg.X.geometry; #pragma unroll for (int s_row = 0; s_row < nSpin; s_row++) { #pragma unroll for (int s_col = 0; s_col < nSpin; s_col++) { complex<Float> M = in(d_in,parity,x_cb,s_row,s_col,c_row,c_col); arg.X(d_out,parity,x_cb,s_row,s_col,c_row,c_col) = M; } } } } template<typename Float, int nSpin, int nColor, typename Arg> void ConvertCPU(Arg &arg) { for (int parity=0; parity<2; parity++) { #pragma omp parallel for for (int x_cb=0; x_cb<arg.coarseVolumeCB; x_cb++) { for(int c_row = 0; c_row < nColor; c_row++) { for(int c_col = 0; c_col < nColor; c_col++) { convert<Float,nSpin,nColor,Arg>(arg, parity, x_cb, c_row, c_col); } } } } } template<typename Float, int nSpin, int nColor, typename Arg> __global__ void ConvertGPU(Arg arg) { int x_cb = blockDim.x*blockIdx.x + threadIdx.x; if (x_cb >= arg.coarseVolumeCB) return; int parity_c_col = blockDim.y*blockIdx.y + threadIdx.y; if (parity_c_col >= 2*nColor) return; int c_col = parity_c_col % nColor; int parity = parity_c_col / nColor; int c_row = blockDim.z*blockIdx.z + threadIdx.z; if (c_row >= nColor) return; convert<Float,nSpin,nColor,Arg>(arg, parity, x_cb, c_row, c_col); } template<typename Float, int nSpin, int nColor, typename Arg> __device__ __host__ void rescaleY(Arg &arg, int parity, int x_cb, int c_row, int c_col) { #pragma unroll for (int s_row = 0; s_row < nSpin; s_row++) { #pragma unroll for (int s_col = 0; s_col < nSpin; s_col++) { complex<Float> M = arg.Y(arg.dim_index,parity,x_cb,s_row,s_col,c_row,c_col); arg.Y(arg.dim_index,parity,x_cb,s_row,s_col,c_row,c_col) = arg.rescale*M; } } } template<typename Float, int nSpin, int nColor, typename Arg> void RescaleYCPU(Arg &arg) { for (int parity=0; parity<2; parity++) { #pragma omp parallel for for (int x_cb=0; x_cb<arg.coarseVolumeCB; x_cb++) { for(int c_row = 0; c_row < nColor; c_row++) { for(int c_col = 0; c_col < nColor; c_col++) { rescaleY<Float,nSpin,nColor,Arg>(arg, parity, x_cb, c_row, c_col); } } } } } template<typename Float, int nSpin, int nColor, typename Arg> __global__ void RescaleYGPU(Arg arg) { int x_cb = blockDim.x*blockIdx.x + threadIdx.x; if (x_cb >= arg.coarseVolumeCB) return; int parity_c_col = blockDim.y*blockIdx.y + threadIdx.y; if (parity_c_col >= 2*nColor) return; int c_col = parity_c_col % nColor; int parity = parity_c_col / nColor; int c_row = blockDim.z*blockIdx.z + threadIdx.z; if (c_row >= nColor) return; rescaleY<Float,nSpin,nColor,Arg>(arg, parity, x_cb, c_row, c_col); } }