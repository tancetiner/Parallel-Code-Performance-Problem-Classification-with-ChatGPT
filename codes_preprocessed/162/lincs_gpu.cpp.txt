#include "gmxpre.h" #include "lincs_gpu.h" #include <assert.h> #include <stdio.h> #include <cmath> #include <algorithm> #include "gromacs/gpu_utils/devicebuffer.h" #include "gromacs/gpu_utils/gputraits.h" #include "gromacs/math/functions.h" #include "gromacs/math/vec.h" #include "gromacs/mdlib/constr.h" #include "gromacs/mdlib/lincs_gpu_internal.h" #include "gromacs/pbcutil/pbc.h" #include "gromacs/topology/ifunc.h" #include "gromacs/topology/topology.h" namespace gmx { void LincsGpu::apply(const DeviceBuffer<Float3>& d_x, DeviceBuffer<Float3> d_xp, const bool updateVelocities, DeviceBuffer<Float3> d_v, const real invdt, const bool computeVirial, tensor virialScaled, const PbcAiuc& pbcAiuc) { GMX_ASSERT(GMX_GPU_CUDA, "LINCS GPU is only implemented in CUDA."); if (kernelParams_.numConstraintsThreads == 0) { return; } if (computeVirial) { clearDeviceBufferAsync(&kernelParams_.d_virialScaled, 0, 6, deviceStream_); } kernelParams_.pbcAiuc = pbcAiuc; launchLincsGpuKernel( kernelParams_, d_x, d_xp, updateVelocities, d_v, invdt, computeVirial, deviceStream_); if (computeVirial) { copyFromDeviceBuffer(h_virialScaled_.data(), &kernelParams_.d_virialScaled, 0, 6, deviceStream_, GpuApiCallBehavior::Sync, nullptr); virialScaled[XX][XX] += h_virialScaled_[0]; virialScaled[XX][YY] += h_virialScaled_[1]; virialScaled[XX][ZZ] += h_virialScaled_[2]; virialScaled[YY][XX] += h_virialScaled_[1]; virialScaled[YY][YY] += h_virialScaled_[3]; virialScaled[YY][ZZ] += h_virialScaled_[4]; virialScaled[ZZ][XX] += h_virialScaled_[2]; virialScaled[ZZ][YY] += h_virialScaled_[4]; virialScaled[ZZ][ZZ] += h_virialScaled_[5]; } } LincsGpu::LincsGpu(int numIterations, int expansionOrder, const DeviceContext& deviceContext, const DeviceStream& deviceStream) : deviceContext_(deviceContext), deviceStream_(deviceStream) { GMX_RELEASE_ASSERT(GMX_GPU_CUDA, "LINCS GPU is only implemented in CUDA."); kernelParams_.numIterations = numIterations; kernelParams_.expansionOrder = expansionOrder; static_assert(sizeof(real) == sizeof(float), "Real numbers should be in single precision in GPU code."); static_assert( gmx::isPowerOfTwo(c_threadsPerBlock), "Number of threads per block should be a power of two in order for reduction to work."); allocateDeviceBuffer(&kernelParams_.d_virialScaled, 6, deviceContext_); h_virialScaled_.resize(6); numConstraintsThreadsAlloc_ = 0; numAtomsAlloc_ = 0; } LincsGpu::~LincsGpu() { freeDeviceBuffer(&kernelParams_.d_virialScaled); if (numConstraintsThreadsAlloc_ > 0) { freeDeviceBuffer(&kernelParams_.d_constraints); freeDeviceBuffer(&kernelParams_.d_constraintsTargetLengths); freeDeviceBuffer(&kernelParams_.d_coupledConstraintsCounts); freeDeviceBuffer(&kernelParams_.d_coupledConstraintsIndices); freeDeviceBuffer(&kernelParams_.d_massFactors); freeDeviceBuffer(&kernelParams_.d_matrixA); } if (numAtomsAlloc_ > 0) { freeDeviceBuffer(&kernelParams_.d_inverseMasses); } } struct AtomsAdjacencyListElement { AtomsAdjacencyListElement(const int indexOfSecondConstrainedAtom, const int indexOfConstraint, const int signFactor) : indexOfSecondConstrainedAtom_(indexOfSecondConstrainedAtom), indexOfConstraint_(indexOfConstraint), signFactor_(signFactor) { } int indexOfSecondConstrainedAtom_; int indexOfConstraint_; int signFactor_; }; static std::vector<std::vector<AtomsAdjacencyListElement>> constructAtomsAdjacencyList(const int numAtoms, ArrayRef<const int> iatoms) { const int stride = 1 + NRAL(F_CONSTR); const int numConstraints = iatoms.ssize() / stride; std::vector<std::vector<AtomsAdjacencyListElement>> atomsAdjacencyList(numAtoms); for (int c = 0; c < numConstraints; c++) { int a1 = iatoms[stride * c + 1]; int a2 = iatoms[stride * c + 2]; atomsAdjacencyList[a1].emplace_back(a2, c, +1); atomsAdjacencyList[a2].emplace_back(a1, c, -1); } return atomsAdjacencyList; } inline int countCoupled(int a, ArrayRef<int> numCoupledConstraints, ArrayRef<const std::vector<AtomsAdjacencyListElement>> atomsAdjacencyList) { int counted = 0; for (const auto& adjacentAtom : atomsAdjacencyList[a]) { const int c2 = adjacentAtom.indexOfConstraint_; if (numCoupledConstraints[c2] == -1) { numCoupledConstraints[c2] = 0; counted += 1 + countCoupled(adjacentAtom.indexOfSecondConstrainedAtom_, numCoupledConstraints, atomsAdjacencyList); } } return counted; } inline void addWithCoupled(ArrayRef<const int> iatoms, const int stride, ArrayRef<const std::vector<AtomsAdjacencyListElement>> atomsAdjacencyList, ArrayRef<int> splitMap, const int c, int* currentMapIndex) { if (splitMap[c] == -1) { splitMap[c] = *currentMapIndex; (*currentMapIndex)++; for (int atomIndexInConstraint = 0; atomIndexInConstraint < 2; atomIndexInConstraint++) { const int a1 = iatoms[stride * c + 1 + atomIndexInConstraint]; for (const auto& adjacentAtom : atomsAdjacencyList[a1]) { const int c2 = adjacentAtom.indexOfConstraint_; if (c2 != c) { addWithCoupled(iatoms, stride, atomsAdjacencyList, splitMap, c2, currentMapIndex); } } } } } static std::vector<int> countNumCoupledConstraints(ArrayRef<const int> iatoms, ArrayRef<const std::vector<AtomsAdjacencyListElement>> atomsAdjacencyList) { const int stride = 1 + NRAL(F_CONSTR); const int numConstraints = iatoms.ssize() / stride; std::vector<int> numCoupledConstraints(numConstraints, -1); for (int c = 0; c < numConstraints; c++) { const int a1 = iatoms[stride * c + 1]; const int a2 = iatoms[stride * c + 2]; if (numCoupledConstraints[c] == -1) { numCoupledConstraints[c] = countCoupled(a1, numCoupledConstraints, atomsAdjacencyList) + countCoupled(a2, numCoupledConstraints, atomsAdjacencyList); } } return numCoupledConstraints; } bool LincsGpu::isNumCoupledConstraintsSupported(const gmx_mtop_t& mtop) { for (const gmx_moltype_t& molType : mtop.moltype) { ArrayRef<const int> iatoms = molType.ilist[F_CONSTR].iatoms; const auto atomsAdjacencyList = constructAtomsAdjacencyList(molType.atoms.nr, iatoms); const auto numCoupledConstraints = countNumCoupledConstraints(iatoms, atomsAdjacencyList); for (const int numCoupled : numCoupledConstraints) { if (numCoupled > c_threadsPerBlock) { return false; } } } return true; } void LincsGpu::set(const InteractionDefinitions& idef, const int numAtoms, const real* invmass) { GMX_RELEASE_ASSERT(GMX_GPU_CUDA, "LINCS GPU is only implemented in CUDA."); std::vector<AtomPair> constraintsHost; std::vector<float> constraintsTargetLengthsHost; std::vector<int> coupledConstraintsCountsHost; std::vector<int> coupledConstraintsIndicesHost; std::vector<float> massFactorsHost; ArrayRef<const int> iatoms = idef.il[F_CONSTR].iatoms; const int stride = NRAL(F_CONSTR) + 1; const int numConstraints = idef.il[F_CONSTR].size() / stride; if (numConstraints == 0) { kernelParams_.numConstraintsThreads = 0; return; } const auto atomsAdjacencyList = constructAtomsAdjacencyList(numAtoms, iatoms); const auto numCoupledConstraints = countNumCoupledConstraints(iatoms, atomsAdjacencyList); std::vector<int> splitMap(numConstraints, -1); int currentMapIndex = 0; for (int c = 0; c < numConstraints; c++) { if (numCoupledConstraints.at(c) > c_threadsPerBlock) { gmx_fatal(FARGS, "Maximum number of coupled constraints (%d) exceeds the size of the CUDA " "thread block (%d). Most likely, you are trying to use the GPU version of " "LINCS with constraints on all-bonds, which is not supported for large " "molecules. When compatible with the force field and integration settings, " "using constraints on H-bonds only.", numCoupledConstraints.at(c), c_threadsPerBlock); } if (currentMapIndex / c_threadsPerBlock != (currentMapIndex + numCoupledConstraints.at(c)) / c_threadsPerBlock) { currentMapIndex = ((currentMapIndex / c_threadsPerBlock) + 1) * c_threadsPerBlock; } addWithCoupled(iatoms, stride, atomsAdjacencyList, splitMap, c, &currentMapIndex); } kernelParams_.numConstraintsThreads = currentMapIndex + c_threadsPerBlock - currentMapIndex % c_threadsPerBlock; GMX_RELEASE_ASSERT(kernelParams_.numConstraintsThreads % c_threadsPerBlock == 0, "Number of threads should be a multiple of the block size"); AtomPair pair; pair.i = -1; pair.j = -1; constraintsHost.resize(kernelParams_.numConstraintsThreads, pair); std::fill(constraintsHost.begin(), constraintsHost.end(), pair); constraintsTargetLengthsHost.resize(kernelParams_.numConstraintsThreads, 0.0); std::fill(constraintsTargetLengthsHost.begin(), constraintsTargetLengthsHost.end(), 0.0); for (int c = 0; c < numConstraints; c++) { int a1 = iatoms[stride * c + 1]; int a2 = iatoms[stride * c + 2]; int type = iatoms[stride * c]; AtomPair pair; pair.i = a1; pair.j = a2; constraintsHost.at(splitMap.at(c)) = pair; constraintsTargetLengthsHost.at(splitMap.at(c)) = idef.iparams[type].constr.dA; } int maxCoupledConstraints = 0; bool maxCoupledConstraintsHasIncreased = false; for (int c = 0; c < numConstraints; c++) { int a1 = iatoms[stride * c + 1]; int a2 = iatoms[stride * c + 2]; int nCoupledConstraints = atomsAdjacencyList.at(a1).size() + atomsAdjacencyList.at(a2).size() - 2; if (nCoupledConstraints > maxCoupledConstraints) { maxCoupledConstraints = nCoupledConstraints; maxCoupledConstraintsHasIncreased = true; } } coupledConstraintsCountsHost.resize(kernelParams_.numConstraintsThreads, 0); coupledConstraintsIndicesHost.resize(maxCoupledConstraints * kernelParams_.numConstraintsThreads, -1); massFactorsHost.resize(maxCoupledConstraints * kernelParams_.numConstraintsThreads, -1); for (int c1 = 0; c1 < numConstraints; c1++) { coupledConstraintsCountsHost.at(splitMap.at(c1)) = 0; int c1a1 = iatoms[stride * c1 + 1]; int c1a2 = iatoms[stride * c1 + 2]; int c2a1 = c1a1; for (const auto& atomAdjacencyList : atomsAdjacencyList[c1a1]) { int c2 = atomAdjacencyList.indexOfConstraint_; if (c1 != c2) { int c2a2 = atomAdjacencyList.indexOfSecondConstrainedAtom_; int sign = atomAdjacencyList.signFactor_; int index = kernelParams_.numConstraintsThreads * coupledConstraintsCountsHost.at(splitMap.at(c1)) + splitMap.at(c1); int threadBlockStarts = splitMap.at(c1) - splitMap.at(c1) % c_threadsPerBlock; coupledConstraintsIndicesHost.at(index) = splitMap.at(c2) - threadBlockStarts; int center = c1a1; float sqrtmu1 = 1.0 / std::sqrt(invmass[c1a1] + invmass[c1a2]); float sqrtmu2 = 1.0 / std::sqrt(invmass[c2a1] + invmass[c2a2]); massFactorsHost.at(index) = -sign * invmass[center] * sqrtmu1 * sqrtmu2; coupledConstraintsCountsHost.at(splitMap.at(c1))++; } } c2a1 = c1a2; for (const auto& atomAdjacencyList : atomsAdjacencyList[c1a2]) { int c2 = atomAdjacencyList.indexOfConstraint_; if (c1 != c2) { int c2a2 = atomAdjacencyList.indexOfSecondConstrainedAtom_; int sign = atomAdjacencyList.signFactor_; int index = kernelParams_.numConstraintsThreads * coupledConstraintsCountsHost.at(splitMap.at(c1)) + splitMap.at(c1); int threadBlockStarts = splitMap.at(c1) - splitMap.at(c1) % c_threadsPerBlock; coupledConstraintsIndicesHost.at(index) = splitMap.at(c2) - threadBlockStarts; int center = c1a2; float sqrtmu1 = 1.0 / std::sqrt(invmass[c1a1] + invmass[c1a2]); float sqrtmu2 = 1.0 / std::sqrt(invmass[c2a1] + invmass[c2a2]); massFactorsHost.at(index) = sign * invmass[center] * sqrtmu1 * sqrtmu2; coupledConstraintsCountsHost.at(splitMap.at(c1))++; } } } if ((kernelParams_.numConstraintsThreads > numConstraintsThreadsAlloc_) || maxCoupledConstraintsHasIncreased) { if (numConstraintsThreadsAlloc_ > 0) { freeDeviceBuffer(&kernelParams_.d_constraints); freeDeviceBuffer(&kernelParams_.d_constraintsTargetLengths); freeDeviceBuffer(&kernelParams_.d_coupledConstraintsCounts); freeDeviceBuffer(&kernelParams_.d_coupledConstraintsIndices); freeDeviceBuffer(&kernelParams_.d_massFactors); freeDeviceBuffer(&kernelParams_.d_matrixA); } numConstraintsThreadsAlloc_ = kernelParams_.numConstraintsThreads; allocateDeviceBuffer( &kernelParams_.d_constraints, kernelParams_.numConstraintsThreads, deviceContext_); allocateDeviceBuffer(&kernelParams_.d_constraintsTargetLengths, kernelParams_.numConstraintsThreads, deviceContext_); allocateDeviceBuffer(&kernelParams_.d_coupledConstraintsCounts, kernelParams_.numConstraintsThreads, deviceContext_); allocateDeviceBuffer(&kernelParams_.d_coupledConstraintsIndices, maxCoupledConstraints * kernelParams_.numConstraintsThreads, deviceContext_); allocateDeviceBuffer(&kernelParams_.d_massFactors, maxCoupledConstraints * kernelParams_.numConstraintsThreads, deviceContext_); allocateDeviceBuffer(&kernelParams_.d_matrixA, maxCoupledConstraints * kernelParams_.numConstraintsThreads, deviceContext_); } if (numAtoms > numAtomsAlloc_) { if (numAtomsAlloc_ > 0) { freeDeviceBuffer(&kernelParams_.d_inverseMasses); } numAtomsAlloc_ = numAtoms; allocateDeviceBuffer(&kernelParams_.d_inverseMasses, numAtoms, deviceContext_); } copyToDeviceBuffer(&kernelParams_.d_constraints, constraintsHost.data(), 0, kernelParams_.numConstraintsThreads, deviceStream_, GpuApiCallBehavior::Sync, nullptr); copyToDeviceBuffer(&kernelParams_.d_constraintsTargetLengths, constraintsTargetLengthsHost.data(), 0, kernelParams_.numConstraintsThreads, deviceStream_, GpuApiCallBehavior::Sync, nullptr); copyToDeviceBuffer(&kernelParams_.d_coupledConstraintsCounts, coupledConstraintsCountsHost.data(), 0, kernelParams_.numConstraintsThreads, deviceStream_, GpuApiCallBehavior::Sync, nullptr); copyToDeviceBuffer(&kernelParams_.d_coupledConstraintsIndices, coupledConstraintsIndicesHost.data(), 0, maxCoupledConstraints * kernelParams_.numConstraintsThreads, deviceStream_, GpuApiCallBehavior::Sync, nullptr); copyToDeviceBuffer(&kernelParams_.d_massFactors, massFactorsHost.data(), 0, maxCoupledConstraints * kernelParams_.numConstraintsThreads, deviceStream_, GpuApiCallBehavior::Sync, nullptr); GMX_RELEASE_ASSERT(invmass != nullptr, "Masses of atoms should be specified.\n"); copyToDeviceBuffer( &kernelParams_.d_inverseMasses, invmass, 0, numAtoms, deviceStream_, GpuApiCallBehavior::Sync, nullptr); } }