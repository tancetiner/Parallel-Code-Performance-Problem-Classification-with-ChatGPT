#include "gmxpre.h" #include "pme_gather_sycl.h" #include "gromacs/gpu_utils/gmxsycl.h" #include "gromacs/gpu_utils/gputraits_sycl.h" #include "gromacs/gpu_utils/sycl_kernel_utils.h" #include "gromacs/gpu_utils/syclutils.h" #include "gromacs/math/functions.h" #include "pme_gpu_calculate_splines_sycl.h" #include "pme_grid.h" #include "pme_gpu_constants.h" #include "pme_gpu_types_host.h" template<int order, int atomDataSize, int workGroupSize, int subGroupSize> inline void reduceAtomForces(sycl::nd_item<3> itemIdx, sycl::local_ptr<Float3> sm_forces, const int atomIndexLocal, const int splineIndex, const int gmx_unused lineIndex, const float realGridSizeFP[3], float& fx, float& fy, float& fz) { static_assert(gmx::isPowerOfTwo(order)); static_assert(order == 4, "Only order of 4 is implemented"); sycl::sub_group sg = itemIdx.get_sub_group(); static_assert(atomDataSize <= subGroupSize, "TODO: rework for atomDataSize > subGroupSize (order 8 or larger)"); static_assert(gmx::isPowerOfTwo(atomDataSize)); fx += sycl_2020::shift_left(sg, fx, 1); fy += sycl_2020::shift_right(sg, fy, 1); fz += sycl_2020::shift_left(sg, fz, 1); if (splineIndex & 1) { fx = fy; } fx += sycl_2020::shift_left(sg, fx, 2); fz += sycl_2020::shift_right(sg, fz, 2); if (splineIndex & 2) { fx = fz; } static_assert(atomDataSize >= 4); for (int delta = 4; delta < atomDataSize; delta *= 2) { fx += sycl_2020::shift_left(sg, fx, delta); } const int dimIndex = splineIndex; if (dimIndex < DIM) { const float n = realGridSizeFP[dimIndex]; sm_forces[atomIndexLocal][dimIndex] = fx * n; } } template<int order, int atomsPerWarp, bool wrapX, bool wrapY> inline void sumForceComponents(sycl::private_ptr<float> fx, sycl::private_ptr<float> fy, sycl::private_ptr<float> fz, const int ithyMin, const int ithyMax, const int ixBase, const int iz, const int nx, const int ny, const int pny, const int pnz, const int atomIndexLocal, const int splineIndexBase, const sycl::float2 tdz, const sycl::local_ptr<int> sm_gridlineIndices, const sycl::local_ptr<float> sm_theta, const sycl::local_ptr<float> sm_dtheta, const sycl::global_ptr<const float> gm_grid) { for (int ithy = ithyMin; ithy < ithyMax; ithy++) { const int splineIndexY = getSplineParamIndex<order, atomsPerWarp>(splineIndexBase, YY, ithy); const sycl::float2 tdy{ sm_theta[splineIndexY], sm_dtheta[splineIndexY] }; int iy = sm_gridlineIndices[atomIndexLocal * DIM + YY] + ithy; if (wrapY & (iy >= ny)) { iy -= ny; } const int constOffset = iy * pnz + iz; #pragma unroll for (int ithx = 0; ithx < order; ithx++) { int ix = ixBase + ithx; if (wrapX & (ix >= nx)) { ix -= nx; } const int gridIndexGlobal = ix * pny * pnz + constOffset; assert(gridIndexGlobal >= 0); const float gridValue = gm_grid[gridIndexGlobal]; assertIsFinite(gridValue); const int splineIndexX = getSplineParamIndex<order, atomsPerWarp>(splineIndexBase, XX, ithx); const sycl::float2 tdx{ sm_theta[splineIndexX], sm_dtheta[splineIndexX] }; const float fxy1 = tdz[XX] * gridValue; const float fz1 = tdz[YY] * gridValue; *fx += tdx[YY] * tdy[XX] * fxy1; *fy += tdx[XX] * tdy[YY] * fxy1; *fz += tdx[XX] * tdy[XX] * fz1; } } } inline void calculateAndStoreGridForces(sycl::local_ptr<Float3> sm_forces, const int forceIndexLocal, const int forceIndexGlobal, const Float3& recipBox0, const Float3& recipBox1, const Float3& recipBox2, const float scale, const sycl::global_ptr<const float> gm_coefficients) { const Float3 atomForces = sm_forces[forceIndexLocal]; float negCoefficient = -scale * gm_coefficients[forceIndexGlobal]; Float3 result; result[XX] = negCoefficient * recipBox0[XX] * atomForces[XX]; result[YY] = negCoefficient * (recipBox0[YY] * atomForces[XX] + recipBox1[YY] * atomForces[YY]); result[ZZ] = negCoefficient * (recipBox0[ZZ] * atomForces[XX] + recipBox1[ZZ] * atomForces[YY] + recipBox2[ZZ] * atomForces[ZZ]); sm_forces[forceIndexLocal] = result; } template<int order, bool wrapX, bool wrapY, int numGrids, bool readGlobal, ThreadsPerAtom threadsPerAtom, int subGroupSize> auto pmeGatherKernel(sycl::handler& cgh, const int nAtoms, DeviceAccessor<float, mode::read> a_gridA, OptionalAccessor<float, mode::read, numGrids == 2> a_gridB, DeviceAccessor<float, mode::read> a_coefficientsA, OptionalAccessor<float, mode::read, numGrids == 2> a_coefficientsB, OptionalAccessor<Float3, mode::read, !readGlobal> a_coordinates, DeviceAccessor<Float3, mode::read_write> a_forces, DeviceAccessor<float, mode::read> a_theta, DeviceAccessor<float, mode::read> a_dtheta, DeviceAccessor<int, mode::read> a_gridlineIndices, OptionalAccessor<float, mode::read, !readGlobal> a_fractShiftsTable, OptionalAccessor<int, mode::read, !readGlobal> a_gridlineIndicesTable, const gmx::IVec tablesOffsets, const gmx::IVec realGridSize, const gmx::RVec realGridSizeFP, const gmx::IVec realGridSizePadded, const gmx::RVec currentRecipBox0, const gmx::RVec currentRecipBox1, const gmx::RVec currentRecipBox2, const float scale) { static_assert(numGrids == 1 || numGrids == 2); constexpr int threadsPerAtomValue = (threadsPerAtom == ThreadsPerAtom::Order) ? order : order * order; constexpr int atomDataSize = threadsPerAtomValue; constexpr int atomsPerBlock = (c_gatherMaxWarpsPerBlock * subGroupSize) / atomDataSize; static_assert(subGroupSize >= atomDataSize); constexpr int atomsPerWarp = subGroupSize / atomDataSize; constexpr int blockSize = atomsPerBlock * atomDataSize; constexpr int splineParamsSize = atomsPerBlock * DIM * order; constexpr int gridlineIndicesSize = atomsPerBlock * DIM; a_gridA.bind(cgh); a_coefficientsA.bind(cgh); a_forces.bind(cgh); if constexpr (numGrids == 2) { a_gridB.bind(cgh); a_coefficientsB.bind(cgh); } if constexpr (readGlobal) { a_theta.bind(cgh); a_dtheta.bind(cgh); a_gridlineIndices.bind(cgh); } else { a_coordinates.bind(cgh); a_fractShiftsTable.bind(cgh); a_gridlineIndicesTable.bind(cgh); } sycl_2020::local_accessor<int, 1> sm_gridlineIndices(sycl::range<1>(atomsPerBlock * DIM), cgh); sycl_2020::local_accessor<float, 1> sm_theta(sycl::range<1>(atomsPerBlock * DIM * order), cgh); sycl_2020::local_accessor<float, 1> sm_dtheta(sycl::range<1>(atomsPerBlock * DIM * order), cgh); sycl_2020::local_accessor<float, 1> sm_coefficients(sycl::range<1>(atomsPerBlock), cgh); sycl_2020::local_accessor<Float3, 1> sm_coordinates(sycl::range<1>(atomsPerBlock), cgh); sycl_2020::local_accessor<Float3, 1> sm_forces(sycl::range<1>(atomsPerBlock), cgh); auto sm_fractCoords = [&]() { if constexpr (!readGlobal) { return sycl_2020::local_accessor<float, 1>(sycl::range<1>(atomsPerBlock * DIM), cgh); } else { return nullptr; } }(); return [=](sycl::nd_item<3> itemIdx) [[intel::reqd_sub_group_size(subGroupSize)]] { assert(blockSize == itemIdx.get_local_range().size()); const int atomIndexLocal = itemIdx.get_local_id(XX); const int blockIndex = itemIdx.get_group(YY) * itemIdx.get_group_range(ZZ) + itemIdx.get_group(ZZ); const int atomIndexOffset = blockIndex * atomsPerBlock; const int atomIndexGlobal = atomIndexOffset + atomIndexLocal; if (atomIndexOffset >= nAtoms) { return; } const int ithz = itemIdx.get_local_id(ZZ); const int splineIndex = itemIdx.get_local_id(YY) * itemIdx.get_local_range(ZZ) + itemIdx.get_local_id(ZZ); const int threadLocalId = itemIdx.get_local_linear_id(); const int threadLocalIdMax = blockSize; assert(threadLocalId < threadLocalIdMax); const int lineIndex = (itemIdx.get_local_id(XX) * (itemIdx.get_local_range(ZZ) * itemIdx.get_local_range(YY))) + splineIndex; assert(lineIndex == threadLocalId); if constexpr (readGlobal) { const int localGridlineIndicesIndex = threadLocalId; const int globalGridlineIndicesIndex = blockIndex * gridlineIndicesSize + localGridlineIndicesIndex; if (localGridlineIndicesIndex < gridlineIndicesSize) { sm_gridlineIndices[localGridlineIndicesIndex] = a_gridlineIndices[globalGridlineIndicesIndex]; assert(sm_gridlineIndices[localGridlineIndicesIndex] >= 0); } const int iMin = 0; const int iMax = (threadsPerAtom == ThreadsPerAtom::Order) ? 3 : 1; for (int i = iMin; i < iMax; i++) { const int localSplineParamsIndex = threadLocalId + i * threadLocalIdMax; const int globalSplineParamsIndex = blockIndex * splineParamsSize + localSplineParamsIndex; if (localSplineParamsIndex < splineParamsSize) { sm_theta[localSplineParamsIndex] = a_theta[globalSplineParamsIndex]; sm_dtheta[localSplineParamsIndex] = a_dtheta[globalSplineParamsIndex]; assertIsFinite(sm_theta[localSplineParamsIndex]); assertIsFinite(sm_dtheta[localSplineParamsIndex]); } } itemIdx.barrier(fence_space::local_space); } else { pmeGpuStageAtomData<float, atomsPerBlock, 1>( sm_coefficients.get_pointer(), a_coefficientsA.get_pointer(), itemIdx); pmeGpuStageAtomData<Float3, atomsPerBlock, 1>( sm_coordinates.get_pointer(), a_coordinates.get_pointer(), itemIdx); itemIdx.barrier(fence_space::local_space); const Float3 atomX = sm_coordinates[atomIndexLocal]; const float atomCharge = sm_coefficients[atomIndexLocal]; calculateSplines<order, atomsPerBlock, atomsPerWarp, true, false, numGrids, subGroupSize>( atomIndexOffset, atomX, atomCharge, tablesOffsets, realGridSizeFP, currentRecipBox0, currentRecipBox1, currentRecipBox2, nullptr, nullptr, nullptr, a_fractShiftsTable.get_pointer(), a_gridlineIndicesTable.get_pointer(), sm_theta.get_pointer(), sm_dtheta.get_pointer(), sm_gridlineIndices.get_pointer(), sm_fractCoords.get_pointer(), itemIdx); subGroupBarrier(itemIdx); } float fx = 0.0F; float fy = 0.0F; float fz = 0.0F; const int chargeCheck = pmeGpuCheckAtomCharge(a_coefficientsA[atomIndexGlobal]); const int nx = realGridSize[XX]; const int ny = realGridSize[YY]; const int nz = realGridSize[ZZ]; const int pny = realGridSizePadded[YY]; const int pnz = realGridSizePadded[ZZ]; const int atomWarpIndex = atomIndexLocal % atomsPerWarp; const int warpIndex = atomIndexLocal / atomsPerWarp; const int splineIndexBase = getSplineParamIndexBase<order, atomsPerWarp>(warpIndex, atomWarpIndex); const int splineIndexZ = getSplineParamIndex<order, atomsPerWarp>(splineIndexBase, ZZ, ithz); const sycl::float2 tdz{ sm_theta[splineIndexZ], sm_dtheta[splineIndexZ] }; int iz = sm_gridlineIndices[atomIndexLocal * DIM + ZZ] + ithz; const int ixBase = sm_gridlineIndices[atomIndexLocal * DIM + XX]; if (iz >= nz) { iz -= nz; } const int ithyMin = (threadsPerAtom == ThreadsPerAtom::Order) ? 0 : itemIdx.get_local_id(YY); const int ithyMax = (threadsPerAtom == ThreadsPerAtom::Order) ? order : itemIdx.get_local_id(YY) + 1; if (chargeCheck) { sumForceComponents<order, atomsPerWarp, wrapX, wrapY>(&fx, &fy, &fz, ithyMin, ithyMax, ixBase, iz, nx, ny, pny, pnz, atomIndexLocal, splineIndexBase, tdz, sm_gridlineIndices.get_pointer(), sm_theta.get_pointer(), sm_dtheta.get_pointer(), a_gridA.get_pointer()); } reduceAtomForces<order, atomDataSize, blockSize, subGroupSize>( itemIdx, sm_forces.get_pointer(), atomIndexLocal, splineIndex, lineIndex, realGridSizeFP, fx, fy, fz); itemIdx.barrier(fence_space::local_space); const int forceIndexLocal = threadLocalId; const int forceIndexGlobal = atomIndexOffset + forceIndexLocal; if (forceIndexLocal < atomsPerBlock) { calculateAndStoreGridForces(sm_forces.get_pointer(), forceIndexLocal, forceIndexGlobal, currentRecipBox0, currentRecipBox1, currentRecipBox2, scale, a_coefficientsA.get_pointer()); } itemIdx.barrier(fence_space::local_space); static_assert(atomsPerBlock <= subGroupSize); constexpr int blockForcesSize = atomsPerBlock * DIM; constexpr int numIter = (blockForcesSize + subGroupSize - 1) / subGroupSize; constexpr int iterThreads = blockForcesSize / numIter; if (threadLocalId < iterThreads) { #pragma unroll for (int i = 0; i < numIter; i++) { const int floatIndexLocal = i * iterThreads + threadLocalId; const int float3IndexLocal = floatIndexLocal / 3; const int dimLocal = floatIndexLocal % 3; static_assert(blockForcesSize % DIM == 0); const int float3IndexGlobal = blockIndex * atomsPerBlock + float3IndexLocal; a_forces[float3IndexGlobal][dimLocal] = sm_forces[float3IndexLocal][dimLocal]; } } if constexpr (numGrids == 2) { itemIdx.barrier(fence_space::local_space); fx = 0.0F; fy = 0.0F; fz = 0.0F; const bool chargeCheck = pmeGpuCheckAtomCharge(a_coefficientsB[atomIndexGlobal]); if (chargeCheck) { sumForceComponents<order, atomsPerWarp, wrapX, wrapY>(&fx, &fy, &fz, ithyMin, ithyMax, ixBase, iz, nx, ny, pny, pnz, atomIndexLocal, splineIndexBase, tdz, sm_gridlineIndices.get_pointer(), sm_theta.get_pointer(), sm_dtheta.get_pointer(), a_gridB.get_pointer()); } reduceAtomForces<order, atomDataSize, blockSize, subGroupSize>( itemIdx, sm_forces.get_pointer(), atomIndexLocal, splineIndex, lineIndex, realGridSizeFP, fx, fy, fz); itemIdx.barrier(fence_space::local_space); if (forceIndexLocal < atomsPerBlock) { calculateAndStoreGridForces(sm_forces.get_pointer(), forceIndexLocal, forceIndexGlobal, currentRecipBox0, currentRecipBox1, currentRecipBox2, 1.0F - scale, a_coefficientsB.get_pointer()); } itemIdx.barrier(fence_space::local_space); if (threadLocalId < iterThreads) { #pragma unroll for (int i = 0; i < numIter; i++) { const int floatIndexLocal = i * iterThreads + threadLocalId; const int float3IndexLocal = floatIndexLocal / 3; const int dimLocal = floatIndexLocal % 3; static_assert(blockForcesSize % DIM == 0); const int float3IndexGlobal = blockIndex * atomsPerBlock + float3IndexLocal; a_forces[float3IndexGlobal][dimLocal] += sm_forces[float3IndexLocal][dimLocal]; } } } }; } template<int order, bool wrapX, bool wrapY, int numGrids, bool readGlobal, ThreadsPerAtom threadsPerAtom, int subGroupSize> PmeGatherKernel<order, wrapX, wrapY, numGrids, readGlobal, threadsPerAtom, subGroupSize>::PmeGatherKernel() { reset(); } template<int order, bool wrapX, bool wrapY, int numGrids, bool readGlobal, ThreadsPerAtom threadsPerAtom, int subGroupSize> void PmeGatherKernel<order, wrapX, wrapY, numGrids, readGlobal, threadsPerAtom, subGroupSize>::setArg( size_t argIndex, void* arg) { if (argIndex == 0) { auto* params = reinterpret_cast<PmeGpuKernelParams*>(arg); gridParams_ = &params->grid; atomParams_ = &params->atoms; dynamicParams_ = &params->current; } else { GMX_RELEASE_ASSERT(argIndex == 0, "Trying to pass too many args to the kernel"); } } template<int order, bool wrapX, bool wrapY, int numGrids, bool readGlobal, ThreadsPerAtom threadsPerAtom, int subGroupSize> sycl::event PmeGatherKernel<order, wrapX, wrapY, numGrids, readGlobal, threadsPerAtom, subGroupSize>::launch( const KernelLaunchConfig& config, const DeviceStream& deviceStream) { GMX_RELEASE_ASSERT(gridParams_, "Can not launch the kernel before setting its args"); GMX_RELEASE_ASSERT(atomParams_, "Can not launch the kernel before setting its args"); GMX_RELEASE_ASSERT(dynamicParams_, "Can not launch the kernel before setting its args"); using kernelNameType = PmeGatherKernel<order, wrapX, wrapY, numGrids, readGlobal, threadsPerAtom, subGroupSize>; const sycl::range<3> localSize{ config.blockSize[2], config.blockSize[1], config.blockSize[0] }; const sycl::range<3> groupRange{ config.gridSize[2], config.gridSize[1], config.gridSize[0] }; const sycl::nd_range<3> range{ groupRange * localSize, localSize }; sycl::queue q = deviceStream.stream(); sycl::event e = q.submit([&](sycl::handler& cgh) { auto kernel = pmeGatherKernel<order, wrapX, wrapY, numGrids, readGlobal, threadsPerAtom, subGroupSize>( cgh, atomParams_->nAtoms, gridParams_->d_realGrid[0], gridParams_->d_realGrid[1], atomParams_->d_coefficients[0], atomParams_->d_coefficients[1], atomParams_->d_coordinates, atomParams_->d_forces, atomParams_->d_theta, atomParams_->d_dtheta, atomParams_->d_gridlineIndices, gridParams_->d_fractShiftsTable, gridParams_->d_gridlineIndicesTable, gridParams_->tablesOffsets, gridParams_->realGridSize, gridParams_->realGridSizeFP, gridParams_->realGridSizePadded, dynamicParams_->recipBox[0], dynamicParams_->recipBox[1], dynamicParams_->recipBox[2], dynamicParams_->scale); cgh.parallel_for<kernelNameType>(range, kernel); }); reset(); return e; } template<int order, bool wrapX, bool wrapY, int numGrids, bool readGlobal, ThreadsPerAtom threadsPerAtom, int subGroupSize> void PmeGatherKernel<order, wrapX, wrapY, numGrids, readGlobal, threadsPerAtom, subGroupSize>::reset() { gridParams_ = nullptr; atomParams_ = nullptr; dynamicParams_ = nullptr; } #ifdef __clang__ # pragma clang diagnostic push # pragma clang diagnostic ignored "-Wweak-template-vtables" #endif #define INSTANTIATE_3(order, numGrids, readGlobal, threadsPerAtom, subGroupSize) \ template class PmeGatherKernel<order, true, true, numGrids, readGlobal, threadsPerAtom, subGroupSize>; #define INSTANTIATE_2(order, numGrids, threadsPerAtom, subGroupSize) \ INSTANTIATE_3(order, numGrids, true, threadsPerAtom, subGroupSize); \ INSTANTIATE_3(order, numGrids, false, threadsPerAtom, subGroupSize); #define INSTANTIATE(order, subGroupSize) \ INSTANTIATE_2(order, 1, ThreadsPerAtom::Order, subGroupSize); \ INSTANTIATE_2(order, 1, ThreadsPerAtom::OrderSquared, subGroupSize); \ INSTANTIATE_2(order, 2, ThreadsPerAtom::Order, subGroupSize); \ INSTANTIATE_2(order, 2, ThreadsPerAtom::OrderSquared, subGroupSize); #if GMX_SYCL_DPCPP INSTANTIATE(4, 16); INSTANTIATE(4, 32); #elif GMX_SYCL_HIPSYCL INSTANTIATE(4, 32); INSTANTIATE(4, 64); #endif #ifdef __clang__ # pragma clang diagnostic pop #endif