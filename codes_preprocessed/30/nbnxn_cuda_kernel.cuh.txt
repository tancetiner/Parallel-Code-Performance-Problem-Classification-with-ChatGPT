#include "gromacs/gpu_utils/cuda_arch_utils.cuh" #include "gromacs/math/utilities.h" #include "gromacs/pbcutil/ishift.h" #if GMX_PTX_ARCH >= 300 #define REDUCE_SHUFFLE #define IATYPE_SHMEM #define USE_TEXOBJ #endif #if defined EL_EWALD_ANA || defined EL_EWALD_TAB #define EL_EWALD_ANY #endif #if defined EL_EWALD_ANY || defined EL_RF || defined LJ_EWALD || (defined EL_CUTOFF && defined CALC_ENERGIES) #define EXCLUSION_FORCES #endif #if defined LJ_EWALD_COMB_GEOM || defined LJ_EWALD_COMB_LB #define LJ_EWALD #endif #if GMX_PTX_ARCH == 370 #define NTHREAD_Z (2) #define MIN_BLOCKS_PER_MP (16) #else #define NTHREAD_Z (1) #define MIN_BLOCKS_PER_MP (16) #endif #define THREADS_PER_BLOCK (c_clSize*c_clSize*NTHREAD_Z) #if GMX_PTX_ARCH >= 350 #if (GMX_PTX_ARCH <= 210) && (NTHREAD_Z > 1) #error NTHREAD_Z > 1 will give incorrect results on CC 2.x #endif __launch_bounds__(THREADS_PER_BLOCK, MIN_BLOCKS_PER_MP) #else __launch_bounds__(THREADS_PER_BLOCK) #endif #ifdef PRUNE_NBL #ifdef CALC_ENERGIES __global__ void NB_KERNEL_FUNC_NAME(nbnxn_kernel, _VF_prune_cuda) #else __global__ void NB_KERNEL_FUNC_NAME(nbnxn_kernel, _F_prune_cuda) #endif #else #ifdef CALC_ENERGIES __global__ void NB_KERNEL_FUNC_NAME(nbnxn_kernel, _VF_cuda) #else __global__ void NB_KERNEL_FUNC_NAME(nbnxn_kernel, _F_cuda) #endif #endif (const cu_atomdata_t atdat, const cu_nbparam_t nbparam, const cu_plist_t plist, bool bCalcFshift) #ifdef FUNCTION_DECLARATION_ONLY ; #else { const nbnxn_sci_t *pl_sci = plist.sci; #ifndef PRUNE_NBL const #endif nbnxn_cj4_t *pl_cj4 = plist.cj4; const nbnxn_excl_t *excl = plist.excl; const int *atom_types = atdat.atom_types; int ntypes = atdat.ntypes; const float4 *xq = atdat.xq; float3 *f = atdat.f; const float3 *shift_vec = atdat.shift_vec; float rcoulomb_sq = nbparam.rcoulomb_sq; #ifdef VDW_CUTOFF_CHECK float rvdw_sq = nbparam.rvdw_sq; float vdw_in_range; #endif #ifdef LJ_EWALD float lje_coeff2, lje_coeff6_6; #endif #ifdef EL_RF float two_k_rf = nbparam.two_k_rf; #endif #ifdef EL_EWALD_TAB float coulomb_tab_scale = nbparam.coulomb_tab_scale; #endif #ifdef EL_EWALD_ANA float beta2 = nbparam.ewald_beta*nbparam.ewald_beta; float beta3 = nbparam.ewald_beta*nbparam.ewald_beta*nbparam.ewald_beta; #endif #ifdef PRUNE_NBL float rlist_sq = nbparam.rlist_sq; #endif #ifdef CALC_ENERGIES #ifdef EL_EWALD_ANY float beta = nbparam.ewald_beta; float ewald_shift = nbparam.sh_ewald; #else float c_rf = nbparam.c_rf; #endif float *e_lj = atdat.e_lj; float *e_el = atdat.e_el; #endif unsigned int tidxi = threadIdx.x; unsigned int tidxj = threadIdx.y; unsigned int tidx = threadIdx.y * blockDim.x + threadIdx.x; #if NTHREAD_Z == 1 unsigned int tidxz = 0; #else unsigned int tidxz = threadIdx.z; #endif unsigned int bidx = blockIdx.x; unsigned int widx = tidx / warp_size; int sci, ci, cj, ci_offset, ai, aj, cij4_start, cij4_end, typei, typej, i, jm, j4, wexcl_idx; float qi, qj_f, r2, inv_r, inv_r2, inv_r6, c6, c12, int_bit, F_invr; #ifdef CALC_ENERGIES float E_lj, E_el; #endif #if defined CALC_ENERGIES || defined LJ_POT_SWITCH float E_lj_p; #endif unsigned int wexcl, imask, mask_ji; float4 xqbuf; float3 xi, xj, rv, f_ij, fcj_buf; float3 fci_buf[c_numClPerSupercl]; nbnxn_sci_t nb_sci; const unsigned superClInteractionMask = ((1U << c_numClPerSupercl) - 1U); extern __shared__ float4 xqib[]; int *cjs = ((int *)(xqib + c_numClPerSupercl * c_clSize)) + tidxz * 2 * c_nbnxnGpuJgroupSize; #ifdef IATYPE_SHMEM int *atib = ((int *)(xqib + c_numClPerSupercl * c_clSize)) + NTHREAD_Z * 2 * c_nbnxnGpuJgroupSize; #endif #ifndef REDUCE_SHUFFLE #ifdef IATYPE_SHMEM float *f_buf = (float *)(atib + c_numClPerSupercl * c_clSize); #else float *f_buf = (float *)(cjs + NTHREAD_Z * 2 * c_nbnxnGpuJgroupSize); #endif #endif nb_sci = pl_sci[bidx]; sci = nb_sci.sci; cij4_start = nb_sci.cj4_ind_start; cij4_end = nb_sci.cj4_ind_end; if (tidxz == 0) { ci = sci * c_numClPerSupercl + tidxj; ai = ci * c_clSize + tidxi; xqib[tidxj * c_clSize + tidxi] = xq[ai] + shift_vec[nb_sci.shift]; #ifdef IATYPE_SHMEM atib[tidxj * c_clSize + tidxi] = atom_types[ai]; #endif } __syncthreads(); for (ci_offset = 0; ci_offset < c_numClPerSupercl; ci_offset++) { fci_buf[ci_offset] = make_float3(0.0f); } #ifdef LJ_EWALD lje_coeff2 = nbparam.ewaldcoeff_lj*nbparam.ewaldcoeff_lj; lje_coeff6_6 = lje_coeff2*lje_coeff2*lje_coeff2*c_oneSixth; #endif #ifdef CALC_ENERGIES E_lj = 0.0f; E_el = 0.0f; #if defined EXCLUSION_FORCES if (nb_sci.shift == CENTRAL && pl_cj4[cij4_start].cj[0] == sci*c_numClPerSupercl) { for (i = 0; i < c_numClPerSupercl; i++) { #if defined EL_EWALD_ANY || defined EL_RF || defined EL_CUTOFF qi = xqib[i * c_clSize + tidxi].w; E_el += qi*qi; #endif #if defined LJ_EWALD #ifdef USE_TEXOBJ E_lj += tex1Dfetch<float>(nbparam.nbfp_texobj, atom_types[(sci*c_numClPerSupercl + i)*c_clSize + tidxi]*(ntypes + 1)*2); #else E_lj += tex1Dfetch(nbfp_texref, atom_types[(sci*c_numClPerSupercl + i)*c_clSize + tidxi]*(ntypes + 1)*2); #endif #endif } #ifdef LJ_EWALD E_lj /= c_clSize*NTHREAD_Z; E_lj *= 0.5f*c_oneSixth*lje_coeff6_6; #endif #if defined EL_EWALD_ANY || defined EL_RF || defined EL_CUTOFF E_el /= c_clSize*NTHREAD_Z; #if defined EL_RF || defined EL_CUTOFF E_el *= -nbparam.epsfac*0.5f*c_rf; #else E_el *= -nbparam.epsfac*beta*M_FLOAT_1_SQRTPI; #endif #endif } #endif #endif if (nb_sci.shift == CENTRAL) { bCalcFshift = false; } for (j4 = cij4_start + tidxz; j4 < cij4_end; j4 += NTHREAD_Z) { wexcl_idx = pl_cj4[j4].imei[widx].excl_ind; imask = pl_cj4[j4].imei[widx].imask; wexcl = excl[wexcl_idx].pair[(tidx) & (warp_size - 1)]; #ifndef PRUNE_NBL if (imask) #endif { if ((tidxj == 0 || tidxj == 4) && tidxi < c_nbnxnGpuJgroupSize) { cjs[tidxi + tidxj * c_nbnxnGpuJgroupSize/c_splitClSize] = pl_cj4[j4].cj[tidxi]; } #if !defined PRUNE_NBL && GMX_PTX_ARCH < 300 #pragma unroll 4 #endif for (jm = 0; jm < c_nbnxnGpuJgroupSize; jm++) { if (imask & (superClInteractionMask << (jm * c_numClPerSupercl))) { mask_ji = (1U << (jm * c_numClPerSupercl)); cj = cjs[jm + (tidxj & 4) * c_nbnxnGpuJgroupSize/c_splitClSize]; aj = cj * c_clSize + tidxj; xqbuf = xq[aj]; xj = make_float3(xqbuf.x, xqbuf.y, xqbuf.z); qj_f = nbparam.epsfac * xqbuf.w; typej = atom_types[aj]; fcj_buf = make_float3(0.0f); #if !defined PRUNE_NBL #pragma unroll 8 #endif for (i = 0; i < c_numClPerSupercl; i++) { if (imask & mask_ji) { ci_offset = i; ci = sci * c_numClPerSupercl + i; ai = ci * c_clSize + tidxi; xqbuf = xqib[i * c_clSize + tidxi]; xi = make_float3(xqbuf.x, xqbuf.y, xqbuf.z); rv = xi - xj; r2 = norm2(rv); #ifdef PRUNE_NBL if (!__any(r2 < rlist_sq)) { imask &= ~mask_ji; } #endif int_bit = (wexcl & mask_ji) ? 1.0f : 0.0f; #ifdef EXCLUSION_FORCES if (r2 < rcoulomb_sq * (nb_sci.shift != CENTRAL || ci != cj || tidxj > tidxi)) #else if (r2 < rcoulomb_sq * int_bit) #endif { qi = xqbuf.w; #ifdef IATYPE_SHMEM typei = atib[i * c_clSize + tidxi]; #else typei = atom_types[ai]; #endif #ifdef USE_TEXOBJ c6 = tex1Dfetch<float>(nbparam.nbfp_texobj, 2 * (ntypes * typei + typej)); c12 = tex1Dfetch<float>(nbparam.nbfp_texobj, 2 * (ntypes * typei + typej) + 1); #else c6 = tex1Dfetch(nbfp_texref, 2 * (ntypes * typei + typej)); c12 = tex1Dfetch(nbfp_texref, 2 * (ntypes * typei + typej) + 1); #endif r2 += (1.0f - int_bit) * NBNXN_AVOID_SING_R2_INC; inv_r = rsqrt(r2); inv_r2 = inv_r * inv_r; inv_r6 = inv_r2 * inv_r2 * inv_r2; #if defined EXCLUSION_FORCES inv_r6 *= int_bit; #endif F_invr = inv_r6 * (c12 * inv_r6 - c6) * inv_r2; #if defined CALC_ENERGIES || defined LJ_POT_SWITCH E_lj_p = int_bit * (c12 * (inv_r6 * inv_r6 + nbparam.repulsion_shift.cpot)*c_oneTwelveth - c6 * (inv_r6 + nbparam.dispersion_shift.cpot)*c_oneSixth); #endif #ifdef LJ_FORCE_SWITCH #ifdef CALC_ENERGIES calculate_force_switch_F_E(nbparam, c6, c12, inv_r, r2, &F_invr, &E_lj_p); #else calculate_force_switch_F(nbparam, c6, c12, inv_r, r2, &F_invr); #endif #endif #ifdef LJ_EWALD #ifdef LJ_EWALD_COMB_GEOM #ifdef CALC_ENERGIES calculate_lj_ewald_comb_geom_F_E(nbparam, typei, typej, r2, inv_r2, lje_coeff2, lje_coeff6_6, int_bit, &F_invr, &E_lj_p); #else calculate_lj_ewald_comb_geom_F(nbparam, typei, typej, r2, inv_r2, lje_coeff2, lje_coeff6_6, &F_invr); #endif #elif defined LJ_EWALD_COMB_LB calculate_lj_ewald_comb_LB_F_E(nbparam, typei, typej, r2, inv_r2, lje_coeff2, lje_coeff6_6, #ifdef CALC_ENERGIES int_bit, &F_invr, &E_lj_p #else 0, &F_invr, NULL #endif ); #endif #endif #ifdef VDW_CUTOFF_CHECK vdw_in_range = (r2 < rvdw_sq) ? 1.0f : 0.0f; F_invr *= vdw_in_range; #ifdef CALC_ENERGIES E_lj_p *= vdw_in_range; #endif #endif #ifdef LJ_POT_SWITCH #ifdef CALC_ENERGIES calculate_potential_switch_F_E(nbparam, c6, c12, inv_r, r2, &F_invr, &E_lj_p); #else calculate_potential_switch_F(nbparam, c6, c12, inv_r, r2, &F_invr, &E_lj_p); #endif #endif #ifdef CALC_ENERGIES E_lj += E_lj_p; #endif #ifdef EL_CUTOFF #ifdef EXCLUSION_FORCES F_invr += qi * qj_f * int_bit * inv_r2 * inv_r; #else F_invr += qi * qj_f * inv_r2 * inv_r; #endif #endif #ifdef EL_RF F_invr += qi * qj_f * (int_bit*inv_r2 * inv_r - two_k_rf); #endif #if defined EL_EWALD_ANA F_invr += qi * qj_f * (int_bit*inv_r2*inv_r + pmecorrF(beta2*r2)*beta3); #elif defined EL_EWALD_TAB F_invr += qi * qj_f * (int_bit*inv_r2 - #ifdef USE_TEXOBJ interpolate_coulomb_force_r(nbparam.coulomb_tab_texobj, r2 * inv_r, coulomb_tab_scale) #else interpolate_coulomb_force_r(r2 * inv_r, coulomb_tab_scale) #endif ) * inv_r; #endif #ifdef CALC_ENERGIES #ifdef EL_CUTOFF E_el += qi * qj_f * (int_bit*inv_r - c_rf); #endif #ifdef EL_RF E_el += qi * qj_f * (int_bit*inv_r + 0.5f * two_k_rf * r2 - c_rf); #endif #ifdef EL_EWALD_ANY E_el += qi * qj_f * (inv_r * (int_bit - erff(r2 * inv_r * beta)) - int_bit * ewald_shift); #endif #endif f_ij = rv * F_invr; fcj_buf -= f_ij; fci_buf[ci_offset] += f_ij; } } mask_ji += mask_ji; } #ifdef REDUCE_SHUFFLE reduce_force_j_warp_shfl(fcj_buf, f, tidxi, aj); #else f_buf[ tidx] = fcj_buf.x; f_buf[ c_fbufStride + tidx] = fcj_buf.y; f_buf[2 * c_fbufStride + tidx] = fcj_buf.z; reduce_force_j_generic(f_buf, f, tidxi, tidxj, aj); #endif } } #ifdef PRUNE_NBL pl_cj4[j4].imei[widx].imask = imask; #endif } } float fshift_buf = 0.0f; for (ci_offset = 0; ci_offset < c_numClPerSupercl; ci_offset++) { ai = (sci * c_numClPerSupercl + ci_offset) * c_clSize + tidxi; #ifdef REDUCE_SHUFFLE reduce_force_i_warp_shfl(fci_buf[ci_offset], f, &fshift_buf, bCalcFshift, tidxj, ai); #else f_buf[ tidx] = fci_buf[ci_offset].x; f_buf[ c_fbufStride + tidx] = fci_buf[ci_offset].y; f_buf[2 * c_fbufStride + tidx] = fci_buf[ci_offset].z; __syncthreads(); reduce_force_i(f_buf, f, &fshift_buf, bCalcFshift, tidxi, tidxj, ai); __syncthreads(); #endif } #ifdef REDUCE_SHUFFLE if (bCalcFshift && (tidxj & 3) < 3) { atomicAdd(&(atdat.fshift[nb_sci.shift].x) + (tidxj & ~4), fshift_buf); } #else if (bCalcFshift && tidxj < 3) { atomicAdd(&(atdat.fshift[nb_sci.shift].x) + tidxj, fshift_buf); } #endif #ifdef CALC_ENERGIES #ifdef REDUCE_SHUFFLE reduce_energy_warp_shfl(E_lj, E_el, e_lj, e_el, tidx); #else f_buf[ tidx] = E_lj; f_buf[c_fbufStride + tidx] = E_el; reduce_energy_pow2(f_buf + (tidx & warp_size), e_lj, e_el, tidx & ~warp_size); #endif #endif } #endif #undef REDUCE_SHUFFLE #undef IATYPE_SHMEM #undef USE_TEXOBJ #undef NTHREAD_Z #undef MIN_BLOCKS_PER_MP #undef THREADS_PER_BLOCK #undef EL_EWALD_ANY #undef EXCLUSION_FORCES #undef LJ_EWALD