#include "nbnxn_ocl_kernel_utils.clh" #if defined EL_EWALD_ANA || defined EL_EWALD_TAB #define EL_EWALD_ANY #endif #if defined EL_EWALD_ANY || defined EL_RF || defined LJ_EWALD || (defined EL_CUTOFF && defined CALC_ENERGIES) #define EXCLUSION_FORCES #endif #if defined LJ_EWALD_COMB_GEOM || defined LJ_EWALD_COMB_LB #define LJ_EWALD #endif __attribute__((reqd_work_group_size(CL_SIZE, CL_SIZE, 1))) #ifdef PRUNE_NBL #ifdef CALC_ENERGIES __kernel void NB_KERNEL_FUNC_NAME(nbnxn_kernel, _VF_prune_opencl) #else __kernel void NB_KERNEL_FUNC_NAME(nbnxn_kernel, _F_prune_opencl) #endif #else #ifdef CALC_ENERGIES __kernel void NB_KERNEL_FUNC_NAME(nbnxn_kernel, _VF_opencl) #else __kernel void NB_KERNEL_FUNC_NAME(nbnxn_kernel, _F_opencl) #endif #endif (int ntypes, cl_nbparam_params_t nbparam_params, const __global float4 *restrict xq, __global float *restrict f, __global float *restrict e_lj, __global float *restrict e_el, __global float *restrict fshift, const __global int *restrict atom_types, const __global float *restrict shift_vec, __constant float* nbfp_climg2d, __constant float* nbfp_comb_climg2d, __constant float* coulomb_tab_climg2d, const __global nbnxn_sci_t* pl_sci, #ifndef PRUNE_NBL const #endif __global nbnxn_cj4_t* pl_cj4, const __global nbnxn_excl_t* excl, int bCalcFshift, __local float4 *xqib, __global float *debug_buffer ) { cl_nbparam_params_t *nbparam = &nbparam_params; float rcoulomb_sq = nbparam->rcoulomb_sq; #ifdef VDW_CUTOFF_CHECK float rvdw_sq = nbparam_params.rvdw_sq; float vdw_in_range; #endif #ifdef LJ_EWALD float lje_coeff2, lje_coeff6_6; #endif #ifdef EL_RF float two_k_rf = nbparam->two_k_rf; #endif #ifdef EL_EWALD_TAB float coulomb_tab_scale = nbparam->coulomb_tab_scale; #endif #ifdef EL_EWALD_ANA float beta2 = nbparam->ewald_beta*nbparam->ewald_beta; float beta3 = nbparam->ewald_beta*nbparam->ewald_beta*nbparam->ewald_beta; #endif #ifdef PRUNE_NBL float rlist_sq = nbparam->rlist_sq; #endif #ifdef CALC_ENERGIES #ifdef EL_EWALD_ANY float beta = nbparam->ewald_beta; float ewald_shift = nbparam->sh_ewald; #else float c_rf = nbparam->c_rf; #endif #endif unsigned int tidxi = get_local_id(0); unsigned int tidxj = get_local_id(1); unsigned int tidx = get_local_id(1) * get_local_size(0) + get_local_id(0); unsigned int bidx = get_group_id(0); unsigned int widx = tidx / WARP_SIZE; int sci, ci, cj, ci_offset, ai, aj, cij4_start, cij4_end, typei, typej, i, jm, j4, wexcl_idx; float qi, qj_f, r2, inv_r, inv_r2, inv_r6, c6, c12, int_bit, F_invr; #ifdef CALC_ENERGIES float E_lj, E_el; #endif #if defined CALC_ENERGIES || defined LJ_POT_SWITCH float E_lj_p; #endif unsigned int wexcl, imask, mask_ji; float4 xqbuf; float3 xi, xj, rv, f_ij, fcj_buf; float fshift_buf; float3 fci_buf[NCL_PER_SUPERCL]; nbnxn_sci_t nb_sci; const unsigned superClInteractionMask = ((1U << NCL_PER_SUPERCL) - 1U); __local int *cjs = (__local int *)(xqib + NCL_PER_SUPERCL * CL_SIZE); #define LOCAL_OFFSET cjs + 2 * NBNXN_GPU_JGROUP_SIZE #ifdef IATYPE_SHMEM __local int *atib = (__local int *)(LOCAL_OFFSET); #undef LOCAL_OFFSET #define LOCAL_OFFSET atib + NCL_PER_SUPERCL * CL_SIZE #endif #ifndef REDUCE_SHUFFLE __local float *f_buf = (__local float *)(LOCAL_OFFSET); #undef LOCAL_OFFSET #define LOCAL_OFFSET f_buf + CL_SIZE * CL_SIZE * 3 #endif volatile __local uint *warp_any = (__local uint*)(LOCAL_OFFSET); #undef LOCAL_OFFSET nb_sci = pl_sci[bidx]; sci = nb_sci.sci; cij4_start = nb_sci.cj4_ind_start; cij4_end = nb_sci.cj4_ind_end; ci = sci * NCL_PER_SUPERCL + tidxj; ai = ci * CL_SIZE + tidxi; xqib[tidxj * CL_SIZE + tidxi] = xq[ai] + (float4)(shift_vec[3 * nb_sci.shift], shift_vec[3 * nb_sci.shift + 1], shift_vec[3 * nb_sci.shift + 2], 0.0f); #ifdef IATYPE_SHMEM atib[tidxj * CL_SIZE + tidxi] = atom_types[ai]; #endif if(tidx==0 || tidx==32) warp_any[widx] = 0; barrier(CLK_LOCAL_MEM_FENCE); for (ci_offset = 0; ci_offset < NCL_PER_SUPERCL; ci_offset++) { fci_buf[ci_offset] = (float3)(0.0f); } #ifdef LJ_EWALD lje_coeff2 = nbparam->ewaldcoeff_lj*nbparam->ewaldcoeff_lj; lje_coeff6_6 = lje_coeff2*lje_coeff2*lje_coeff2*ONE_SIXTH_F; #endif #ifdef CALC_ENERGIES E_lj = 0.0f; E_el = 0.0f; #if defined EXCLUSION_FORCES if (nb_sci.shift == CENTRAL && pl_cj4[cij4_start].cj[0] == sci*NCL_PER_SUPERCL) { for (i = 0; i < NCL_PER_SUPERCL; i++) { #if defined EL_EWALD_ANY || defined EL_RF || defined EL_CUTOFF qi = xqib[i * CL_SIZE + tidxi].w; E_el += qi*qi; #endif #if defined LJ_EWALD E_lj += nbfp_climg2d[atom_types[(sci*NCL_PER_SUPERCL + i)*CL_SIZE + tidxi]*(ntypes + 1)*2]; #endif } #ifdef LJ_EWALD E_lj /= CL_SIZE; E_lj *= 0.5f*ONE_SIXTH_F*lje_coeff6_6; #endif #if defined EL_EWALD_ANY || defined EL_RF || defined EL_CUTOFF E_el /= CL_SIZE; #if defined EL_RF || defined EL_CUTOFF E_el *= -nbparam->epsfac*0.5f*c_rf; #else E_el *= -nbparam->epsfac*beta*M_FLOAT_1_SQRTPI; #endif #endif } #endif #endif if (nb_sci.shift == CENTRAL) { bCalcFshift = false; } fshift_buf = 0.0f; for (j4 = cij4_start; j4 < cij4_end; j4++) { wexcl_idx = pl_cj4[j4].imei[widx].excl_ind; imask = pl_cj4[j4].imei[widx].imask; wexcl = excl[wexcl_idx].pair[(tidx) & (WARP_SIZE - 1)]; #ifndef PRUNE_NBL if (imask) #endif { if ((tidxj == 0 || tidxj == 4) && tidxi < NBNXN_GPU_JGROUP_SIZE) { cjs[tidxi + tidxj * NBNXN_GPU_JGROUP_SIZE / 4] = pl_cj4[j4].cj[tidxi]; } #if !defined PRUNE_NBL #pragma unroll 4 #endif for (jm = 0; jm < NBNXN_GPU_JGROUP_SIZE; jm++) { if (imask & (superClInteractionMask << (jm * NCL_PER_SUPERCL))) { mask_ji = (1U << (jm * NCL_PER_SUPERCL)); cj = cjs[jm + (tidxj & 4) * NBNXN_GPU_JGROUP_SIZE / 4]; aj = cj * CL_SIZE + tidxj; xqbuf = xq[aj]; xj = (float3)(xqbuf.xyz); qj_f = nbparam->epsfac * xqbuf.w; typej = atom_types[aj]; fcj_buf = (float3)(0.0f); #if !defined PRUNE_NBL #pragma unroll 8 #endif for (i = 0; i < NCL_PER_SUPERCL; i++) { if (imask & mask_ji) { ci_offset = i; ci = sci * NCL_PER_SUPERCL + i; ai = ci * CL_SIZE + tidxi; xqbuf = xqib[i * CL_SIZE + tidxi]; xi = (float3)(xqbuf.xyz); rv = xi - xj; r2 = norm2(rv); #ifdef PRUNE_NBL if (r2 < rlist_sq) warp_any[widx]=1; if (!warp_any[widx]) imask &= ~mask_ji; warp_any[widx]=0; #endif int_bit = (wexcl & mask_ji) ? 1.0f : 0.0f; #ifdef EXCLUSION_FORCES if (r2 < rcoulomb_sq * (nb_sci.shift != CENTRAL || ci != cj || tidxj > tidxi)) #else if (r2 < rcoulomb_sq * int_bit) #endif { qi = xqbuf.w; #ifdef IATYPE_SHMEM typei = atib[i * CL_SIZE + tidxi]; #else typei = atom_types[ai]; #endif c6 = nbfp_climg2d[2 * (ntypes * typei + typej)]; c12 = nbfp_climg2d[2 * (ntypes * typei + typej)+1]; r2 += (1.0f - int_bit) * NBNXN_AVOID_SING_R2_INC; inv_r = rsqrt(r2); inv_r2 = inv_r * inv_r; inv_r6 = inv_r2 * inv_r2 * inv_r2; #if defined EXCLUSION_FORCES inv_r6 *= int_bit; #endif F_invr = inv_r6 * (c12 * inv_r6 - c6) * inv_r2; #if defined CALC_ENERGIES || defined LJ_POT_SWITCH E_lj_p = int_bit * (c12 * (inv_r6 * inv_r6 + nbparam->repulsion_shift.cpot)*ONE_TWELVETH_F - c6 * (inv_r6 + nbparam->dispersion_shift.cpot)*ONE_SIXTH_F); #endif #ifdef LJ_FORCE_SWITCH #ifdef CALC_ENERGIES calculate_force_switch_F_E(nbparam, c6, c12, inv_r, r2, &F_invr, &E_lj_p); #else calculate_force_switch_F(nbparam, c6, c12, inv_r, r2, &F_invr); #endif #endif #ifdef LJ_EWALD #ifdef LJ_EWALD_COMB_GEOM #ifdef CALC_ENERGIES calculate_lj_ewald_comb_geom_F_E(nbfp_comb_climg2d, nbparam, typei, typej, r2, inv_r2, lje_coeff2, lje_coeff6_6, int_bit, &F_invr, &E_lj_p); #else calculate_lj_ewald_comb_geom_F(nbfp_comb_climg2d, typei, typej, r2, inv_r2, lje_coeff2, lje_coeff6_6, &F_invr); #endif #elif defined LJ_EWALD_COMB_LB calculate_lj_ewald_comb_LB_F_E(nbfp_comb_climg2d, nbparam, typei, typej, r2, inv_r2, lje_coeff2, lje_coeff6_6, #ifdef CALC_ENERGIES int_bit, true, &F_invr, &E_lj_p #else 0, false, &F_invr, 0 #endif ); #endif #endif #ifdef VDW_CUTOFF_CHECK vdw_in_range = (r2 < rvdw_sq) ? 1.0f : 0.0f; F_invr *= vdw_in_range; #ifdef CALC_ENERGIES E_lj_p *= vdw_in_range; #endif #endif #ifdef LJ_POT_SWITCH #ifdef CALC_ENERGIES calculate_potential_switch_F_E(nbparam, c6, c12, inv_r, r2, &F_invr, &E_lj_p); #else calculate_potential_switch_F(nbparam, c6, c12, inv_r, r2, &F_invr, &E_lj_p); #endif #endif #ifdef CALC_ENERGIES E_lj += E_lj_p; #endif #ifdef EL_CUTOFF #ifdef EXCLUSION_FORCES F_invr += qi * qj_f * int_bit * inv_r2 * inv_r; #else F_invr += qi * qj_f * inv_r2 * inv_r; #endif #endif #ifdef EL_RF F_invr += qi * qj_f * (int_bit*inv_r2 * inv_r - two_k_rf); #endif #if defined EL_EWALD_ANA F_invr += qi * qj_f * (int_bit*inv_r2*inv_r + pmecorrF(beta2*r2)*beta3); #elif defined EL_EWALD_TAB F_invr += qi * qj_f * (int_bit*inv_r2 - #ifdef USE_TEXOBJ interpolate_coulomb_force_r(nbparam->coulomb_tab_texobj, r2 * inv_r, coulomb_tab_scale) #else interpolate_coulomb_force_r(coulomb_tab_climg2d, r2 * inv_r, coulomb_tab_scale) #endif ) * inv_r; #endif #ifdef CALC_ENERGIES #ifdef EL_CUTOFF E_el += qi * qj_f * (int_bit*inv_r - c_rf); #endif #ifdef EL_RF E_el += qi * qj_f * (int_bit*inv_r + 0.5f * two_k_rf * r2 - c_rf); #endif #ifdef EL_EWALD_ANY E_el += qi * qj_f * (inv_r * (int_bit - erf(r2 * inv_r * beta)) - int_bit * ewald_shift); #endif #endif f_ij = rv * F_invr; fcj_buf -= f_ij; fci_buf[ci_offset] += f_ij; } } mask_ji += mask_ji; } f_buf[ tidx] = fcj_buf.x; f_buf[ FBUF_STRIDE + tidx] = fcj_buf.y; f_buf[2 * FBUF_STRIDE + tidx] = fcj_buf.z; reduce_force_j_generic(f_buf, f, tidxi, tidxj, aj); } } #ifdef PRUNE_NBL pl_cj4[j4].imei[widx].imask = imask; #endif } } for (ci_offset = 0; ci_offset < NCL_PER_SUPERCL; ci_offset++) { ai = (sci * NCL_PER_SUPERCL + ci_offset) * CL_SIZE + tidxi; f_buf[ tidx] = fci_buf[ci_offset].x; f_buf[ FBUF_STRIDE + tidx] = fci_buf[ci_offset].y; f_buf[2 * FBUF_STRIDE + tidx] = fci_buf[ci_offset].z; barrier(CLK_LOCAL_MEM_FENCE); reduce_force_i(f_buf, f, &fshift_buf, bCalcFshift, tidxi, tidxj, ai); barrier(CLK_LOCAL_MEM_FENCE); } if (bCalcFshift) { if (tidxj < 3) atomicAdd_g_f(&(fshift[3 * nb_sci.shift + tidxj]), fshift_buf); } #ifdef CALC_ENERGIES f_buf[ tidx] = E_lj; f_buf[FBUF_STRIDE + tidx] = E_el; reduce_energy_pow2(f_buf + (tidx & WARP_SIZE), e_lj, e_el, tidx & ~WARP_SIZE); #endif } #undef EL_EWALD_ANY #undef EXCLUSION_FORCES #undef LJ_EWALD