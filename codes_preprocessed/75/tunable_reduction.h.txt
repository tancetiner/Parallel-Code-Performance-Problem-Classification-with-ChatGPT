#pragma once #include <tunable_kernel.h> #include <lattice_field.h> #include <reduction_kernel.h> #include <reduction_kernel_host.h> namespace quda { template <int block_size_y = 2> class TunableReduction2D : public TunableKernel { protected: static constexpr bool grid_stride = true; bool tuneGridDim() const final { return grid_stride; } virtual unsigned int minGridSize() const { return Tunable::minGridSize(); } virtual int gridStep() const { return minGridSize(); } virtual unsigned int maxBlockSize(const TuneParam &) const { return device::max_reduce_block_size<block_size_y>(); } template <int block_size_x, template <typename> class Functor, typename FunctorArg> std::enable_if_t<block_size_x != device::warp_size(), qudaError_t> launch(FunctorArg &arg, const TuneParam &tp, const qudaStream_t &stream) { if (tp.block.x == block_size_x) { using Arg = ReduceKernelArg<block_size_x, block_size_y, FunctorArg>; return TunableKernel::launch_device<Functor, grid_stride>(KERNEL(Reduction2D), tp, stream, Arg(arg)); } else { return launch<block_size_x - device::warp_size(), Functor>(arg, tp, stream); } } template <int block_size_x, template <typename> class Functor, typename FunctorArg> std::enable_if_t<block_size_x == device::warp_size(), qudaError_t> launch(FunctorArg &arg, const TuneParam &tp, const qudaStream_t &stream) { if (tp.block.x == block_size_x) { using Arg = ReduceKernelArg<block_size_x, block_size_y, FunctorArg>; return TunableKernel::launch_device<Functor, grid_stride>(KERNEL(Reduction2D), tp, stream, Arg(arg)); } else { errorQuda("Unexpected block size %d\n", tp.block.x); return QUDA_ERROR; } } template <template <typename> class Functor, typename Arg, typename T> void launch_device(std::vector<T> &result, const TuneParam &tp, const qudaStream_t &stream, Arg &arg) { arg.launch_error = launch<device::max_reduce_block_size<block_size_y>(), Functor>(arg, tp, stream); if (!commAsyncReduction()) { arg.complete(result, stream); if (!activeTuning() && commGlobalReduction()) { comm_allreduce_array((double*)result.data(), result.size() * sizeof(T) / sizeof(double)); } } } template <template <typename> class Functor, typename Arg, typename T> void launch_device(T &result, const TuneParam &tp, const qudaStream_t &stream, Arg &arg) { std::vector<T> result_(1); launch_device<Functor>(result_, tp, stream, arg); result = result_[0]; } template <template <typename> class Functor, typename Arg, typename T> void launch_host(std::vector<T> &result, const TuneParam &, const qudaStream_t &, Arg &arg) { using reduce_t = typename Functor<Arg>::reduce_t; reduce_t value = Reduction2D_host<Functor, Arg>(arg); int input_size = vec_length<reduce_t>::value; int output_size = result.size() * vec_length<T>::value; if (output_size != input_size) errorQuda("Input %d and output %d length do not match", input_size, output_size); for (int i = 0; i < output_size; i++) { reinterpret_cast<typename scalar<T>::type*>(result.data())[i] = reinterpret_cast<typename scalar<reduce_t>::type*>(&value)[i]; } if (!activeTuning() && commGlobalReduction()) { comm_allreduce_array((double*)result.data(), result.size() * sizeof(T) / sizeof(double)); } } template <template <typename> class Functor, typename Arg, typename T> void launch_host(T &result, const TuneParam &tp, const qudaStream_t &stream, Arg &arg) { std::vector<T> result_(1); launch_host<Functor>(result_, tp, stream, arg); result = result_[0]; } template <template <typename> class Functor, bool enable_host = false, typename T, typename Arg> std::enable_if_t<!enable_host, void> launch(std::vector<T> &result, const TuneParam &tp, const qudaStream_t &stream, Arg &arg) { if (location == QUDA_CUDA_FIELD_LOCATION) { launch_device<Functor>(result, tp, stream, arg); } else { errorQuda("CPU not supported yet"); } } template <template <typename> class Functor, bool enable_host = false, typename T, typename Arg> std::enable_if_t<enable_host, void> launch(std::vector<T> &result, const TuneParam &tp, const qudaStream_t &stream, Arg &arg) { if (location == QUDA_CUDA_FIELD_LOCATION) { launch_device<Functor>(result, tp, stream, arg); } else { launch_host<Functor>(result, tp, stream, arg); } } template <template <typename> class Functor, typename T, typename Arg> void launch(T &result, const TuneParam &tp, const qudaStream_t &stream, Arg &arg) { std::vector<T> result_(1); launch<Functor>(result_, tp, stream, arg); result = result_[0]; } public: TunableReduction2D(const LatticeField &field, QudaFieldLocation location = QUDA_INVALID_FIELD_LOCATION) : TunableKernel(location != QUDA_INVALID_FIELD_LOCATION ? location : field.Location()) { strcpy(vol, field.VolString()); strcpy(aux, compile_type_str(field, location)); strcat(aux, field.AuxString()); #ifdef QUDA_FAST_COMPILE_REDUCE strcat(aux, ",fast_compile"); #endif } TunableReduction2D(size_t n_items, QudaFieldLocation location = QUDA_INVALID_FIELD_LOCATION) : TunableKernel(location) { u64toa(vol, n_items); strcpy(aux, compile_type_str(location)); } virtual bool advanceBlockDim(TuneParam &param) const { bool rtn = Tunable::advanceBlockDim(param); param.block.y = block_size_y; return rtn; } virtual void initTuneParam(TuneParam &param) const { Tunable::initTuneParam(param); param.block.y = block_size_y; } virtual void defaultTuneParam(TuneParam &param) const { Tunable::defaultTuneParam(param); param.block.y = block_size_y; } }; template <int block_size_y = 1> class TunableMultiReduction : public TunableReduction2D<block_size_y> { static_assert(block_size_y == 1, "only block_size_y = 1 supported"); using Tunable::apply; using TunableReduction2D<block_size_y>::location; using TunableReduction2D<block_size_y>::grid_stride; protected: const int n_batch; virtual unsigned int minGridSize() const { return Tunable::minGridSize(); } virtual int gridStep() const { return Tunable::gridStep(); } unsigned int maxBlockSize(const TuneParam &) const { return device::max_multi_reduce_block_size(); } template <int block_size_x, template <typename> class Functor, typename FunctorArg> std::enable_if_t<block_size_x != device::warp_size(), qudaError_t> launch(FunctorArg &arg, const TuneParam &tp, const qudaStream_t &stream) { if (tp.block.x == block_size_x) { using Arg = ReduceKernelArg<block_size_x, block_size_y, FunctorArg>; return TunableKernel::launch_device<Functor, grid_stride>(KERNEL(MultiReduction), tp, stream, Arg(arg)); } else { return launch<block_size_x - device::warp_size(), Functor>(arg, tp, stream); } } template <int block_size_x, template <typename> class Functor, typename FunctorArg> std::enable_if_t<block_size_x == device::warp_size(), qudaError_t> launch(FunctorArg &arg, const TuneParam &tp, const qudaStream_t &stream) { if (tp.block.x == block_size_x) { using Arg = ReduceKernelArg<block_size_x, block_size_y, FunctorArg>; return TunableKernel::launch_device<Functor, grid_stride>(KERNEL(MultiReduction), tp, stream, Arg(arg)); } else { errorQuda("Unexpected block size %d\n", tp.block.x); return QUDA_ERROR; } } template <template <typename> class Functor, typename Arg, typename T> void launch_device(std::vector<T> &result, const TuneParam &tp, const qudaStream_t &stream, Arg &arg) { arg.launch_error = launch<device::max_multi_reduce_block_size(), Functor>(arg, tp, stream); if (!commAsyncReduction()) { arg.complete(result, stream); #if 0 if (!activeTuning() && commGlobalReduction()) { comm_allreduce_array((double*)result.data(), result.size() * sizeof(T) / sizeof(double)); } #endif } } template <template <typename> class Functor, typename Arg, typename T> void launch_host(std::vector<T> &result, const TuneParam &, const qudaStream_t &, Arg &arg) { using reduce_t = typename Functor<Arg>::reduce_t; int input_size = vec_length<reduce_t>::value; int output_size = vec_length<T>::value; if (output_size != input_size) errorQuda("Input %d and output %d length do not match", input_size, output_size); auto value = MultiReduction_host<Functor, Arg>(arg); for (int j = 0; j < (int)arg.threads.y; j++) { for (int i = 0; i < output_size; i++) { reinterpret_cast<typename scalar<T>::type*>(&result[j])[i] = reinterpret_cast<typename scalar<reduce_t>::type*>(&value[j])[i]; } } #if 0 if (!activeTuning() && commGlobalReduction()) { comm_allreduce_array((double*)result.data(), result.size() * sizeof(T) / sizeof(double)); } #endif } template <template <typename> class Functor, bool enable_host = false, typename T, typename Arg> std::enable_if_t<!enable_host, void> launch(std::vector<T> &result, const TuneParam &tp, const qudaStream_t &stream, Arg &arg) { if (location == QUDA_CUDA_FIELD_LOCATION) { launch_device<Functor>(result, tp, stream, arg); } else { errorQuda("CPU not supported yet"); } } template <template <typename> class Functor, bool enable_host, typename T, typename Arg> std::enable_if_t<enable_host, void> launch(std::vector<T> &result, const TuneParam &tp, const qudaStream_t &stream, Arg &arg) { if (location == QUDA_CUDA_FIELD_LOCATION) { launch_device<Functor>(result, tp, stream, arg); } else { launch_host<Functor>(result, tp, stream, arg); } } public: TunableMultiReduction(const LatticeField &field, int n_batch, QudaFieldLocation location = QUDA_INVALID_FIELD_LOCATION) : TunableReduction2D<block_size_y>(field, location), n_batch(n_batch) { } TunableMultiReduction(size_t n_items, int n_batch, QudaFieldLocation location = QUDA_INVALID_FIELD_LOCATION) : TunableReduction2D<block_size_y>(n_items, location), n_batch(n_batch) { } bool advanceBlockDim(TuneParam &param) const { return Tunable::advanceBlockDim(param); } void initTuneParam(TuneParam &param) const { Tunable::initTuneParam(param); param.block.y = 1; param.grid.y = n_batch; } void defaultTuneParam(TuneParam &param) const { Tunable::defaultTuneParam(param); param.block.y = 1; param.grid.y = n_batch; } }; }