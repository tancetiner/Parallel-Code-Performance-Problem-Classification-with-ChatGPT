#pragma once #include <type_traits> #include <algorithm> #ifdef _NVHPC_CUDA #include <nv/target> #endif #if defined(__CUDACC__) || defined(_NVHPC_CUDA) || (defined(__clang__) && defined(__CUDA__)) #define QUDA_CUDA_CC #endif namespace quda { namespace target { #ifdef _NVHPC_CUDA template <template <bool, typename ...> class f, typename ...Args> __host__ __device__ auto dispatch(Args &&... args) { if target (nv::target::is_device) { return f<true>()(args...); } else if target (nv::target::is_host) { return f<false>()(args...); } } #else template <template <bool, typename ...> class f, typename ...Args> __host__ __device__ auto dispatch(Args &&... args) { #ifdef __CUDA_ARCH__ return f<true>()(args...); #else return f<false>()(args...); #endif } #endif template <bool is_device> struct is_device_impl { constexpr bool operator()() { return false; } }; template <> struct is_device_impl<true> { constexpr bool operator()() { return true; } }; __device__ __host__ inline bool is_device() { return dispatch<is_device_impl>(); } template <bool is_device> struct is_host_impl { constexpr bool operator()() { return true; } }; template <> struct is_host_impl<true> { constexpr bool operator()() { return false; } }; __device__ __host__ inline bool is_host() { return dispatch<is_host_impl>(); } template <bool is_device> struct block_dim_impl { dim3 operator()() { return dim3(1, 1, 1); } }; #ifdef QUDA_CUDA_CC template <> struct block_dim_impl<true> { __device__ dim3 operator()() { return dim3(blockDim.x, blockDim.y, blockDim.z); } }; #endif __device__ __host__ inline dim3 block_dim() { return dispatch<block_dim_impl>(); } template <bool is_device> struct grid_dim_impl { dim3 operator()() { return dim3(1, 1, 1); } }; #ifdef QUDA_CUDA_CC template <> struct grid_dim_impl<true> { __device__ dim3 operator()() { return dim3(gridDim.x, gridDim.y, gridDim.z); } }; #endif __device__ __host__ inline dim3 grid_dim() { return dispatch<grid_dim_impl>(); } template <bool is_device> struct block_idx_impl { dim3 operator()() { return dim3(0, 0, 0); } }; #ifdef QUDA_CUDA_CC template <> struct block_idx_impl<true> { __device__ dim3 operator()() { return dim3(blockIdx.x, blockIdx.y, blockIdx.z); } }; #endif __device__ __host__ inline dim3 block_idx() { return dispatch<block_idx_impl>(); } template <bool is_device> struct thread_idx_impl { dim3 operator()() { return dim3(0, 0, 0); } }; #ifdef QUDA_CUDA_CC template <> struct thread_idx_impl<true> { __device__ dim3 operator()() { return dim3(threadIdx.x, threadIdx.y, threadIdx.z); } }; #endif __device__ __host__ inline dim3 thread_idx() { return dispatch<thread_idx_impl>(); } } namespace device { constexpr int warp_size() { return 32; } constexpr unsigned int warp_converged_mask() { return 0xffffffff; } template <int block_size_y = 1, int block_size_z = 1> constexpr unsigned int max_block_size() { return std::max(warp_size(), 1024 / (block_size_y * block_size_z)); } template <int block_size_y = 1, int block_size_z = 1> constexpr unsigned int max_reduce_block_size() { #ifdef QUDA_FAST_COMPILE_REDUCE return warp_size(); #else return max_block_size<block_size_y, block_size_z>(); #endif } constexpr unsigned int max_multi_reduce_block_size() { #ifdef QUDA_FAST_COMPILE_REDUCE return warp_size(); #else return 128; #endif } constexpr size_t max_constant_size() { return 32768; } constexpr size_t max_kernel_arg_size() { return 4096; } constexpr int shared_memory_bank_width() { return 32; } template <typename Arg> constexpr bool use_kernel_arg() { return (sizeof(Arg) <= device::max_kernel_arg_size() && Arg::use_kernel_arg); } template <typename Arg> constexpr std::enable_if_t<use_kernel_arg<Arg>(), const Arg&> get_arg() { return reinterpret_cast<Arg&>(nullptr); } template <typename Arg> constexpr std::enable_if_t<use_kernel_arg<Arg>(), void *> get_constant_buffer() { return nullptr; } } } #undef QUDA_CUDA_CC