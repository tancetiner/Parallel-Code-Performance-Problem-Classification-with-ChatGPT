#include "gromacs/gpu_utils/cuda_arch_utils.cuh" #include "gromacs/gpu_utils/cuda_kernel_utils.cuh" #include "gromacs/gpu_utils/typecasts.cuh" #include "gromacs/math/units.h" #include "gromacs/math/utilities.h" #include "gromacs/pbcutil/ishift.h" #if defined EL_EWALD_ANA || defined EL_EWALD_TAB # define EL_EWALD_ANY #endif #if defined LJ_EWALD_COMB_GEOM || defined LJ_EWALD_COMB_LB # define LJ_EWALD #endif #if defined EL_EWALD_ANY || defined EL_RF || defined LJ_EWALD \ || (defined EL_CUTOFF && defined CALC_ENERGIES) # define EXCLUSION_FORCES #endif #if defined LJ_COMB_GEOM || defined LJ_COMB_LB # define LJ_COMB #endif #if GMX_PTX_ARCH == 370 # define NTHREAD_Z (2) # define MIN_BLOCKS_PER_MP (16) #else # define NTHREAD_Z (1) # define MIN_BLOCKS_PER_MP (16) #endif #define THREADS_PER_BLOCK (c_clSize * c_clSize * NTHREAD_Z) #if GMX_PTX_ARCH >= 350 __launch_bounds__(THREADS_PER_BLOCK, MIN_BLOCKS_PER_MP) #else __launch_bounds__(THREADS_PER_BLOCK) #endif #ifdef PRUNE_NBL # ifdef CALC_ENERGIES __global__ void NB_KERNEL_FUNC_NAME(nbnxn_kernel, _VF_prune_cuda) # else __global__ void NB_KERNEL_FUNC_NAME(nbnxn_kernel, _F_prune_cuda) # endif #else # ifdef CALC_ENERGIES __global__ void NB_KERNEL_FUNC_NAME(nbnxn_kernel, _VF_cuda) # else __global__ void NB_KERNEL_FUNC_NAME(nbnxn_kernel, _F_cuda) # endif #endif (NBAtomDataGpu atdat, NBParamGpu nbparam, Nbnxm::gpu_plist plist, bool bCalcFshift) #ifdef FUNCTION_DECLARATION_ONLY ; #else { const nbnxn_sci_t* pl_sci = plist.sci; # ifndef PRUNE_NBL const # endif nbnxn_cj4_t* pl_cj4 = plist.cj4; const nbnxn_excl_t* excl = plist.excl; # ifndef LJ_COMB const int* atom_types = atdat.atomTypes; int ntypes = atdat.numTypes; # else const float2* lj_comb = atdat.ljComb; float2 ljcp_i, ljcp_j; # endif const float4* xq = atdat.xq; float3* f = asFloat3(atdat.f); const float3* shift_vec = asFloat3(atdat.shiftVec); float rcoulomb_sq = nbparam.rcoulomb_sq; # ifdef VDW_CUTOFF_CHECK float rvdw_sq = nbparam.rvdw_sq; float vdw_in_range; # endif # ifdef LJ_EWALD float lje_coeff2, lje_coeff6_6; # endif # ifdef EL_RF float two_k_rf = nbparam.two_k_rf; # endif # ifdef EL_EWALD_ANA float beta2 = nbparam.ewald_beta * nbparam.ewald_beta; float beta3 = nbparam.ewald_beta * nbparam.ewald_beta * nbparam.ewald_beta; # endif # ifdef PRUNE_NBL float rlist_sq = nbparam.rlistOuter_sq; # endif # ifdef CALC_ENERGIES # ifdef EL_EWALD_ANY float beta = nbparam.ewald_beta; float ewald_shift = nbparam.sh_ewald; # else float reactionFieldShift = nbparam.c_rf; # endif float* e_lj = atdat.eLJ; float* e_el = atdat.eElec; # endif unsigned int tidxi = threadIdx.x; unsigned int tidxj = threadIdx.y; unsigned int tidx = threadIdx.y * blockDim.x + threadIdx.x; # if NTHREAD_Z == 1 unsigned int tidxz = 0; # else unsigned int tidxz = threadIdx.z; # endif unsigned int bidx = blockIdx.x; unsigned int widx = tidx / warp_size; int sci, ci, cj, ai, aj, cij4_start, cij4_end; # ifndef LJ_COMB int typei, typej; # endif int i, jm, j4, wexcl_idx; float qi, qj_f, r2, inv_r, inv_r2; # if !defined LJ_COMB_LB || defined CALC_ENERGIES float inv_r6, c6, c12; # endif # ifdef LJ_COMB_LB float sigma, epsilon; # endif float int_bit, F_invr; # ifdef CALC_ENERGIES float E_lj, E_el; # endif # if defined CALC_ENERGIES || defined LJ_POT_SWITCH float E_lj_p; # endif unsigned int wexcl, imask, mask_ji; float4 xqbuf; float3 xi, xj, rv, f_ij, fcj_buf; float3 fci_buf[c_nbnxnGpuNumClusterPerSupercluster]; nbnxn_sci_t nb_sci; const unsigned superClInteractionMask = ((1U << c_nbnxnGpuNumClusterPerSupercluster) - 1U); constexpr bool c_preloadCj = (GMX_PTX_ARCH < 700 || GMX_PTX_ARCH == 750); extern __shared__ char sm_dynamicShmem[]; char* sm_nextSlotPtr = sm_dynamicShmem; static_assert(sizeof(char) == 1, "The shared memory offset calculation assumes that char is 1 byte"); float4* xqib = reinterpret_cast<float4*>(sm_nextSlotPtr); sm_nextSlotPtr += (c_nbnxnGpuNumClusterPerSupercluster * c_clSize * sizeof(*xqib)); int* cjs = reinterpret_cast<int*>(sm_nextSlotPtr); if (c_preloadCj) { cjs += tidxz * c_nbnxnGpuClusterpairSplit * c_nbnxnGpuJgroupSize; sm_nextSlotPtr += (NTHREAD_Z * c_nbnxnGpuClusterpairSplit * c_nbnxnGpuJgroupSize * sizeof(*cjs)); } # ifndef LJ_COMB int* atib = reinterpret_cast<int*>(sm_nextSlotPtr); sm_nextSlotPtr += (c_nbnxnGpuNumClusterPerSupercluster * c_clSize * sizeof(*atib)); # else float2* ljcpib = reinterpret_cast<float2*>(sm_nextSlotPtr); sm_nextSlotPtr += (c_nbnxnGpuNumClusterPerSupercluster * c_clSize * sizeof(*ljcpib)); # endif nb_sci = pl_sci[bidx]; sci = nb_sci.sci; cij4_start = nb_sci.cj4_ind_start; cij4_end = nb_sci.cj4_ind_end; if (tidxz == 0) { ci = sci * c_nbnxnGpuNumClusterPerSupercluster + tidxj; ai = ci * c_clSize + tidxi; const float* shiftptr = reinterpret_cast<const float*>(&shift_vec[nb_sci.shift]); xqbuf = xq[ai] + make_float4(LDG(shiftptr), LDG(shiftptr + 1), LDG(shiftptr + 2), 0.0F); xqbuf.w *= nbparam.epsfac; xqib[tidxj * c_clSize + tidxi] = xqbuf; # ifndef LJ_COMB atib[tidxj * c_clSize + tidxi] = atom_types[ai]; # else ljcpib[tidxj * c_clSize + tidxi] = lj_comb[ai]; # endif } __syncthreads(); for (i = 0; i < c_nbnxnGpuNumClusterPerSupercluster; i++) { fci_buf[i] = make_float3(0.0F); } # ifdef LJ_EWALD lje_coeff2 = nbparam.ewaldcoeff_lj * nbparam.ewaldcoeff_lj; lje_coeff6_6 = lje_coeff2 * lje_coeff2 * lje_coeff2 * c_oneSixth; # endif # ifdef CALC_ENERGIES E_lj = 0.0F; E_el = 0.0F; # ifdef EXCLUSION_FORCES if (nb_sci.shift == gmx::c_centralShiftIndex && pl_cj4[cij4_start].cj[0] == sci * c_nbnxnGpuNumClusterPerSupercluster) { for (i = 0; i < c_nbnxnGpuNumClusterPerSupercluster; i++) { # if defined EL_EWALD_ANY || defined EL_RF || defined EL_CUTOFF qi = xqib[i * c_clSize + tidxi].w; E_el += qi * qi; # endif # ifdef LJ_EWALD E_lj += LDG(reinterpret_cast<float*>( &nbparam.nbfp[atom_types[(sci * c_nbnxnGpuNumClusterPerSupercluster + i) * c_clSize + tidxi] * (ntypes + 1)])); # endif } # ifdef LJ_EWALD E_lj /= c_clSize * NTHREAD_Z; E_lj *= 0.5F * c_oneSixth * lje_coeff6_6; # endif # if defined EL_EWALD_ANY || defined EL_RF || defined EL_CUTOFF E_el /= nbparam.epsfac * c_clSize * NTHREAD_Z; # if defined EL_RF || defined EL_CUTOFF E_el *= -0.5F * reactionFieldShift; # else E_el *= -beta * M_FLOAT_1_SQRTPI; # endif # endif } # endif # endif # ifdef EXCLUSION_FORCES const int nonSelfInteraction = !(nb_sci.shift == gmx::c_centralShiftIndex & tidxj <= tidxi); # endif for (j4 = cij4_start + tidxz; j4 < cij4_end; j4 += NTHREAD_Z) { wexcl_idx = pl_cj4[j4].imei[widx].excl_ind; imask = pl_cj4[j4].imei[widx].imask; wexcl = excl[wexcl_idx].pair[(tidx) & (warp_size - 1)]; # ifndef PRUNE_NBL if (imask) # endif { if (c_preloadCj) { if ((tidxj == 0 | tidxj == 4) & (tidxi < c_nbnxnGpuJgroupSize)) { cjs[tidxi + tidxj * c_nbnxnGpuJgroupSize / c_splitClSize] = pl_cj4[j4].cj[tidxi]; } __syncwarp(c_fullWarpMask); } for (jm = 0; jm < c_nbnxnGpuJgroupSize; jm++) { if (imask & (superClInteractionMask << (jm * c_nbnxnGpuNumClusterPerSupercluster))) { mask_ji = (1U << (jm * c_nbnxnGpuNumClusterPerSupercluster)); cj = c_preloadCj ? cjs[jm + (tidxj & 4) * c_nbnxnGpuJgroupSize / c_splitClSize] : cj = pl_cj4[j4].cj[jm]; aj = cj * c_clSize + tidxj; xqbuf = xq[aj]; xj = make_float3(xqbuf.x, xqbuf.y, xqbuf.z); qj_f = xqbuf.w; # ifndef LJ_COMB typej = atom_types[aj]; # else ljcp_j = lj_comb[aj]; # endif fcj_buf = make_float3(0.0F); # if !defined PRUNE_NBL # pragma unroll 8 # endif for (i = 0; i < c_nbnxnGpuNumClusterPerSupercluster; i++) { if (imask & mask_ji) { ci = sci * c_nbnxnGpuNumClusterPerSupercluster + i; xqbuf = xqib[i * c_clSize + tidxi]; xi = make_float3(xqbuf.x, xqbuf.y, xqbuf.z); rv = xi - xj; r2 = norm2(rv); # ifdef PRUNE_NBL if (!__any_sync(c_fullWarpMask, r2 < rlist_sq)) { imask &= ~mask_ji; } # endif int_bit = (wexcl & mask_ji) ? 1.0F : 0.0F; # ifdef EXCLUSION_FORCES if ((r2 < rcoulomb_sq) * (nonSelfInteraction | (ci != cj))) # else if ((r2 < rcoulomb_sq) * int_bit) # endif { qi = xqbuf.w; # ifndef LJ_COMB typei = atib[i * c_clSize + tidxi]; fetch_nbfp_c6_c12(c6, c12, nbparam, ntypes * typei + typej); # else ljcp_i = ljcpib[i * c_clSize + tidxi]; # ifdef LJ_COMB_GEOM c6 = ljcp_i.x * ljcp_j.x; c12 = ljcp_i.y * ljcp_j.y; # else sigma = ljcp_i.x + ljcp_j.x; epsilon = ljcp_i.y * ljcp_j.y; # if defined CALC_ENERGIES || defined LJ_FORCE_SWITCH || defined LJ_POT_SWITCH convert_sigma_epsilon_to_c6_c12(sigma, epsilon, &c6, &c12); # endif # endif # endif r2 = max(r2, c_nbnxnMinDistanceSquared); inv_r = rsqrt(r2); inv_r2 = inv_r * inv_r; # if !defined LJ_COMB_LB || defined CALC_ENERGIES inv_r6 = inv_r2 * inv_r2 * inv_r2; # ifdef EXCLUSION_FORCES inv_r6 *= int_bit; # endif F_invr = inv_r6 * (c12 * inv_r6 - c6) * inv_r2; # if defined CALC_ENERGIES || defined LJ_POT_SWITCH E_lj_p = int_bit * (c12 * (inv_r6 * inv_r6 + nbparam.repulsion_shift.cpot) * c_oneTwelveth - c6 * (inv_r6 + nbparam.dispersion_shift.cpot) * c_oneSixth); # endif # else float sig_r = sigma * inv_r; float sig_r2 = sig_r * sig_r; float sig_r6 = sig_r2 * sig_r2 * sig_r2; # ifdef EXCLUSION_FORCES sig_r6 *= int_bit; # endif F_invr = epsilon * sig_r6 * (sig_r6 - 1.0F) * inv_r2; # endif # ifdef LJ_FORCE_SWITCH # ifdef CALC_ENERGIES calculate_force_switch_F_E(nbparam, c6, c12, inv_r, r2, &F_invr, &E_lj_p); # else calculate_force_switch_F(nbparam, c6, c12, inv_r, r2, &F_invr); # endif # endif # ifdef LJ_EWALD # ifdef LJ_EWALD_COMB_GEOM # ifdef CALC_ENERGIES calculate_lj_ewald_comb_geom_F_E( nbparam, typei, typej, r2, inv_r2, lje_coeff2, lje_coeff6_6, int_bit, &F_invr, &E_lj_p); # else calculate_lj_ewald_comb_geom_F( nbparam, typei, typej, r2, inv_r2, lje_coeff2, lje_coeff6_6, &F_invr); # endif # elif defined LJ_EWALD_COMB_LB calculate_lj_ewald_comb_LB_F_E(nbparam, typei, typej, r2, inv_r2, lje_coeff2, lje_coeff6_6, # ifdef CALC_ENERGIES int_bit, &F_invr, &E_lj_p # else 0, &F_invr, nullptr # endif ); # endif # endif # ifdef LJ_POT_SWITCH # ifdef CALC_ENERGIES calculate_potential_switch_F_E(nbparam, inv_r, r2, &F_invr, &E_lj_p); # else calculate_potential_switch_F(nbparam, inv_r, r2, &F_invr, &E_lj_p); # endif # endif # ifdef VDW_CUTOFF_CHECK vdw_in_range = (r2 < rvdw_sq) ? 1.0F : 0.0F; F_invr *= vdw_in_range; # ifdef CALC_ENERGIES E_lj_p *= vdw_in_range; # endif # endif # ifdef CALC_ENERGIES E_lj += E_lj_p; # endif # ifdef EL_CUTOFF # ifdef EXCLUSION_FORCES F_invr += qi * qj_f * int_bit * inv_r2 * inv_r; # else F_invr += qi * qj_f * inv_r2 * inv_r; # endif # endif # ifdef EL_RF F_invr += qi * qj_f * (int_bit * inv_r2 * inv_r - two_k_rf); # endif # if defined EL_EWALD_ANA F_invr += qi * qj_f * (int_bit * inv_r2 * inv_r + pmecorrF(beta2 * r2) * beta3); # elif defined EL_EWALD_TAB F_invr += qi * qj_f * (int_bit * inv_r2 - interpolate_coulomb_force_r(nbparam, r2 * inv_r)) * inv_r; # endif # ifdef CALC_ENERGIES # ifdef EL_CUTOFF E_el += qi * qj_f * (int_bit * inv_r - reactionFieldShift); # endif # ifdef EL_RF E_el += qi * qj_f * (int_bit * inv_r + 0.5F * two_k_rf * r2 - reactionFieldShift); # endif # ifdef EL_EWALD_ANY E_el += qi * qj_f * (inv_r * (int_bit - erff(r2 * inv_r * beta)) - int_bit * ewald_shift); # endif # endif f_ij = rv * F_invr; fcj_buf -= f_ij; fci_buf[i] += f_ij; } } mask_ji += mask_ji; } reduce_force_j_warp_shfl(fcj_buf, f, tidxi, aj, c_fullWarpMask); } } # ifdef PRUNE_NBL pl_cj4[j4].imei[widx].imask = imask; # endif } if (c_preloadCj) { __syncwarp(c_fullWarpMask); } } if (nb_sci.shift == gmx::c_centralShiftIndex) { bCalcFshift = false; } float fshift_buf = 0.0F; for (i = 0; i < c_nbnxnGpuNumClusterPerSupercluster; i++) { ai = (sci * c_nbnxnGpuNumClusterPerSupercluster + i) * c_clSize + tidxi; reduce_force_i_warp_shfl(fci_buf[i], f, &fshift_buf, bCalcFshift, tidxj, ai, c_fullWarpMask); } if (bCalcFshift && (tidxj & 3) < 3) { float3* fShift = asFloat3(atdat.fShift); atomicAdd(&(fShift[nb_sci.shift].x) + (tidxj & 3), fshift_buf); } # ifdef CALC_ENERGIES reduce_energy_warp_shfl(E_lj, E_el, e_lj, e_el, tidx, c_fullWarpMask); # endif } #endif #undef NTHREAD_Z #undef MIN_BLOCKS_PER_MP #undef THREADS_PER_BLOCK #undef EL_EWALD_ANY #undef EXCLUSION_FORCES #undef LJ_EWALD #undef LJ_COMB