#include "gmxpre.h" #include <assert.h> #include <math.h> #include <stdarg.h> #include <stdio.h> #include <stdlib.h> #include <string.h> #include "gromacs/gpu_utils/gpu_utils.h" #include "gromacs/gpu_utils/oclutils.h" #include "gromacs/hardware/detecthardware.h" #include "gromacs/hardware/gpu_hw_info.h" #include "gromacs/math/vectypes.h" #include "gromacs/mdlib/force_flags.h" #include "gromacs/mdlib/nb_verlet.h" #include "gromacs/mdlib/nbnxn_consts.h" #include "gromacs/mdlib/nbnxn_gpu.h" #include "gromacs/mdlib/nbnxn_gpu_data_mgmt.h" #include "gromacs/mdlib/nbnxn_gpu_jit_support.h" #include "gromacs/mdtypes/interaction_const.h" #include "gromacs/mdtypes/md_enums.h" #include "gromacs/pbcutil/ishift.h" #include "gromacs/timing/gpu_timing.h" #include "gromacs/utility/cstringutil.h" #include "gromacs/utility/fatalerror.h" #include "gromacs/utility/gmxassert.h" #include "gromacs/utility/real.h" #include "gromacs/utility/smalloc.h" #include "nbnxn_ocl_types.h" static unsigned int gpu_min_ci_balanced_factor = 50; void ocl_free_buffered(cl_mem d_ptr, int *n, int *nalloc) { cl_int gmx_unused cl_error; if (d_ptr) { cl_error = clReleaseMemObject(d_ptr); assert(cl_error == CL_SUCCESS); } if (n) { *n = -1; } if (nalloc) { *nalloc = -1; } } void ocl_realloc_buffered(cl_mem *d_dest, void *h_src, size_t type_size, int *curr_size, int *curr_alloc_size, int req_size, cl_context context, cl_command_queue s, bool bAsync = true, cl_event *copy_event = NULL) { cl_int cl_error; if (d_dest == NULL || req_size < 0) { return; } if (req_size > *curr_alloc_size) { if (*curr_alloc_size >= 0) { ocl_free_buffered(*d_dest, curr_size, curr_alloc_size); } *curr_alloc_size = over_alloc_large(req_size); *d_dest = clCreateBuffer(context, CL_MEM_READ_WRITE, *curr_alloc_size * type_size, NULL, &cl_error); assert(cl_error == CL_SUCCESS); } *curr_size = req_size; if (h_src) { if (bAsync) { ocl_copy_H2D_async(*d_dest, h_src, 0, *curr_size * type_size, s, copy_event); } else { ocl_copy_H2D(*d_dest, h_src, 0, *curr_size * type_size, s); } } } static void free_ocl_buffer(cl_mem *buffer) { cl_int gmx_unused cl_error; assert(NULL != buffer); if (*buffer) { cl_error = clReleaseMemObject(*buffer); assert(CL_SUCCESS == cl_error); *buffer = NULL; } } static void init_ewald_coulomb_force_table(const interaction_const_t *ic, cl_nbparam_t *nbp, const gmx_device_info_t *dev_info) { cl_mem coul_tab; cl_int cl_error; if (nbp->coulomb_tab_climg2d != NULL) { free_ocl_buffer(&(nbp->coulomb_tab_climg2d)); } coul_tab = clCreateBuffer(dev_info->context, CL_MEM_READ_ONLY | CL_MEM_COPY_HOST_PTR, ic->tabq_size*sizeof(cl_float), ic->tabq_coul_F, &cl_error); assert(cl_error == CL_SUCCESS); nbp->coulomb_tab_climg2d = coul_tab; nbp->coulomb_tab_size = ic->tabq_size; nbp->coulomb_tab_scale = ic->tabq_scale; } static void init_atomdata_first(cl_atomdata_t *ad, int ntypes, gmx_device_info_t *dev_info) { cl_int cl_error; ad->ntypes = ntypes; ad->shift_vec_elem_size = sizeof(*(((nbnxn_atomdata_t*)0)->shift_vec)); ad->shift_vec = clCreateBuffer(dev_info->context, CL_MEM_READ_WRITE, SHIFTS * ad->shift_vec_elem_size, NULL, &cl_error); assert(cl_error == CL_SUCCESS); ad->bShiftVecUploaded = false; ad->fshift_elem_size = sizeof(*(((cl_nb_staging_t*)0)->fshift)); ad->fshift = clCreateBuffer(dev_info->context, CL_MEM_READ_WRITE, SHIFTS * ad->fshift_elem_size, NULL, &cl_error); assert(cl_error == CL_SUCCESS); ad->e_lj = clCreateBuffer(dev_info->context, CL_MEM_READ_WRITE, sizeof(float), NULL, &cl_error); assert(cl_error == CL_SUCCESS); ad->e_el = clCreateBuffer(dev_info->context, CL_MEM_READ_WRITE, sizeof(float), NULL, &cl_error); assert(cl_error == CL_SUCCESS); ad->xq = NULL; ad->f = NULL; ad->natoms = -1; ad->nalloc = -1; } static void set_cutoff_parameters(cl_nbparam_t *nbp, const interaction_const_t *ic) { nbp->ewald_beta = ic->ewaldcoeff_q; nbp->sh_ewald = ic->sh_ewald; nbp->epsfac = ic->epsfac; nbp->two_k_rf = 2.0 * ic->k_rf; nbp->c_rf = ic->c_rf; nbp->rvdw_sq = ic->rvdw * ic->rvdw; nbp->rcoulomb_sq = ic->rcoulomb * ic->rcoulomb; nbp->rlist_sq = ic->rlist * ic->rlist; nbp->sh_lj_ewald = ic->sh_lj_ewald; nbp->ewaldcoeff_lj = ic->ewaldcoeff_lj; nbp->rvdw_switch = ic->rvdw_switch; nbp->dispersion_shift = ic->dispersion_shift; nbp->repulsion_shift = ic->repulsion_shift; nbp->vdw_switch = ic->vdw_switch; } static void map_interaction_types_to_gpu_kernel_flavors(const interaction_const_t *ic, int *gpu_eeltype, int *gpu_vdwtype) { if (ic->vdwtype == evdwCUT) { switch (ic->vdw_modifier) { case eintmodNONE: case eintmodPOTSHIFT: *gpu_vdwtype = evdwOclCUT; break; case eintmodFORCESWITCH: *gpu_vdwtype = evdwOclFSWITCH; break; case eintmodPOTSWITCH: *gpu_vdwtype = evdwOclPSWITCH; break; default: gmx_incons("The requested VdW interaction modifier is not implemented in the GPU accelerated kernels!"); break; } } else if (ic->vdwtype == evdwPME) { if (ic->ljpme_comb_rule == ljcrGEOM) { *gpu_vdwtype = evdwOclEWALDGEOM; } else { *gpu_vdwtype = evdwOclEWALDLB; } } else { gmx_incons("The requested VdW type is not implemented in the GPU accelerated kernels!"); } if (ic->eeltype == eelCUT) { *gpu_eeltype = eelOclCUT; } else if (EEL_RF(ic->eeltype)) { *gpu_eeltype = eelOclRF; } else if ((EEL_PME(ic->eeltype) || ic->eeltype == eelEWALD)) { *gpu_eeltype = nbnxn_gpu_pick_ewald_kernel_type(false); } else { gmx_incons("The requested electrostatics type is not implemented in the GPU accelerated kernels!"); } } static void init_nbparam(cl_nbparam_t *nbp, const interaction_const_t *ic, const nbnxn_atomdata_t *nbat, const gmx_device_info_t *dev_info) { int ntypes, nnbfp, nnbfp_comb; cl_int cl_error; ntypes = nbat->ntype; set_cutoff_parameters(nbp, ic); map_interaction_types_to_gpu_kernel_flavors(ic, &(nbp->eeltype), &(nbp->vdwtype)); if (ic->vdwtype == evdwPME) { if (ic->ljpme_comb_rule == ljcrGEOM) { assert(nbat->comb_rule == ljcrGEOM); } else { assert(nbat->comb_rule == ljcrLB); } } nbp->coulomb_tab_climg2d = NULL; if (nbp->eeltype == eelOclEWALD_TAB || nbp->eeltype == eelOclEWALD_TAB_TWIN) { init_ewald_coulomb_force_table(ic, nbp, dev_info); } else { nbp->coulomb_tab_climg2d = clCreateBuffer(dev_info->context, CL_MEM_READ_ONLY, sizeof(cl_float), NULL, &cl_error); } nnbfp = 2*ntypes*ntypes; nnbfp_comb = 2*ntypes; { nbp->nbfp_climg2d = clCreateBuffer(dev_info->context, CL_MEM_READ_ONLY | CL_MEM_COPY_HOST_PTR, nnbfp*sizeof(cl_float), nbat->nbfp, &cl_error); assert(cl_error == CL_SUCCESS); if (ic->vdwtype == evdwPME) { nbp->nbfp_comb_climg2d = clCreateBuffer(dev_info->context, CL_MEM_READ_ONLY | CL_MEM_COPY_HOST_PTR, nnbfp_comb*sizeof(cl_float), nbat->nbfp_comb, &cl_error); assert(cl_error == CL_SUCCESS); } else { nbp->nbfp_comb_climg2d = clCreateBuffer(dev_info->context, CL_MEM_READ_ONLY, sizeof(cl_float), NULL, &cl_error); assert(cl_error == CL_SUCCESS); } } } void nbnxn_gpu_pme_loadbal_update_param(const nonbonded_verlet_t *nbv, const interaction_const_t *ic) { if (!nbv || nbv->grp[0].kernel_type != nbnxnk8x8x8_GPU) { return; } gmx_nbnxn_ocl_t *nb = nbv->gpu_nbv; cl_nbparam_t *nbp = nb->nbparam; set_cutoff_parameters(nbp, ic); nbp->eeltype = nbnxn_gpu_pick_ewald_kernel_type(ic->rcoulomb != ic->rvdw); init_ewald_coulomb_force_table(ic, nb->nbparam, nb->dev_info); } static void init_plist(cl_plist_t *pl) { pl->sci = NULL; pl->cj4 = NULL; pl->excl = NULL; pl->na_c = -1; pl->nsci = -1; pl->sci_nalloc = -1; pl->ncj4 = -1; pl->cj4_nalloc = -1; pl->nexcl = -1; pl->excl_nalloc = -1; pl->bDoPrune = false; } static void init_timers(cl_timers_t gmx_unused *t, bool gmx_unused bUseTwoStreams) { } static void init_timings(gmx_wallclock_gpu_t *t) { int i, j; t->nb_h2d_t = 0.0; t->nb_d2h_t = 0.0; t->nb_c = 0; t->pl_h2d_t = 0.0; t->pl_h2d_c = 0; for (i = 0; i < 2; i++) { for (j = 0; j < 2; j++) { t->ktime[i][j].t = 0.0; t->ktime[i][j].c = 0; } } } static void nbnxn_gpu_create_context(gmx_nbnxn_ocl_t *nb, int rank) { cl_context_properties context_properties[3]; cl_platform_id platform_id; cl_device_id device_id; cl_context context; cl_int cl_error; platform_id = nb->dev_info->ocl_gpu_id.ocl_platform_id; device_id = nb->dev_info->ocl_gpu_id.ocl_device_id; context_properties[0] = CL_CONTEXT_PLATFORM; context_properties[1] = (cl_context_properties) platform_id; context_properties[2] = 0; context = clCreateContext(context_properties, 1, &device_id, NULL, NULL, &cl_error); if (CL_SUCCESS != cl_error) { gmx_fatal(FARGS, "On rank %d failed to create context for GPU #%s: OpenCL error %d", rank, nb->dev_info->device_name, cl_error); return; } nb->dev_info->context = context; } static cl_kernel nbnxn_gpu_create_kernel(gmx_nbnxn_ocl_t *nb, const char *kernel_name) { cl_kernel kernel; cl_int cl_error; kernel = clCreateKernel(nb->dev_info->program, kernel_name, &cl_error); if (CL_SUCCESS != cl_error) { gmx_fatal(FARGS, "Failed to create kernel '%s' for GPU #%s: OpenCL error %d", kernel_name, nb->dev_info->device_name, cl_error); } return kernel; } static void nbnxn_ocl_clear_e_fshift(gmx_nbnxn_ocl_t *nb) { cl_int cl_error; cl_atomdata_t * adat = nb->atdat; cl_command_queue ls = nb->stream[eintLocal]; size_t local_work_size[3] = {1, 1, 1}; size_t global_work_size[3] = {1, 1, 1}; cl_int shifts = SHIFTS*3; cl_int arg_no; cl_kernel zero_e_fshift = nb->kernel_zero_e_fshift; local_work_size[0] = 64; global_work_size[0] = ((shifts/64)*64) + ((shifts%64) ? 64 : 0); arg_no = 0; cl_error = clSetKernelArg(zero_e_fshift, arg_no++, sizeof(cl_mem), &(adat->fshift)); cl_error |= clSetKernelArg(zero_e_fshift, arg_no++, sizeof(cl_mem), &(adat->e_lj)); cl_error |= clSetKernelArg(zero_e_fshift, arg_no++, sizeof(cl_mem), &(adat->e_el)); cl_error |= clSetKernelArg(zero_e_fshift, arg_no++, sizeof(cl_uint), &shifts); GMX_ASSERT(cl_error == CL_SUCCESS, ocl_get_error_string(cl_error)); cl_error = clEnqueueNDRangeKernel(ls, zero_e_fshift, 3, NULL, global_work_size, local_work_size, 0, NULL, NULL); GMX_ASSERT(cl_error == CL_SUCCESS, ocl_get_error_string(cl_error)); } static void nbnxn_gpu_init_kernels(gmx_nbnxn_ocl_t *nb) { memset(nb->kernel_ener_noprune_ptr, 0, sizeof(nb->kernel_ener_noprune_ptr)); memset(nb->kernel_ener_prune_ptr, 0, sizeof(nb->kernel_ener_prune_ptr)); memset(nb->kernel_noener_noprune_ptr, 0, sizeof(nb->kernel_noener_noprune_ptr)); memset(nb->kernel_noener_prune_ptr, 0, sizeof(nb->kernel_noener_prune_ptr)); nb->kernel_memset_f = nbnxn_gpu_create_kernel(nb, "memset_f"); nb->kernel_memset_f2 = nbnxn_gpu_create_kernel(nb, "memset_f2"); nb->kernel_memset_f3 = nbnxn_gpu_create_kernel(nb, "memset_f3"); nb->kernel_zero_e_fshift = nbnxn_gpu_create_kernel(nb, "zero_e_fshift"); } static void nbnxn_ocl_init_const(gmx_nbnxn_ocl_t *nb, const interaction_const_t *ic, const nonbonded_verlet_group_t *nbv_group) { init_atomdata_first(nb->atdat, nbv_group[0].nbat->ntype, nb->dev_info); init_nbparam(nb->nbparam, ic, nbv_group[0].nbat, nb->dev_info); } void nbnxn_gpu_init(gmx_nbnxn_ocl_t **p_nb, const gmx_gpu_info_t *gpu_info, const gmx_gpu_opt_t *gpu_opt, const interaction_const_t *ic, nonbonded_verlet_group_t *nbv_grp, int my_gpu_index, int rank, gmx_bool bLocalAndNonlocal) { gmx_nbnxn_ocl_t *nb; cl_int cl_error; cl_command_queue_properties queue_properties; assert(gpu_info); assert(gpu_opt); assert(ic); if (p_nb == NULL) { return; } snew(nb, 1); snew(nb->atdat, 1); snew(nb->nbparam, 1); snew(nb->plist[eintLocal], 1); if (bLocalAndNonlocal) { snew(nb->plist[eintNonlocal], 1); } nb->bUseTwoStreams = bLocalAndNonlocal; snew(nb->timers, 1); snew(nb->timings, 1); nb->dev_info = gpu_info->gpu_dev + gpu_opt->dev_use[my_gpu_index]; nb->debug_buffer = NULL; ocl_pmalloc((void**)&nb->nbst.e_lj, sizeof(*nb->nbst.e_lj)); ocl_pmalloc((void**)&nb->nbst.e_el, sizeof(*nb->nbst.e_el)); ocl_pmalloc((void**)&nb->nbst.fshift, SHIFTS * sizeof(*nb->nbst.fshift)); init_plist(nb->plist[eintLocal]); nb->bDoTime = (getenv("GMX_DISABLE_OCL_TIMING") == NULL); if (nb->bDoTime) { queue_properties = CL_QUEUE_PROFILING_ENABLE; } else { queue_properties = 0; } nbnxn_gpu_create_context(nb, rank); nb->stream[eintLocal] = clCreateCommandQueue(nb->dev_info->context, nb->dev_info->ocl_gpu_id.ocl_device_id, queue_properties, &cl_error); if (CL_SUCCESS != cl_error) { gmx_fatal(FARGS, "On rank %d failed to create context for GPU #%s: OpenCL error %d", rank, nb->dev_info->device_name, cl_error); return; } if (nb->bUseTwoStreams) { init_plist(nb->plist[eintNonlocal]); nb->stream[eintNonlocal] = clCreateCommandQueue(nb->dev_info->context, nb->dev_info->ocl_gpu_id.ocl_device_id, queue_properties, &cl_error); if (CL_SUCCESS != cl_error) { gmx_fatal(FARGS, "On rank %d failed to create context for GPU #%s: OpenCL error %d", rank, nb->dev_info->device_name, cl_error); return; } } if (nb->bDoTime) { init_timers(nb->timers, nb->bUseTwoStreams); init_timings(nb->timings); } nbnxn_ocl_init_const(nb, ic, nbv_grp); nbnxn_gpu_compile_kernels(nb); nbnxn_gpu_init_kernels(nb); nbnxn_ocl_clear_e_fshift(nb); *p_nb = nb; if (debug) { fprintf(debug, "Initialized OpenCL data structures.\n"); } } static void nbnxn_ocl_clear_f(gmx_nbnxn_ocl_t *nb, int natoms_clear) { cl_int cl_error; cl_atomdata_t * adat = nb->atdat; cl_command_queue ls = nb->stream[eintLocal]; cl_float value = 0.0f; size_t local_work_size[3] = {1, 1, 1}; size_t global_work_size[3] = {1, 1, 1}; cl_int arg_no; cl_kernel memset_f = nb->kernel_memset_f; cl_uint natoms_flat = natoms_clear * (sizeof(rvec)/sizeof(real)); local_work_size[0] = 64; global_work_size[0] = ((natoms_flat/local_work_size[0])*local_work_size[0]) + ((natoms_flat%local_work_size[0]) ? local_work_size[0] : 0); arg_no = 0; cl_error = clSetKernelArg(memset_f, arg_no++, sizeof(cl_mem), &(adat->f)); cl_error |= clSetKernelArg(memset_f, arg_no++, sizeof(cl_float), &value); cl_error |= clSetKernelArg(memset_f, arg_no++, sizeof(cl_uint), &natoms_flat); assert(cl_error == CL_SUCCESS); cl_error = clEnqueueNDRangeKernel(ls, memset_f, 3, NULL, global_work_size, local_work_size, 0, NULL, NULL); assert(cl_error == CL_SUCCESS); } void nbnxn_gpu_clear_outputs(gmx_nbnxn_ocl_t *nb, int flags) { nbnxn_ocl_clear_f(nb, nb->atdat->natoms); if (flags & GMX_FORCE_VIRIAL) { nbnxn_ocl_clear_e_fshift(nb); } cl_int gmx_unused cl_error; cl_error = clFlush(nb->stream[eintLocal]); assert(CL_SUCCESS == cl_error); } void nbnxn_gpu_init_pairlist(gmx_nbnxn_ocl_t *nb, const nbnxn_pairlist_t *h_plist, int iloc) { char sbuf[STRLEN]; cl_command_queue stream = nb->stream[iloc]; cl_plist_t *d_plist = nb->plist[iloc]; if (d_plist->na_c < 0) { d_plist->na_c = h_plist->na_ci; } else { if (d_plist->na_c != h_plist->na_ci) { sprintf(sbuf, "In cu_init_plist: the #atoms per cell has changed (from %d to %d)", d_plist->na_c, h_plist->na_ci); gmx_incons(sbuf); } } ocl_realloc_buffered(&d_plist->sci, h_plist->sci, sizeof(nbnxn_sci_t), &d_plist->nsci, &d_plist->sci_nalloc, h_plist->nsci, nb->dev_info->context, stream, true, &(nb->timers->pl_h2d_sci[iloc])); ocl_realloc_buffered(&d_plist->cj4, h_plist->cj4, sizeof(nbnxn_cj4_t), &d_plist->ncj4, &d_plist->cj4_nalloc, h_plist->ncj4, nb->dev_info->context, stream, true, &(nb->timers->pl_h2d_cj4[iloc])); ocl_realloc_buffered(&d_plist->excl, h_plist->excl, sizeof(nbnxn_excl_t), &d_plist->nexcl, &d_plist->excl_nalloc, h_plist->nexcl, nb->dev_info->context, stream, true, &(nb->timers->pl_h2d_excl[iloc])); d_plist->bDoPrune = true; } void nbnxn_gpu_upload_shiftvec(gmx_nbnxn_ocl_t *nb, const nbnxn_atomdata_t *nbatom) { cl_atomdata_t *adat = nb->atdat; cl_command_queue ls = nb->stream[eintLocal]; if (nbatom->bDynamicBox || !adat->bShiftVecUploaded) { ocl_copy_H2D_async(adat->shift_vec, nbatom->shift_vec, 0, SHIFTS * adat->shift_vec_elem_size, ls, NULL); adat->bShiftVecUploaded = true; } } void nbnxn_gpu_init_atomdata(gmx_nbnxn_ocl_t *nb, const struct nbnxn_atomdata_t *nbat) { cl_int cl_error; int nalloc, natoms; bool realloced; bool bDoTime = nb->bDoTime; cl_timers_t *timers = nb->timers; cl_atomdata_t *d_atdat = nb->atdat; cl_command_queue ls = nb->stream[eintLocal]; natoms = nbat->natoms; realloced = false; if (natoms > d_atdat->nalloc) { nalloc = over_alloc_small(natoms); if (d_atdat->nalloc != -1) { ocl_free_buffered(d_atdat->f, &d_atdat->natoms, &d_atdat->nalloc); ocl_free_buffered(d_atdat->xq, NULL, NULL); ocl_free_buffered(d_atdat->atom_types, NULL, NULL); } d_atdat->f_elem_size = sizeof(rvec); d_atdat->f = clCreateBuffer(nb->dev_info->context, CL_MEM_READ_WRITE, nalloc * d_atdat->f_elem_size, NULL, &cl_error); assert(CL_SUCCESS == cl_error); d_atdat->xq = clCreateBuffer(nb->dev_info->context, CL_MEM_READ_WRITE, nalloc * sizeof(cl_float4), NULL, &cl_error); assert(CL_SUCCESS == cl_error); d_atdat->atom_types = clCreateBuffer(nb->dev_info->context, CL_MEM_READ_WRITE, nalloc * sizeof(int), NULL, &cl_error); assert(CL_SUCCESS == cl_error); d_atdat->nalloc = nalloc; realloced = true; } d_atdat->natoms = natoms; d_atdat->natoms_local = nbat->natoms_local; if (realloced) { nbnxn_ocl_clear_f(nb, nalloc); } ocl_copy_H2D_async(d_atdat->atom_types, nbat->type, 0, natoms*sizeof(int), ls, bDoTime ? &(timers->atdat) : NULL); cl_error = clFlush(ls); assert(CL_SUCCESS == cl_error); } void free_kernel(cl_kernel *kernel_ptr) { cl_int gmx_unused cl_error; assert(NULL != kernel_ptr); if (*kernel_ptr) { cl_error = clReleaseKernel(*kernel_ptr); assert(cl_error == CL_SUCCESS); *kernel_ptr = NULL; } } void free_kernels(cl_kernel *kernels, int count) { int i; for (i = 0; i < count; i++) { free_kernel(kernels + i); } } void nbnxn_gpu_free(gmx_nbnxn_ocl_t *nb) { int kernel_count; kernel_count = sizeof(nb->kernel_ener_noprune_ptr) / sizeof(nb->kernel_ener_noprune_ptr[0][0]); free_kernels((cl_kernel*)nb->kernel_ener_noprune_ptr, kernel_count); kernel_count = sizeof(nb->kernel_ener_prune_ptr) / sizeof(nb->kernel_ener_prune_ptr[0][0]); free_kernels((cl_kernel*)nb->kernel_ener_prune_ptr, kernel_count); kernel_count = sizeof(nb->kernel_noener_noprune_ptr) / sizeof(nb->kernel_noener_noprune_ptr[0][0]); free_kernels((cl_kernel*)nb->kernel_noener_noprune_ptr, kernel_count); kernel_count = sizeof(nb->kernel_noener_prune_ptr) / sizeof(nb->kernel_noener_prune_ptr[0][0]); free_kernels((cl_kernel*)nb->kernel_noener_prune_ptr, kernel_count); free_kernel(&(nb->kernel_memset_f)); free_kernel(&(nb->kernel_memset_f2)); free_kernel(&(nb->kernel_memset_f3)); free_kernel(&(nb->kernel_zero_e_fshift)); free_ocl_buffer(&(nb->atdat->xq)); free_ocl_buffer(&(nb->atdat->f)); free_ocl_buffer(&(nb->atdat->e_lj)); free_ocl_buffer(&(nb->atdat->e_el)); free_ocl_buffer(&(nb->atdat->fshift)); free_ocl_buffer(&(nb->atdat->atom_types)); free_ocl_buffer(&(nb->atdat->shift_vec)); sfree(nb->atdat); free_ocl_buffer(&(nb->nbparam->nbfp_climg2d)); free_ocl_buffer(&(nb->nbparam->nbfp_comb_climg2d)); free_ocl_buffer(&(nb->nbparam->coulomb_tab_climg2d)); sfree(nb->nbparam); free_ocl_buffer(&(nb->plist[eintLocal]->sci)); free_ocl_buffer(&(nb->plist[eintLocal]->cj4)); free_ocl_buffer(&(nb->plist[eintLocal]->excl)); sfree(nb->plist[eintLocal]); if (nb->bUseTwoStreams) { free_ocl_buffer(&(nb->plist[eintNonlocal]->sci)); free_ocl_buffer(&(nb->plist[eintNonlocal]->cj4)); free_ocl_buffer(&(nb->plist[eintNonlocal]->excl)); sfree(nb->plist[eintNonlocal]); } ocl_pfree(nb->nbst.e_lj); nb->nbst.e_lj = NULL; ocl_pfree(nb->nbst.e_el); nb->nbst.e_el = NULL; ocl_pfree(nb->nbst.fshift); nb->nbst.fshift = NULL; free_ocl_buffer(&nb->debug_buffer); clReleaseCommandQueue(nb->stream[eintLocal]); nb->stream[eintLocal] = NULL; if (nb->bUseTwoStreams) { clReleaseCommandQueue(nb->stream[eintNonlocal]); nb->stream[eintNonlocal] = NULL; } if (nb->nonlocal_done) { clReleaseEvent(nb->nonlocal_done); nb->nonlocal_done = NULL; } if (nb->misc_ops_and_local_H2D_done) { clReleaseEvent(nb->misc_ops_and_local_H2D_done); nb->misc_ops_and_local_H2D_done = NULL; } sfree(nb->timers); sfree(nb->timings); sfree(nb); if (debug) { fprintf(debug, "Cleaned up OpenCL data structures.\n"); } } gmx_wallclock_gpu_t * nbnxn_gpu_get_timings(gmx_nbnxn_ocl_t *nb) { return (nb != NULL && nb->bDoTime) ? nb->timings : NULL; } void nbnxn_gpu_reset_timings(nonbonded_verlet_t* nbv) { if (nbv->gpu_nbv && nbv->gpu_nbv->bDoTime) { init_timings(nbv->gpu_nbv->timings); } } int nbnxn_gpu_min_ci_balanced(gmx_nbnxn_ocl_t *nb) { return nb != NULL ? gpu_min_ci_balanced_factor * nb->dev_info->compute_units : 0; } gmx_bool nbnxn_gpu_is_kernel_ewald_analytical(const gmx_nbnxn_ocl_t *nb) { return ((nb->nbparam->eeltype == eelOclEWALD_ANA) || (nb->nbparam->eeltype == eelOclEWALD_ANA_TWIN)); }