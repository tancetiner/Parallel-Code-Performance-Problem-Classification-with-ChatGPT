#include "gmxpre.h" #include "config.h" #include <assert.h> #include <stdlib.h> #if defined(_MSVC) #include <limits> #endif #include "gromacs/gpu_utils/oclutils.h" #include "gromacs/hardware/hw_info.h" #include "gromacs/mdlib/force_flags.h" #include "gromacs/mdlib/nb_verlet.h" #include "gromacs/mdlib/nbnxn_consts.h" #include "gromacs/mdlib/nbnxn_pairlist.h" #include "gromacs/timing/gpu_timing.h" #ifdef TMPI_ATOMICS #include "thread_mpi/atomic.h" #endif #include "gromacs/mdlib/nbnxn_gpu.h" #include "gromacs/mdlib/nbnxn_gpu_data_mgmt.h" #include "gromacs/pbcutil/ishift.h" #include "gromacs/utility/cstringutil.h" #include "gromacs/utility/fatalerror.h" #include "gromacs/utility/gmxassert.h" #include "nbnxn_ocl_types.h" #if defined TEXOBJ_SUPPORTED && __CUDA_ARCH__ >= 300 #define USE_TEXOBJ #endif static const int c_numClPerSupercl = c_nbnxnGpuNumClusterPerSupercluster; static const int c_clSize = c_nbnxnGpuClusterSize; static bool always_ener = (getenv("GMX_GPU_ALWAYS_ENER") != NULL); static bool never_ener = (getenv("GMX_GPU_NEVER_ENER") != NULL); static bool always_prune = (getenv("GMX_GPU_ALWAYS_PRUNE") != NULL); #define DEBUG_RUN_STEP 2 static inline void validate_global_work_size(size_t *global_work_size, int work_dim, gmx_device_info_t *dinfo) { cl_uint device_size_t_size_bits; cl_uint host_size_t_size_bits; assert(dinfo); device_size_t_size_bits = dinfo->adress_bits; host_size_t_size_bits = (cl_uint)(sizeof(size_t) * 8); if (host_size_t_size_bits > device_size_t_size_bits) { size_t device_limit; device_limit = (((size_t)1) << device_size_t_size_bits) - 1; for (int i = 0; i < work_dim; i++) { if (global_work_size[i] > device_limit) { gmx_fatal(FARGS, "Watch out, the input system is too large to simulate!\n" "The number of nonbonded work units (=number of super-clusters) exceeds the" "device capabilities. Global work size limit exceeded (%d > %d)!", global_work_size[i], device_limit); } } } } static const char* nb_kfunc_noener_noprune_ptr[eelOclNR][evdwOclNR] = { { "nbnxn_kernel_ElecCut_VdwLJ_F_opencl", "nbnxn_kernel_ElecCut_VdwLJFsw_F_opencl", "nbnxn_kernel_ElecCut_VdwLJPsw_F_opencl", "nbnxn_kernel_ElecCut_VdwLJEwCombGeom_F_opencl", "nbnxn_kernel_ElecCut_VdwLJEwCombLB_F_opencl" }, { "nbnxn_kernel_ElecRF_VdwLJ_F_opencl", "nbnxn_kernel_ElecRF_VdwLJFsw_F_opencl", "nbnxn_kernel_ElecRF_VdwLJPsw_F_opencl", "nbnxn_kernel_ElecRF_VdwLJEwCombGeom_F_opencl", "nbnxn_kernel_ElecRF_VdwLJEwCombLB_F_opencl" }, { "nbnxn_kernel_ElecEwQSTab_VdwLJ_F_opencl", "nbnxn_kernel_ElecEwQSTab_VdwLJFsw_F_opencl", "nbnxn_kernel_ElecEwQSTab_VdwLJPsw_F_opencl", "nbnxn_kernel_ElecEwQSTab_VdwLJEwCombGeom_F_opencl", "nbnxn_kernel_ElecEwQSTab_VdwLJEwCombLB_F_opencl" }, { "nbnxn_kernel_ElecEwQSTabTwinCut_VdwLJ_F_opencl", "nbnxn_kernel_ElecEwQSTabTwinCut_VdwLJFsw_F_opencl", "nbnxn_kernel_ElecEwQSTabTwinCut_VdwLJPsw_F_opencl", "nbnxn_kernel_ElecEwQSTabTwinCut_VdwLJEwCombGeom_F_opencl", "nbnxn_kernel_ElecEwQSTabTwinCut_VdwLJEwCombLB_F_opencl" }, { "nbnxn_kernel_ElecEw_VdwLJ_F_opencl", "nbnxn_kernel_ElecEw_VdwLJFsw_F_opencl", "nbnxn_kernel_ElecEw_VdwLJPsw_F_opencl", "nbnxn_kernel_ElecEw_VdwLJEwCombGeom_F_opencl", "nbnxn_kernel_ElecEw_VdwLJEwCombLB_F_opencl" }, { "nbnxn_kernel_ElecEwTwinCut_VdwLJ_F_opencl", "nbnxn_kernel_ElecEwTwinCut_VdwLJFsw_F_opencl", "nbnxn_kernel_ElecEwTwinCut_VdwLJPsw_F_opencl", "nbnxn_kernel_ElecEwTwinCut_VdwLJEwCombGeom_F_opencl", "nbnxn_kernel_ElecEwTwinCut_VdwLJEwCombLB_F_opencl" } }; static const char* nb_kfunc_ener_noprune_ptr[eelOclNR][evdwOclNR] = { { "nbnxn_kernel_ElecCut_VdwLJ_VF_opencl", "nbnxn_kernel_ElecCut_VdwLJFsw_VF_opencl", "nbnxn_kernel_ElecCut_VdwLJPsw_VF_opencl", "nbnxn_kernel_ElecCut_VdwLJEwCombGeom_VF_opencl", "nbnxn_kernel_ElecCut_VdwLJEwCombLB_VF_opencl" }, { "nbnxn_kernel_ElecRF_VdwLJ_VF_opencl", "nbnxn_kernel_ElecRF_VdwLJFsw_VF_opencl", "nbnxn_kernel_ElecRF_VdwLJPsw_VF_opencl", "nbnxn_kernel_ElecRF_VdwLJEwCombGeom_VF_opencl", "nbnxn_kernel_ElecRF_VdwLJEwCombLB_VF_opencl" }, { "nbnxn_kernel_ElecEwQSTab_VdwLJ_VF_opencl", "nbnxn_kernel_ElecEwQSTab_VdwLJFsw_VF_opencl", "nbnxn_kernel_ElecEwQSTab_VdwLJPsw_VF_opencl", "nbnxn_kernel_ElecEwQSTab_VdwLJEwCombGeom_VF_opencl", "nbnxn_kernel_ElecEwQSTab_VdwLJEwCombLB_VF_opencl" }, { "nbnxn_kernel_ElecEwQSTabTwinCut_VdwLJ_VF_opencl", "nbnxn_kernel_ElecEwQSTabTwinCut_VdwLJFsw_VF_opencl", "nbnxn_kernel_ElecEwQSTabTwinCut_VdwLJPsw_VF_opencl", "nbnxn_kernel_ElecEwQSTabTwinCut_VdwLJEwCombGeom_VF_opencl", "nbnxn_kernel_ElecEwQSTabTwinCut_VdwLJEwCombLB_VF_opencl" }, { "nbnxn_kernel_ElecEw_VdwLJ_VF_opencl", "nbnxn_kernel_ElecEw_VdwLJFsw_VF_opencl", "nbnxn_kernel_ElecEw_VdwLJPsw_VF_opencl", "nbnxn_kernel_ElecEw_VdwLJEwCombGeom_VF_opencl", "nbnxn_kernel_ElecEw_VdwLJEwCombLB_VF_opencl" }, { "nbnxn_kernel_ElecEwTwinCut_VdwLJ_VF_opencl", "nbnxn_kernel_ElecEwTwinCut_VdwLJFsw_VF_opencl", "nbnxn_kernel_ElecEwTwinCut_VdwLJPsw_VF_opencl", "nbnxn_kernel_ElecEwTwinCut_VdwLJEwCombGeom_VF_opencl", "nbnxn_kernel_ElecEwTwinCut_VdwLJEwCombLB_VF_opencl" } }; static const char* nb_kfunc_noener_prune_ptr[eelOclNR][evdwOclNR] = { { "nbnxn_kernel_ElecCut_VdwLJ_F_prune_opencl", "nbnxn_kernel_ElecCut_VdwLJFsw_F_prune_opencl", "nbnxn_kernel_ElecCut_VdwLJPsw_F_prune_opencl", "nbnxn_kernel_ElecCut_VdwLJEwCombGeom_F_prune_opencl", "nbnxn_kernel_ElecCut_VdwLJEwCombLB_F_prune_opencl" }, { "nbnxn_kernel_ElecRF_VdwLJ_F_prune_opencl", "nbnxn_kernel_ElecRF_VdwLJFsw_F_prune_opencl", "nbnxn_kernel_ElecRF_VdwLJPsw_F_prune_opencl", "nbnxn_kernel_ElecRF_VdwLJEwCombGeom_F_prune_opencl", "nbnxn_kernel_ElecRF_VdwLJEwCombLB_F_prune_opencl" }, { "nbnxn_kernel_ElecEwQSTab_VdwLJ_F_prune_opencl", "nbnxn_kernel_ElecEwQSTab_VdwLJFsw_F_prune_opencl", "nbnxn_kernel_ElecEwQSTab_VdwLJPsw_F_prune_opencl", "nbnxn_kernel_ElecEwQSTab_VdwLJEwCombGeom_F_prune_opencl", "nbnxn_kernel_ElecEwQSTab_VdwLJEwCombLB_F_prune_opencl" }, { "nbnxn_kernel_ElecEwQSTabTwinCut_VdwLJ_F_prune_opencl", "nbnxn_kernel_ElecEwQSTabTwinCut_VdwLJFsw_F_prune_opencl", "nbnxn_kernel_ElecEwQSTabTwinCut_VdwLJPsw_F_prune_opencl", "nbnxn_kernel_ElecEwQSTabTwinCut_VdwLJEwCombGeom_F_prune_opencl", "nbnxn_kernel_ElecEwQSTabTwinCut_VdwLJEwCombLB_F_prune_opencl" }, { "nbnxn_kernel_ElecEw_VdwLJ_F_prune_opencl", "nbnxn_kernel_ElecEw_VdwLJFsw_F_prune_opencl", "nbnxn_kernel_ElecEw_VdwLJPsw_F_prune_opencl", "nbnxn_kernel_ElecEw_VdwLJEwCombGeom_F_prune_opencl", "nbnxn_kernel_ElecEw_VdwLJEwCombLB_F_prune_opencl" }, { "nbnxn_kernel_ElecEwTwinCut_VdwLJ_F_prune_opencl", "nbnxn_kernel_ElecEwTwinCut_VdwLJFsw_F_prune_opencl", "nbnxn_kernel_ElecEwTwinCut_VdwLJPsw_F_prune_opencl", "nbnxn_kernel_ElecEwTwinCut_VdwLJEwCombGeom_F_prune_opencl", "nbnxn_kernel_ElecEwTwinCut_VdwLJEwCombLB_F_prune_opencl" } }; static const char* nb_kfunc_ener_prune_ptr[eelOclNR][evdwOclNR] = { { "nbnxn_kernel_ElecCut_VdwLJ_VF_prune_opencl", "nbnxn_kernel_ElecCut_VdwLJFsw_VF_prune_opencl", "nbnxn_kernel_ElecCut_VdwLJPsw_VF_prune_opencl", "nbnxn_kernel_ElecCut_VdwLJEwCombGeom_VF_prune_opencl", "nbnxn_kernel_ElecCut_VdwLJEwCombLB_VF_prune_opencl" }, { "nbnxn_kernel_ElecRF_VdwLJ_VF_prune_opencl", "nbnxn_kernel_ElecRF_VdwLJFsw_VF_prune_opencl", "nbnxn_kernel_ElecRF_VdwLJPsw_VF_prune_opencl", "nbnxn_kernel_ElecRF_VdwLJEwCombGeom_VF_prune_opencl", "nbnxn_kernel_ElecRF_VdwLJEwCombLB_VF_prune_opencl" }, { "nbnxn_kernel_ElecEwQSTab_VdwLJ_VF_prune_opencl", "nbnxn_kernel_ElecEwQSTab_VdwLJFsw_VF_prune_opencl", "nbnxn_kernel_ElecEwQSTab_VdwLJPsw_VF_prune_opencl", "nbnxn_kernel_ElecEwQSTab_VdwLJEwCombGeom_VF_prune_opencl", "nbnxn_kernel_ElecEwQSTab_VdwLJEwCombLB_VF_prune_opencl" }, { "nbnxn_kernel_ElecEwQSTabTwinCut_VdwLJ_VF_prune_opencl", "nbnxn_kernel_ElecEwQSTabTwinCut_VdwLJFsw_VF_prune_opencl", "nbnxn_kernel_ElecEwQSTabTwinCut_VdwLJPsw_VF_prune_opencl", "nbnxn_kernel_ElecEwQSTabTwinCut_VdwLJEwCombGeom_VF_prune_opencl", "nbnxn_kernel_ElecEwQSTabTwinCut_VdwLJEwCombLB_VF_prune_opencl" }, { "nbnxn_kernel_ElecEw_VdwLJ_VF_prune_opencl", "nbnxn_kernel_ElecEw_VdwLJFsw_VF_prune_opencl", "nbnxn_kernel_ElecEw_VdwLJPsw_VF_prune_opencl", "nbnxn_kernel_ElecEw_VdwLJEwCombGeom_VF_prune_opencl", "nbnxn_kernel_ElecEw_VdwLJEwCombLB_VF_prune_opencl" }, { "nbnxn_kernel_ElecEwTwinCut_VdwLJ_VF_prune_opencl", "nbnxn_kernel_ElecEwTwinCut_VdwLJFsw_VF_prune_opencl", "nbnxn_kernel_ElecEwTwinCut_VdwLJPsw_VF_prune_opencl", "nbnxn_kernel_ElecEwTwinCut_VdwLJEwCombGeom_VF_prune_opencl", "nbnxn_kernel_ElecEwTwinCut_VdwLJEwCombLB_VF_prune_opencl" } }; static inline cl_kernel select_nbnxn_kernel(gmx_nbnxn_ocl_t *nb, int eeltype, int evdwtype, bool bDoEne, bool bDoPrune) { const char* kernel_name_to_run; cl_kernel *kernel_ptr; cl_int cl_error; assert(eeltype < eelOclNR); assert(evdwtype < eelOclNR); if (bDoEne) { if (bDoPrune) { kernel_name_to_run = nb_kfunc_ener_prune_ptr[eeltype][evdwtype]; kernel_ptr = &(nb->kernel_ener_prune_ptr[eeltype][evdwtype]); } else { kernel_name_to_run = nb_kfunc_ener_noprune_ptr[eeltype][evdwtype]; kernel_ptr = &(nb->kernel_ener_noprune_ptr[eeltype][evdwtype]); } } else { if (bDoPrune) { kernel_name_to_run = nb_kfunc_noener_prune_ptr[eeltype][evdwtype]; kernel_ptr = &(nb->kernel_noener_prune_ptr[eeltype][evdwtype]); } else { kernel_name_to_run = nb_kfunc_noener_noprune_ptr[eeltype][evdwtype]; kernel_ptr = &(nb->kernel_noener_noprune_ptr[eeltype][evdwtype]); } } if (NULL == kernel_ptr[0]) { *kernel_ptr = clCreateKernel(nb->dev_info->program, kernel_name_to_run, &cl_error); assert(cl_error == CL_SUCCESS); } return *kernel_ptr; } static inline int calc_shmem_required() { int shmem; shmem = c_numClPerSupercl * c_clSize * sizeof(float) * 4; shmem += 2 * c_nbnxnGpuJgroupSize * sizeof(int); #ifdef IATYPE_SHMEM #pragma error "Should not be defined" shmem += c_numClPerSupercl * c_clSize * sizeof(int); #endif shmem += c_clSize * c_clSize * 3 * sizeof(float); shmem += sizeof(cl_uint) * 2; return shmem; } static void fillin_ocl_structures(cl_nbparam_t *nbp, cl_nbparam_params_t *nbparams_params) { nbparams_params->coulomb_tab_scale = nbp->coulomb_tab_scale; nbparams_params->coulomb_tab_size = nbp->coulomb_tab_size; nbparams_params->c_rf = nbp->c_rf; nbparams_params->dispersion_shift = nbp->dispersion_shift; nbparams_params->eeltype = nbp->eeltype; nbparams_params->epsfac = nbp->epsfac; nbparams_params->ewaldcoeff_lj = nbp->ewaldcoeff_lj; nbparams_params->ewald_beta = nbp->ewald_beta; nbparams_params->rcoulomb_sq = nbp->rcoulomb_sq; nbparams_params->repulsion_shift = nbp->repulsion_shift; nbparams_params->rlist_sq = nbp->rlist_sq; nbparams_params->rvdw_sq = nbp->rvdw_sq; nbparams_params->rvdw_switch = nbp->rvdw_switch; nbparams_params->sh_ewald = nbp->sh_ewald; nbparams_params->sh_lj_ewald = nbp->sh_lj_ewald; nbparams_params->two_k_rf = nbp->two_k_rf; nbparams_params->vdwtype = nbp->vdwtype; nbparams_params->vdw_switch = nbp->vdw_switch; } void wait_ocl_event(cl_event *ocl_event) { cl_int gmx_unused cl_error; cl_error = clWaitForEvents(1, ocl_event); assert(CL_SUCCESS == cl_error); cl_error = clReleaseEvent(*ocl_event); assert(CL_SUCCESS == cl_error); *ocl_event = 0; } void sync_ocl_event(cl_command_queue stream, cl_event *ocl_event) { cl_int gmx_unused cl_error; #ifdef CL_VERSION_1_2 cl_error = clEnqueueBarrierWithWaitList(stream, 1, ocl_event, NULL); #else cl_error = clEnqueueWaitForEvents(stream, 1, ocl_event); #endif GMX_RELEASE_ASSERT(CL_SUCCESS == cl_error, ocl_get_error_string(cl_error)); cl_error = clReleaseEvent(*ocl_event); assert(CL_SUCCESS == cl_error); *ocl_event = 0; } double ocl_event_elapsed_ms(cl_event *ocl_event) { cl_int gmx_unused cl_error; cl_ulong start_ns, end_ns; double elapsed_ms; elapsed_ms = 0.0; assert(NULL != ocl_event); if (*ocl_event) { cl_error = clGetEventProfilingInfo(*ocl_event, CL_PROFILING_COMMAND_START, sizeof(cl_ulong), &start_ns, NULL); assert(CL_SUCCESS == cl_error); cl_error = clGetEventProfilingInfo(*ocl_event, CL_PROFILING_COMMAND_END, sizeof(cl_ulong), &end_ns, NULL); assert(CL_SUCCESS == cl_error); clReleaseEvent(*ocl_event); *ocl_event = 0; elapsed_ms = (end_ns - start_ns) / 1000000.0; } return elapsed_ms; } void nbnxn_gpu_launch_kernel(gmx_nbnxn_ocl_t *nb, const struct nbnxn_atomdata_t *nbatom, int flags, int iloc) { cl_int cl_error; int adat_begin, adat_len; int shmem; size_t local_work_size[3], global_work_size[3]; cl_kernel nb_kernel = NULL; cl_atomdata_t *adat = nb->atdat; cl_nbparam_t *nbp = nb->nbparam; cl_plist_t *plist = nb->plist[iloc]; cl_timers_t *t = nb->timers; cl_command_queue stream = nb->stream[iloc]; bool bCalcEner = flags & GMX_FORCE_ENERGY; int bCalcFshift = flags & GMX_FORCE_VIRIAL; bool bDoTime = nb->bDoTime; cl_uint arg_no; cl_nbparam_params_t nbparams_params; #ifdef DEBUG_OCL float * debug_buffer_h; size_t debug_buffer_size; #endif bCalcEner = (bCalcEner || always_ener) && !never_ener; if (iloc == eintNonlocal && plist->nsci == 0) { return; } if (LOCAL_I(iloc)) { adat_begin = 0; adat_len = adat->natoms_local; } else { adat_begin = adat->natoms_local; adat_len = adat->natoms - adat->natoms_local; } ocl_copy_H2D_async(adat->xq, nbatom->x + adat_begin * 4, adat_begin*sizeof(float)*4, adat_len * sizeof(float) * 4, stream, bDoTime ? (&(t->nb_h2d[iloc])) : NULL); if (nb->bUseTwoStreams) { if (iloc == eintLocal) { #ifdef CL_VERSION_1_2 cl_error = clEnqueueMarkerWithWaitList(stream, 0, NULL, &(nb->misc_ops_and_local_H2D_done)); #else cl_error = clEnqueueMarker(stream, &(nb->misc_ops_and_local_H2D_done)); #endif assert(CL_SUCCESS == cl_error); cl_error = clFlush(stream); assert(CL_SUCCESS == cl_error); } else { sync_ocl_event(stream, &(nb->misc_ops_and_local_H2D_done)); } } if (plist->nsci == 0) { return; } nb_kernel = select_nbnxn_kernel(nb, nbp->eeltype, nbp->vdwtype, bCalcEner, plist->bDoPrune || always_prune); local_work_size[0] = c_clSize; local_work_size[1] = c_clSize; local_work_size[2] = 1; global_work_size[0] = plist->nsci * local_work_size[0]; global_work_size[1] = 1 * local_work_size[1]; global_work_size[2] = 1 * local_work_size[2]; validate_global_work_size(global_work_size, 3, nb->dev_info); shmem = calc_shmem_required(); #ifdef DEBUG_OCL { static int run_step = 1; if (DEBUG_RUN_STEP == run_step) { debug_buffer_size = global_work_size[0] * global_work_size[1] * global_work_size[2] * sizeof(float); debug_buffer_h = (float*)calloc(1, debug_buffer_size); assert(NULL != debug_buffer_h); if (NULL == nb->debug_buffer) { nb->debug_buffer = clCreateBuffer(nb->dev_info->context, CL_MEM_READ_WRITE | CL_MEM_COPY_HOST_PTR, debug_buffer_size, debug_buffer_h, &cl_error); assert(CL_SUCCESS == cl_error); } } run_step++; } #endif if (debug) { fprintf(debug, "GPU launch configuration:\n\tLocal work size: %dx%dx%d\n\t" "Global work size : %dx%d\n\t#Super-clusters/clusters: %d/%d (%d)\n", (int)(local_work_size[0]), (int)(local_work_size[1]), (int)(local_work_size[2]), (int)(global_work_size[0]), (int)(global_work_size[1]), plist->nsci*c_numClPerSupercl, c_numClPerSupercl, plist->na_c); } fillin_ocl_structures(nbp, &nbparams_params); arg_no = 0; cl_error = clSetKernelArg(nb_kernel, arg_no++, sizeof(int), &(adat->ntypes)); cl_error |= clSetKernelArg(nb_kernel, arg_no++, sizeof(nbparams_params), &(nbparams_params)); cl_error |= clSetKernelArg(nb_kernel, arg_no++, sizeof(cl_mem), &(adat->xq)); cl_error |= clSetKernelArg(nb_kernel, arg_no++, sizeof(cl_mem), &(adat->f)); cl_error |= clSetKernelArg(nb_kernel, arg_no++, sizeof(cl_mem), &(adat->e_lj)); cl_error |= clSetKernelArg(nb_kernel, arg_no++, sizeof(cl_mem), &(adat->e_el)); cl_error |= clSetKernelArg(nb_kernel, arg_no++, sizeof(cl_mem), &(adat->fshift)); cl_error |= clSetKernelArg(nb_kernel, arg_no++, sizeof(cl_mem), &(adat->atom_types)); cl_error |= clSetKernelArg(nb_kernel, arg_no++, sizeof(cl_mem), &(adat->shift_vec)); cl_error |= clSetKernelArg(nb_kernel, arg_no++, sizeof(cl_mem), &(nbp->nbfp_climg2d)); cl_error |= clSetKernelArg(nb_kernel, arg_no++, sizeof(cl_mem), &(nbp->nbfp_comb_climg2d)); cl_error |= clSetKernelArg(nb_kernel, arg_no++, sizeof(cl_mem), &(nbp->coulomb_tab_climg2d)); cl_error |= clSetKernelArg(nb_kernel, arg_no++, sizeof(cl_mem), &(plist->sci)); cl_error |= clSetKernelArg(nb_kernel, arg_no++, sizeof(cl_mem), &(plist->cj4)); cl_error |= clSetKernelArg(nb_kernel, arg_no++, sizeof(cl_mem), &(plist->excl)); cl_error |= clSetKernelArg(nb_kernel, arg_no++, sizeof(int), &bCalcFshift); cl_error |= clSetKernelArg(nb_kernel, arg_no++, shmem, NULL); cl_error |= clSetKernelArg(nb_kernel, arg_no++, sizeof(cl_mem), &(nb->debug_buffer)); assert(cl_error == CL_SUCCESS); if (cl_error) { printf("ClERROR! %d\n", cl_error); } cl_error = clEnqueueNDRangeKernel(stream, nb_kernel, 3, NULL, global_work_size, local_work_size, 0, NULL, bDoTime ? &(t->nb_k[iloc]) : NULL); assert(cl_error == CL_SUCCESS); #ifdef DEBUG_OCL { static int run_step = 1; if (DEBUG_RUN_STEP == run_step) { FILE *pf; char file_name[256] = {0}; ocl_copy_D2H_async(debug_buffer_h, nb->debug_buffer, 0, debug_buffer_size, stream, NULL); clFinish(stream); printf("\nWriting debug_buffer to debug_buffer_ocl.txt..."); sprintf(file_name, "debug_buffer_ocl_%d.txt", DEBUG_RUN_STEP); pf = fopen(file_name, "wt"); assert(pf != NULL); fprintf(pf, "%20s", ""); for (int j = 0; j < global_work_size[0]; j++) { char label[20]; sprintf(label, "(wIdx=%2d thIdx=%2d)", j / local_work_size[0], j % local_work_size[0]); fprintf(pf, "%20s", label); } for (int i = 0; i < global_work_size[1]; i++) { char label[20]; sprintf(label, "(wIdy=%2d thIdy=%2d)", i / local_work_size[1], i % local_work_size[1]); fprintf(pf, "\n%20s", label); for (int j = 0; j < global_work_size[0]; j++) { fprintf(pf, "%20.5f", debug_buffer_h[i * global_work_size[0] + j]); } } fclose(pf); printf(" done.\n"); free(debug_buffer_h); debug_buffer_h = NULL; } run_step++; } #endif } void nbnxn_gpu_launch_cpyback(gmx_nbnxn_ocl_t *nb, const struct nbnxn_atomdata_t *nbatom, int flags, int aloc) { cl_int gmx_unused cl_error; int adat_begin, adat_len; int iloc = -1; if (LOCAL_A(aloc)) { iloc = eintLocal; } else if (NONLOCAL_A(aloc)) { iloc = eintNonlocal; } else { char stmp[STRLEN]; sprintf(stmp, "Invalid atom locality passed (%d); valid here is only " "local (%d) or nonlocal (%d)", aloc, eatLocal, eatNonlocal); gmx_incons(stmp); } cl_atomdata_t *adat = nb->atdat; cl_timers_t *t = nb->timers; bool bDoTime = nb->bDoTime; cl_command_queue stream = nb->stream[iloc]; bool bCalcEner = flags & GMX_FORCE_ENERGY; int bCalcFshift = flags & GMX_FORCE_VIRIAL; if (iloc == eintNonlocal && nb->plist[iloc]->nsci == 0) { nb->bNonLocalStreamActive = false; return; } if (LOCAL_A(aloc)) { adat_begin = 0; adat_len = adat->natoms_local; } else { adat_begin = adat->natoms_local; adat_len = adat->natoms - adat->natoms_local; } if (iloc == eintLocal && nb->bNonLocalStreamActive) { sync_ocl_event(stream, &(nb->nonlocal_done)); } ocl_copy_D2H_async(nbatom->out[0].f + adat_begin * 3, adat->f, adat_begin*3*sizeof(float), (adat_len)* adat->f_elem_size, stream, bDoTime ? &(t->nb_d2h_f[iloc]) : NULL); cl_error = clFlush(stream); assert(CL_SUCCESS == cl_error); if (iloc == eintNonlocal) { #ifdef CL_VERSION_1_2 cl_error = clEnqueueMarkerWithWaitList(stream, 0, NULL, &(nb->nonlocal_done)); #else cl_error = clEnqueueMarker(stream, &(nb->nonlocal_done)); #endif assert(CL_SUCCESS == cl_error); nb->bNonLocalStreamActive = true; } if (LOCAL_I(iloc)) { if (bCalcFshift) { ocl_copy_D2H_async(nb->nbst.fshift, adat->fshift, 0, SHIFTS * adat->fshift_elem_size, stream, bDoTime ? &(t->nb_d2h_fshift[iloc]) : NULL); } if (bCalcEner) { ocl_copy_D2H_async(nb->nbst.e_lj, adat->e_lj, 0, sizeof(float), stream, bDoTime ? &(t->nb_d2h_e_lj[iloc]) : NULL); ocl_copy_D2H_async(nb->nbst.e_el, adat->e_el, 0, sizeof(float), stream, bDoTime ? &(t->nb_d2h_e_el[iloc]) : NULL); } } } void nbnxn_gpu_wait_for_gpu(gmx_nbnxn_ocl_t *nb, int flags, int aloc, real *e_lj, real *e_el, rvec *fshift) { cl_int gmx_unused cl_error; int i, iloc = -1; if (LOCAL_A(aloc)) { iloc = eintLocal; } else if (NONLOCAL_A(aloc)) { iloc = eintNonlocal; } else { char stmp[STRLEN]; sprintf(stmp, "Invalid atom locality passed (%d); valid here is only " "local (%d) or nonlocal (%d)", aloc, eatLocal, eatNonlocal); gmx_incons(stmp); } cl_plist_t *plist = nb->plist[iloc]; cl_timers_t *timers = nb->timers; struct gmx_wallclock_gpu_t *timings = nb->timings; cl_nb_staging nbst = nb->nbst; bool bCalcEner = flags & GMX_FORCE_ENERGY; int bCalcFshift = flags & GMX_FORCE_VIRIAL; bCalcEner = (bCalcEner || always_ener) && !never_ener; if (iloc == eintNonlocal && nb->plist[iloc]->nsci == 0) { return; } cl_error = clFinish(nb->stream[iloc]); assert(CL_SUCCESS == cl_error); if (nb->bDoTime) { if (LOCAL_I(iloc)) { timings->nb_c++; timings->ktime[plist->bDoPrune ? 1 : 0][bCalcEner ? 1 : 0].c += 1; } timings->ktime[plist->bDoPrune ? 1 : 0][bCalcEner ? 1 : 0].t += ocl_event_elapsed_ms(timers->nb_k + iloc); timings->nb_h2d_t += ocl_event_elapsed_ms(timers->nb_h2d + iloc); timings->nb_d2h_t += ocl_event_elapsed_ms(timers->nb_d2h_f + iloc); timings->nb_d2h_t += ocl_event_elapsed_ms(timers->nb_d2h_fshift + iloc); timings->nb_d2h_t += ocl_event_elapsed_ms(timers->nb_d2h_e_el + iloc); timings->nb_d2h_t += ocl_event_elapsed_ms(timers->nb_d2h_e_lj + iloc); if (plist->bDoPrune) { if (LOCAL_A(aloc)) { timings->pl_h2d_c++; timings->pl_h2d_t += ocl_event_elapsed_ms(&(timers->atdat)); } timings->pl_h2d_t += ocl_event_elapsed_ms(timers->pl_h2d_sci + iloc) + ocl_event_elapsed_ms(timers->pl_h2d_cj4 + iloc) + ocl_event_elapsed_ms(timers->pl_h2d_excl + iloc); } } if (LOCAL_I(iloc)) { if (bCalcEner) { *e_lj += *nbst.e_lj; *e_el += *nbst.e_el; } if (bCalcFshift) { for (i = 0; i < SHIFTS; i++) { fshift[i][0] += (nbst.fshift)[i][0]; fshift[i][1] += (nbst.fshift)[i][1]; fshift[i][2] += (nbst.fshift)[i][2]; } } } plist->bDoPrune = false; } int nbnxn_gpu_pick_ewald_kernel_type(bool bTwinCut) { bool bUseAnalyticalEwald, bForceAnalyticalEwald, bForceTabulatedEwald; int kernel_type; bForceAnalyticalEwald = (getenv("GMX_OCL_NB_ANA_EWALD") != NULL); bForceTabulatedEwald = (getenv("GMX_OCL_NB_TAB_EWALD") != NULL); if (bForceAnalyticalEwald && bForceTabulatedEwald) { gmx_incons("Both analytical and tabulated Ewald OpenCL non-bonded kernels " "requested through environment variables."); } if ((1 || bForceAnalyticalEwald) && !bForceTabulatedEwald) { bUseAnalyticalEwald = true; if (debug) { fprintf(debug, "Using analytical Ewald OpenCL kernels\n"); } } else { bUseAnalyticalEwald = false; if (debug) { fprintf(debug, "Using tabulated Ewald OpenCL kernels\n"); } } if (!bTwinCut && (getenv("GMX_OCL_NB_EWALD_TWINCUT") == NULL)) { kernel_type = bUseAnalyticalEwald ? eelOclEWALD_ANA : eelOclEWALD_TAB; } else { kernel_type = bUseAnalyticalEwald ? eelOclEWALD_ANA_TWIN : eelOclEWALD_TAB_TWIN; } return kernel_type; }